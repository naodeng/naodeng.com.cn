[{"content":"Implementation of Interface Automation Projects with Java and REST Assured Framework REST Assured Framework Tutorial Table of Contents The table of contents is not clickable, only for displaying the structure.\nQuick Start Project for RestAssured API Test Introduction to RestAssured Project Structure Versions for Gradle Build Versions for Maven Build Project Dependencies Building REST Assured API Test Project from 0 to 1 Gradle Version Maven Version Advanced Usage Verify Response Data File Upload Logging Filters Continuous Integration Integrate with GitHub Action Integrate Allure Test Report Data-Driven Multi-Environment Support Corresponding Articles for REST Assured Framework Tutorial REST Assured API Test Tutorial: Advanced Usage - Integration with CI/CD and Allure Report:https://naodeng.tech/en/posts/api-automation-testing/rest-assured-tutorial-advance-usage-integration-ci-cd-and-allure-report/ REST Assured API Test Tutorial: Advanced Usage - Verify Response and Logging, Filters, File Upload:https://naodeng.tech/en/posts/api-automation-testing/rest-assured-tutorial-advance-usage-verifying-response-and-logging/ REST Assured API Test Tutorial: Building Your Own Project from 0 to 1:https://naodeng.tech/en/posts/api-automation-testing/rest-assured-tutorial-building-your-own-project-from-0-to-1/ REST Assured API Test Tutorial: Introduction and Environment Setup Preparation:https://naodeng.tech/en/posts/api-automation-testing/rest-assured-tutorial-and-environment-preparation/ Reference Documents for REST Assured Framework Tutorial Demo Project Repository: RestAssured-API-Test-Starterhttps://github.com/Automation-Test-Starter/RestAssured-API-Test-Starter/ Rest Assured Official Documentation: https://rest-assured.io/ Rest Assured Official GitHub: https://github.com/rest-assured/rest-assured Rest Assured Official Chinese Translation: https://github.com/RookieTester/rest-assured-doc Allure Documentation: https://docs.qameta.io/allure/ GitHub Action Documentation: https://docs.github.com/en/actions Implementation of Interface Automation Projects with JavaScript and SuperTest Framework SuperTest Framework Tutorial Table of Contents The table of contents is not clickable, only for displaying the structure.\nQuick Start Project for SuperTest API Test Introduction Project Dependencies Project File Structure Building SuperTest API Test Project from 0 to 1 Mocha Version Jest Version Advanced Usage Continuous Integration Integrate with GitHub Action Common Assertions Built-in Assertions in SuperTest Common Assertions in CHAI Common Assertions in Jest Data-Driven Multi-Environment Support Corresponding Articles for SuperTest Framework Tutorial SuperTest API Test Tutorial: Advanced Usage - Multi-Environment Support:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-advance-usage-multiple-environment-support/ SuperTest API Test Tutorial: Advanced Usage - Data-Driven:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-advance-usage-data-driven/ SuperTest API Test Tutorial: Advanced Usage - Common Assertions:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-advance-usage-common-assertions/ SuperTest API Test Tutorial: Advanced Usage - Integration with CI/CD and GitHub Action:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-advance-usage-integration-ci-cd-and-github-action/ SuperTest API Test Tutorial: Building Your Own Project from 0 to 1:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-building-your-own-project-from-0-to-1/ SuperTest API Test Tutorial: Getting Started and Own Environment Preparation:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-getting-started-and-own-environment-preparation/ Reference Documents for SuperTest Framework Tutorial Demo Project Repository: SuperTest-API-Test-Starterhttps://github.com/Automation-Test-Starter/SuperTest-API-Test-Starter\nSuperTest Documentation: https://github.com/ladjs/supertest\nJest Documentation: https://jestjs.io/docs/en/getting-started\nMocha Documentation: https://mochajs.org/\nChai Documentation: https://www.chaijs.com/\nAllure Documentation: https://docs.qameta.io/allure/\nGitHub Action Documentation: https://docs.github.com/en/actions\nImplementation of Interface Automation Projects with Python and Pytest Framework Pytest Framework Tutorial Table of Contents The table of contents is not clickable, only for displaying the structure.\nQuick Start Project for Pytest API Test Introduction Introduction to Pytest Introduction to Python Virtual Environment Project Dependencies Project Directory Structure Building Pytest API Test Project from 0 to 1 Advanced Usage Continuous Integration Integrate with GitHub Action Common Assertions Data-Driven Multi-Environment Support and Integration with Allure Report Concurrent Testing and Distributed Testing Filtering Test Case Execution Corresponding Articles for Pytest Framework Tutorial Pytest API Test Tutorial: Advanced Usage - Filtering Test Case Execution, Concurrent Testing, and Distributed Testing:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-advance-usage-filter-testcase-and-concurrent-testing-distributed-testing/ Pytest API Test Tutorial: Advanced Usage - Multi-Environment Support and Integration with Allure Report:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-advance-usage-multiple-environment-support-and-integration-allure-report/ Pytest API Test Tutorial: Advanced Usage - Common Assertions and Data-Driven:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-advance-usage-common-assertions-and-data-driven/ Pytest API Test Tutorial: Advanced Usage - Integration with CI/CD and GitHub Action:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-advance-usage-integration-ci-cd-and-github-action/ Pytest API Test Tutorial: Building Your Own Project from 0 to 1:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-building-your-own-project-from-0-to-1/ Pytest API Test Tutorial: Getting Started and Own Environment Preparation:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-getting-started-and-own-environment-preparation/ Reference Documents for Pytest Framework Tutorial Demo Project Repository: Pytest-API-Test-Starter Pytest Documentation: https://docs.pytest.org/en/stable/ Pytest-html Documentation: https://pypi.org/project/pytest-html/ Pytest-xdist Documentation: https://pypi.org/project/pytest-xdist/ Allure-pytest Documentation: https://pypi.org/project/allure-pytest/ Allure Documentation: https://docs.qameta.io/allure/ GitHub Action Documentation: https://docs.github.com/en/actions Certainly! The provided translation retains the original markdown format:\nImplementation of Interface Automation Testing with Testing Tools Postman API Automation Testing Postman Framework Tutorial Directory The directory is not clickable, only for displaying the structure\nImplementation of Interface Automation Projects with Java and REST Assured Framework REST Assured Framework Tutorial Table of Contents Corresponding Articles for REST Assured Framework Tutorial Reference Documents for REST Assured Framework Tutorial Implementation of Interface Automation Projects with JavaScript and SuperTest Framework SuperTest Framework Tutorial Table of Contents Corresponding Articles for SuperTest Framework Tutorial Reference Documents for SuperTest Framework Tutorial Implementation of Interface Automation Projects with Python and Pytest Framework Pytest Framework Tutorial Table of Contents Corresponding Articles for Pytest Framework Tutorial Reference Documents for Pytest Framework Tutorial Implementation of Interface Automation Testing with Testing Tools Postman API Automation Testing Postman Framework Tutorial Directory Postman Framework Tutorial Articles Postman Framework Tutorial Reference Documents Bruno API Automation Testing Bruno Framework Tutorial Directory Bruno Framework Tutorial Articles Bruno Framework Tutorial Reference Documents Recommended Reading Postman Framework Tutorial Articles Postman API Automation Testing Tutorial: Advanced Usage - Common Command Line Options, File Upload Scenarios, and SSL Certificate Scenarios: https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-advance-usage-common-command-line-options-and-file-upload/ Postman API Automation Testing Tutorial: Advanced Usage - Data-Driven:https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-advance-usage-data-driven-and-environment-data-driven/ Postman API Automation Testing Tutorial: Advanced Usage - Common Test Scripts and Examples of Commonly Used Third-Party Packages: https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-advance-usage-common-test-scripts-and-commonly-used-third-party-packages/ Postman API Automation Testing Tutorial: Advanced Usage - Integration with CI/CD and GitHub Action, Allure Report: https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-advance-usage-integration-html-report-and-allure-report-integration-github-action/ Postman API Automation Testing Tutorial: Getting Started and Building Your Own Project from 0 to 1: https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-getting-started-and-building-your-own-project-from-0-to-1/ Postman Framework Tutorial Reference Documents Demo Project Repository: Link Postman Official Documentation: Link Newman Official Documentation: Link GitHub Action Documentation: Link Allure Documentation: Link Bruno API Automation Testing Bruno Framework Tutorial Directory The directory is not clickable, only for displaying the structure\nbruno-user-guide Why Choose Bruno Installing Bruno Getting Started with the Client Default Main Interface API Request Collections API Requests Writing API Request Test Scripts Environment Variables API Script Interface Automation Preconditions Demo of API Automation Project Integration with CI Integration with GitHub Action Migration from Postman Scripts API Request Collection Migration Environment Variable Migration Reference for Test Script Migration Bruno Framework Tutorial Articles Introduction to Bruno, a Postman Replacement Tool: https://naodeng.tech/zh/posts/api-automation-testing/introduction_of_bruno/ Bruno Framework Tutorial Reference Documents Demo Project Repository: https://github.com/Automation-Test-Starter/Bruno-API-Test-Starter Bruno Documentation: https://docs.usebruno.com/ GitHub Action Documentation: https://docs.github.com/en/actions Recommended Reading Quick Start Series for API Automation Testing Using Postman Quick Start Series for API Automation Testing Using Pytest Quick Start Series for API Automation Testing Using SuperTest Quick Start Series for API Automation Testing Using Rest Assured Quick Start Series for Performance Testing Using Gatling ","permalink":"https://naodeng.tech/posts/api-automation-testing/a-collection-of-tutorials-on-api-automation-testing-for-different-frameworks-and-different-development-languages/","summary":"This blog post compiles tutorials on API automation testing using various frameworks and programming languages, providing readers with comprehensive learning resources. It covers a range of popular testing frameworks and programming languages, enabling you to choose the best solution for your project. Whether you\u0026rsquo;re a developer in Python, Java, JavaScript, or any other language, and whether you prefer using REST Assured, SuperTest, or other frameworks, this collection will offer you in-depth learning guides to help you navigate the field of interface automation testing with ease. A must-read resource to master the various tools and techniques in API automation testing.","title":"A collection of tutorials on API automation testing for different frameworks and different development languages"},{"content":"Advanced Usage This section will cover some advanced usages of Postman and Newman, including testing data, testing scripts, testing reports, and report integration. It will also explain how to integrate Postman and Newman into the CI/CD process for automated testing.\nGenerating HTML Test Reports Using the newman-reporter-htmlextra as an example, the demo will illustrate how to generate HTML test reports.\nInstalling the newman-reporter-htmlextra Dependency npm install newman-reporter-htmlextra --save-dev Note: Currently, there are compatibility issues with some packages in the latest version (V6) of Newman regarding HTML test reports. Therefore, version 5.1.2 is used here.\nAdjusting package.json In the package.json file, update the test script to run test cases and generate HTML test reports:\n\u0026#34;test\u0026#34;: \u0026#34;newman run Testcase/demo.postman_collection.json -e Env/DemoEnv.postman_environment.json -r htmlextra --reporter-htmlextra-export ./Report/Postman-newman-demo-api-testing-report.html\u0026#34; Specify the path for the HTML test report output as Report/Postman-newman-demo-api-testing-report.html\nRun Test Cases to Generate HTML Report Run the test cases npm run test Check the Report folder, you will find that a Postman-newman-demo-api-testing-report.html file has been generated. Open the Postman-newman-demo-api-testing-report.html file in a browser to view the HTML test report. Generating Reports in Multiple Formats The previous configuration is for generating HTML-format test reports. If you want to output reports in multiple formats, such as the command line (CLI) report, add the following script to the package.json file:\n\u0026#34;test\u0026#34;: \u0026#34;newman run Testcase/demo.postman_collection.json -e Env/DemoEnv.postman_environment.json -r cli,htmlextra --reporter-htmlextra-export ./Report/Postman-newman-demo-api-testing-report.html\u0026#34; Run the test cases again, and you will find both HTML and CLI format test reports in the Report folder.\nContinuous Integration (CI) with CI/CD Integrating API automation test code into the CI/CD process enables automated testing, improving testing efficiency.\nIntegrating with GitHub Actions Taking GitHub Actions as an example, similar steps can be followed for other CI tools.\nRefer to the demo: Postman-Newman-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory named .github/workflows. This will be the place to store GitHub Actions workflow files.\nCreate the workflow file: In the .github/workflows directory, create a YAML-formatted workflow file, for example, postman.yml.\nEdit the postman.yml file: Copy and paste the following content into the file:\nname: RUN Postman API Test CI on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: RUN-Postman-API-Test: runs-on: ubuntu-latest strategy: matrix: node-version: [ 18.x] # See supported Node.js release schedule at https://nodejs.org/en/about/releases/ steps: - uses: actions/checkout@v3 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v3 with: node-version: ${{ matrix.node-version }} cache: \u0026#39;npm\u0026#39; - name: Installation of related packages run: npm ci - name: RUN SuperTest API Testing run: npm test - name: Archive Postman test report uses: actions/upload-artifact@v3 with: name: Postman-test-report path: Report - name: Upload Postman report to GitHub uses: actions/upload-artifact@v3 with: name: Postman-test-report path: Report Commit your code: Add the postman.yml file to the repository and commit the changes. View the test report: In GitHub, navigate to your repository. Click on the Actions tab at the top and then click on the RUN-Postman-API-Test workflow on the left. You should see the workflow running, and once it completes, you can view the results. Integrating Allure Test Report Allure is a lightweight, flexible, and multi-language-supported test reporting tool that can generate various types of test reports, including pie charts, bar charts, line charts, etc., making it easy to visualize test results.\nInstalling Allure Test Report Dependencies npm install newman-reporter-allure --save-dev Adjusting the Script in package.json for Generating Allure Test Reports \u0026#34;test\u0026#34;: \u0026#34;newman run Testcase/demo.postman_collection.json -e Env/DemoEnv.postman_environment.json -r cli,allure --reporter-allure-export ./allure-results\u0026#34; Adjusting Postman Test Cases Modify the Tests script in the \u0026ldquo;get-demo\u0026rdquo; request. Add the following script to generate Allure test reports: // @allure.label.suite=postman-new-api-testing-demo // @allure.label.story=\u0026#34;Verify-the-get-api-return-correct-data\u0026#34; // @allure.label.owner=\u0026#34;naodeng\u0026#34; // @allure.label.tag=\u0026#34;GETAPI\u0026#34; pm.test(\u0026#34;res.status should be 200\u0026#34;, function () { pm.response.to.have.status(200); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(1); pm.expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); Adjust the Tests script in the \u0026ldquo;post-demo\u0026rdquo; request. Add the following script to generate Allure test reports: // @allure.label.suite=postman-new-api-testing-demo // @allure.label.story=\u0026#34;Verify-the-post-api-return-correct-data\u0026#34; // @allure.label.owner=\u0026#34;naodeng\u0026#34; // @allure.label.tag=\u0026#34;POSTAPI\u0026#34; pm.test(\u0026#34;res.status should be 201\u0026#34;, function () { pm.response.to.have.status(201); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(101); pm.expect(data.title).to.equal(\u0026#39;foo\u0026#39;); }); Save the modified Postman test cases, export the test case file again, and replace the original test case file. Run Test Cases to Generate Allure Report Run the test cases npm run test The allure-results folder will be generated in the project folder, containing the execution results of the test cases.\nPreviewing the Allure Test Report allure serve Reference Postman docs newman docs newman-reporter-htmlextra newman-reporter-allure github action docs ","permalink":"https://naodeng.tech/posts/api-automation-testing/postman-tutorial-advance-usage-integration-html-report-and-allure-report-integration-github-action/","summary":"This advanced guide focuses on the integration of Postman API automation testing with CI/CD and GitHub Actions, along with the incorporation of Allure test reports. Learn how to seamlessly integrate Postman tests into the CI/CD process, achieving automated testing through GitHub Actions. Additionally, understand how to integrate the Allure test report framework to generate detailed test result reports.","title":"Postman API Automation Testing Tutorial Advance Usage Integration CI CD and allure test report"},{"content":"Introduction Introduction to API Testing What is API? API, which stands for Application Programming Interface, is a computing interface that defines the interactions between multiple software intermediaries. It specifies the types of calls or requests that can be made, how they are made, the data format to be used, and the conventions to be followed. APIs can also provide extension mechanisms, allowing users to extend existing functionalities in various ways. An API can be custom-made for a specific component or designed based on industry standards to ensure interoperability. By hiding information, APIs enable modular programming, allowing users to work independently using interfaces.\nWhat is API Testing? API testing is a type of software testing that includes two types: specifically testing the functionality of Application Programming Interfaces (referred to as API) and, more broadly, testing the overall functionality, reliability, security, and performance in integration testing by invoking APIs.\nAPI Best Practice:\nAPI definition follows the RESTful API style, with semantic URI definitions, accurate HTTP status codes, and the ability to understand the relationships between resources through API definitions. Detailed and accurate API documentation (such as Swagger documentation). External APIs may include version numbers for quick iteration (e.g., https://thoughtworks.com/v1/users/). Testing in different quadrants of the testing pyramid has different purposes and strategies. API testing mainly resides in the second and fourth quadrants.\nAPI testing holds a relatively high position in the testing pyramid, focusing on testing functionality and business logic at the boundaries of systems and services. It is executed after the service is built and deployed in the testing environment for validation.\nTypes of API Testing Functional Testing\nCorrectness Testing Exception Handling Internal Logic \u0026hellip; Non-functional Testing\nPerformance Security \u0026hellip; Steps in API Testing Send Request Get Response Verify Response Result Introduction to Postman and Newman Postman is a popular API development tool that provides an easy-to-use graphical interface for creating, testing, and debugging APIs. Postman also features the ability to easily write and share test scripts. It supports various HTTP request methods, including GET, POST, PUT, DELETE, etc., and can use various authentication and authorization methods for API testing.\nNewman is the command-line tool for Postman, used to run test suites without using the Postman GUI. With Newman, users can easily export Postman collections as an executable file and run them in any environment. Additionally, Newman supports generating test reports in HTML or Junit format and integrating into CI/CD pipelines for automated testing.\nIn summary, Postman is a powerful API development and testing tool, while Newman is a convenient command-line tool for running test suites without using the Postman GUI. Their combination enhances the efficiency and accuracy of API testing and development.\nIn addition to basic functionalities, Postman has the following features:\nEnvironment and Variable Management: Postman supports switching between different environments, such as development, testing, and production, and variable management, making it easy to set variables for different test cases and requests. Automated Testing: Users can create and run automated tests using Postman, integrating them into continuous integration or deployment processes for more accurate and efficient testing. Collaboration and Sharing: Postman supports sharing collections and environments with teams, facilitating collaboration among team members. Monitoring: Postman provides API monitoring, allowing real-time monitoring of API availability and performance. Meanwhile, Newman has the following characteristics:\nCommand-Line Interface: Newman can run in the command line, making it convenient for automated testing and integration into CI/CD processes. Support for Multiple Output Formats: Newman supports multiple output formats, including HTML, JSON, and JUnit formats, making it easy to use in different scenarios. Concurrent Execution: Newman supports concurrent test execution, improving testing efficiency. Lightweight: Compared to the Postman GUI, Newman is a lightweight tool, requiring fewer resources during test execution. In conclusion, Postman and Newman are essential tools for modern API testing, offering powerful features for efficient, accurate, and automated API testing and development.\nIn addition to the mentioned features and characteristics, Postman and Newman have other important functionalities and advantages:\nIntegration: Postman and Newman can integrate with many other tools and services, such as GitHub, Jenkins, Slack, etc., making it easy to integrate into development and deployment processes for more efficient API development and testing. Documentation Generation: Postman can generate API documentation using requests and responses, ensuring accurate and timely documentation. Test Scripts: Postman can use JavaScript to write test scripts, providing flexibility and customization in testing. Users can easily write custom test scripts to ensure the expected behavior of the API. History: Postman can store the history of API requests, making it convenient for users to view and manage previous requests and responses. This is useful for debugging and issue troubleshooting. Multi-Platform Support: Postman and Newman can run on multiple platforms, including Windows, MacOS, and Linux. In summary, Postman and Newman are powerful tools for modern API testing and development, offering rich features and flexible test scripts to help developers and testers build and test APIs faster and more accurately.\nProject Dependencies The following environments need to be installed in advance\nNode.js, with the demo version being v21.1.0 Postman installed, you can download the installation package from the official website and complete the installation Project Structure The following is the file structure of an API automation testing project for Postman and Newman, which contains test configuration files, test case files, test tool files, and test report files. It can be used for reference.\nPostman-Newman-demo ├── README.md ├── package.json ├── package-lock.json ├── Data // Test data folder │ └── testdata.csv // Test data file ├── Testcase // Test case folder │ └── APITestDemo.postman_collection.json // Test case file ├── Env // Test environment folder │ └── DemoEnv.postman_environment.json // Test environment file ├── Report // Test report folder │ └── report.html ├── .gitignore └── node_modules // Project dependencies └── ... Building a Postman API Automation Test Project from 0 to 1 Below, we will introduce how to build a Postman and Newman API automation test project from scratch, including test configuration, test cases, test environment, testing tools, and test reports.\nYou can refer to the demo project: Postman-Newman-demo\nCreate a New Project Folder mkdir Postman-Newman-demo Project initialization // enter the project folder cd Postman-Newman-demo // nodejs project initialization npm init -y Install dependencies Currently, the latest version of newman has some package compatibility issues reported by the html test, so we\u0026rsquo;re using version 5.1.2 here.\n// Install newman library npm install newman@5.1.2--save-dev Writing API Test Cases in Postman Creating a Collection and Request in Postman Open Postman, click the New button in the top left corner, select Collection, enter the name of the collection, click the Create Collection button to create a collection named \u0026ldquo;demo.\u0026rdquo; In the collection, click the three dots in the top right corner, select Add Request, enter the name of the request, and click the Save button to create a request named \u0026ldquo;get-demo.\u0026rdquo; Add another request named \u0026ldquo;post-demo.\u0026rdquo; Editing Request and Writing Test Cases Refer to the interface documentation in the demoAPI.md file in the project folder to obtain information such as the URL, request method, request headers, and request body used by the \u0026ldquo;demo\u0026rdquo; requests.\nget-demo In the \u0026ldquo;get-demo\u0026rdquo; request, select the GET request method and enter the URL as https://jsonplaceholder.typicode.com/posts/1. In the Headers section, add a header with Key as \u0026ldquo;Content-Type\u0026rdquo; and Value as \u0026ldquo;application/json.\u0026rdquo; Under Tests, add the following script to verify the response result: pm.test(\u0026#34;res.status should be 200\u0026#34;, function () { pm.response.to.have.status(200); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(1); pm.expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); Click the Send button to send the request and verify the response result. Confirm that the response result is correct, click the Save button to save the request.\npost-demo In the Request of the post-demo, select the POST request method and enter the URL as https://jsonplaceholder.typicode.com/posts. In Headers, add a request header with Key as Content-Type and Value as application/json. In Body, select raw, select JSON format, and enter the following request body: { \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } Under Tests, add the following script to verify the response result: pm.test(\u0026#34;res.status should be 201\u0026#34;, function () { pm.response.to.have.status(201); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(101); pm.expect(data.title).to.equal(\u0026#39;foo\u0026#39;); }); Confirm that the response result is correct, click the Save button to save the request.\nConfiguring Test Environment in Postman The following steps involve using the host of the API requests as environment variables for demonstration purposes.\nAdding Environment Variables In the top right corner of Postman, click the gear icon, select Manage Environments, click the Add button, enter the environment name as \u0026ldquo;DemoEnv,\u0026rdquo; and click the Add button to create an environment named \u0026ldquo;DemoEnv.\u0026rdquo; Edit the environment variables, add a key named \u0026ldquo;host\u0026rdquo; with a value of https://jsonplaceholder.typicode.com. Click the Add button to save the environment variables. Updating Requests In the \u0026ldquo;get-demo\u0026rdquo; request, update the URL to {{host}}/posts/1. In the \u0026ldquo;post-demo\u0026rdquo; request, update the URL to {{host}}/posts. Verifying Environment Variables In the top right corner of Postman, click the gear icon, select DemoEnv to switch to the \u0026ldquo;DemoEnv\u0026rdquo; environment. Select the \u0026ldquo;get-demo\u0026rdquo; request, click the Send button to send the request, and verify the response result. After confirming the correctness of the response, click the Save button to save the request. Select the \u0026ldquo;post-demo\u0026rdquo; request, click the Send button to send the request, and verify the response result. After confirming the correctness of the response, click the Save button to save the request. Exporting Environment Variables and Test Case Files In the top right corner of Postman, click the gear icon, select Export, choose DemoEnv, and click the Export button to export the environment variables. Select the demo Collection containing the \u0026ldquo;get-demo\u0026rdquo; and \u0026ldquo;post-demo\u0026rdquo; requests, click the three dots in the top right corner, select Export, choose Collection v2.1, and click the Export button to export the test case file. Adjusting Project File Structure Creating Env and Testcase Folders In the project folder, create a folder named Env to store environment variable files. // Create Env folder mkdir Env In the project folder, create a folder named Testcase to store test case files. // Create Testcase folder mkdir Testcase Organizing Case and Environment Files\nPlace the exported environment variable files and test case files into the Env and Testcase folders within the project folder.\nAdjusting the package.json file In the package.json file, add the following script to run the test cases: \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;newman run Testcase/demo.postman_collection.json -e Env/DemoEnv.postman_environment.json\u0026#34; } Running Test Cases npm run test Reference Postman docs newman docs ","permalink":"https://naodeng.tech/posts/api-automation-testing/postman-tutorial-getting-started-and-building-your-own-project-from-0-to-1/","summary":"This guide provides a comprehensive introduction to getting started with Postman API automation testing, covering both the basics and the step-by-step process of building a project from scratch. Learn how to effectively use Postman for API testing, understand the foundational structure of project setup, environment configuration, and writing test cases from the ground up.","title":"Postman API Automation Testing Tutorial: Getting Started and Building a Postman API Automation Test project from 0 to 1"},{"content":"Advanced Usage concurrent testing and distributed testing In the daily process of API automation testing, concurrent execution of test cases is required to improve testing efficiency.\nSometimes it is also necessary to introduce distributed testing in order to run test cases on multiple machines at the same time, which can also better improve testing efficiency.\npytest-xdist is a plugin for Pytest that provides some corresponding functionality, mainly for supporting concurrent and distributed testing.\npytest-xdist Feature Introduction Concurrently run tests:\nUse the -n option: pytest -n NUM allows running tests concurrently, where NUM is the number of concurrent workers. This can speed up test execution, especially on computers with multiple CPU cores. pytest -n 3 # Start 3 concurrent workers to execute the test Distributed testing:\nUse pytest --dist=loadscope: allows tests to be executed on multiple nodes and test runs can be completed faster with distributed testing. pytest --dist=loadscope Use pytest --dist=each: run a set of tests per node, for distributed testing. pytest --dist=each Parameterized tests and Concurrency:\nUse of pytest.mark.run: In conjunction with the pytest.mark.run tag, tests with different tags can optionally be run on different processes or nodes. @pytest.mark.run(processes=2) def test_example(): pass Distributed environment setup:\nUse pytest_configure_node: you can configure the tests before running them on the node. def pytest_configure_node(node): node.slaveinput[\u0026#39;my_option\u0026#39;] = \u0026#39;some value\u0026#39; Use pytest_configure_node: you can configure the tests before running them on the node. def pytest_configure_node(node): node.slaveinput[\u0026#39;my_option\u0026#39;] = \u0026#39;some value\u0026#39; Distributed test environment destruction:\nUse pytest_configure_node: you can clean up after running tests on a node. def pytest_configure_node(node): # Configure the node yield # Perform cleanup after running tests on nodes print(\u0026#34;Cleaning up after test run on node %s\u0026#34; % node.gateway.id) These are some of the features provided by pytest-xdist that can help you perform concurrent and distributed tests more efficiently to speed up test execution and increase efficiency. Be sure to consult the pytest-xdist documentation for more detailed information and usage examples before using it.\nInstalling pytest-xdist dependency pip install pytest-xdist Example of running a test case concurrently Execute test cases concurrently with 3 workers Run the following commands to see how long the test cases take to execute\nConcurrent Execution pytest -n 3 Default Parallel Execution pytest Parallel execution took 9.81s while Concurrent execution took 1.63s, you can see that concurrent execution of test cases can greatly improve the Parallel of testing.\nconcurrently executes the test cases with 3 workers, and each worker prints the progress of the test cases pytest -n 3 -v The progress of the test is printed in the test results, which provides a better understanding of the execution of the test cases.\nDistributed testing example Distributed test where each node runs a set of tests pytest --dist=each Distributed testing allows for faster test runs.\nDistributed testing, where each node runs a set of tests and each worker prints the progress of the test cases pytest --dist=each -v The progress of the test will be printed in the test results, so you can better understand the execution of the test cases.\nDistributed testing, each node runs a set of tests, and each worker prints the progress of the test cases, as well as the output of the test logs pytest --dist=each -v --capture=no The output of the test log is printed in the test results, which gives a better understanding of the execution of the test cases.\nFiltering test case execution In the daily API testing process, we need to selectively execute test cases according to the actual situation in order to improve the testing efficiency.\nGenerally, when we use allure test reports, we can use the Allure tag feature to filter the use cases corresponding to the tag to execute the test, but the Pytest framework does not directly support running tests based on Allure tags. However, the Pytest framework does not directly support running tests based on Allure tags, so you can use Pytest markers to accomplish this.\nPytest provides a marks tagging feature that can be used to tag different types of test cases and then filter them for execution.\nThe general process is that you can mark tests with custom markers (e.g. Regression/Smoke) and then use pytest\u0026rsquo;s -m option to run only those tests.\nDefining Pytest Markers Edit the pytest.ini file and add the following: customize the type of markers\nRegression: Marks the use case for regression testing. Smoke: mark it as a use case for smoke testing markers = Regression: marks tests as Regression Smoke: marks tests as Smoke Marking Test Cases The operation steps are:\nIntroduce pytest Mark the test case with @pytest.mark. To differentiate, create a new test case file named test_demo_filter.py.\nimport pytest import requests import json class TestPytestMultiEnvDemo: @pytest.mark.Regression # mark the test case as regression def test_get_demo_filter(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] get_api = env_config[\u0026#34;getAPI\u0026#34;] get_api_response_data = env_response_data[\u0026#34;getAPI\u0026#34;] # send request response = requests.get(host+get_api) # assert assert response.status_code == 200 assert response.json() == get_api_response_data @pytest.mark.Smoke # mark the test case as smoke def test_post_demo_filter(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] post_api = env_config[\u0026#34;postAPI\u0026#34;] post_api_request_data = env_request_data[\u0026#34;postAPI\u0026#34;] print(\u0026#34;make the request\u0026#34;) post_api_response_data = env_response_data[\u0026#34;postAPI\u0026#34;] # Your test code here response = requests.post(host + post_api, json=post_api_request_data) print(\u0026#34;verify the response status code\u0026#34;) assert response.status_code == 201 print(\u0026#34;verify the response data\u0026#34;) assert response.json() == post_api_response_data Filtering Test Case Execution Running Regression-tagged test cases pytest -m Regression This command tells pytest to run only the tests labeled Regression.\nRunning Smoke-tagged test cases pytest -m Smoke This command tells pytest to run only the tests labeled Smoke.\nreference pytest-xdist docs:https://pytest-xdist.readthedocs.io/en/stable/ pytest makers docs:https://docs.pytest.org/en/6.2.x/example/markers.html pytest docs:https://docs.pytest.org/en/6.2.x/ ","permalink":"https://naodeng.tech/posts/api-automation-testing/pytest-tutorial-advance-usage-filter-testcase-and-concurrent-testing-distributed-testing/","summary":"Focus on test case screening, concurrency testing and distributed testing. Learn how to execute test cases in a targeted manner to improve testing efficiency. Explore Pytest concurrent testing features and learn how to execute multiple test cases at the same time to reduce testing time.","title":"Pytest API Automation Testing Tutorial Advance Usage Filtering test case execution and Concurrent testing"},{"content":"Advanced Usage Multi-environment support In the actual API automation testing process, we need to run test cases in different environments to ensure that the API works properly in each environment.\nBy using Pytest\u0026rsquo;s fixture feature, we can easily support multiple environments.\nRefer to the demo:https://github.com/Automation-Test-Starter/Pytest-API-Test-Demo\nNew test configuration files for different environments Configuration file will be stored in json format for example, other formats such as YAML, CSV, etc. are similar, can refer to the\n// Create a new test configuration folder mkdir config // Go to the test configuration folder cd config // Create a new test configuration file for the development environment touch dev_config.json // Create a new test configuration file for the production environment touch prod_config.json Writing different environment test profiles Writing Development Environment Test Profiles Configure the development environment test profiles according to the actual situation.\n{ \u0026#34;host\u0026#34;: \u0026#34;https://jsonplaceholder.typicode.com\u0026#34;, \u0026#34;getAPI\u0026#34;: \u0026#34;/posts/1\u0026#34;, \u0026#34;postAPI\u0026#34;:\u0026#34;/posts\u0026#34; } Configuring Production Environment Test Profiles Configure production environment test profiles according to the actual situation\n{ \u0026#34;host\u0026#34;: \u0026#34;https://jsonplaceholder.typicode.com\u0026#34;, \u0026#34;getAPI\u0026#34;: \u0026#34;/posts/1\u0026#34;, \u0026#34;postAPI\u0026#34;:\u0026#34;/posts\u0026#34; } New Different Environment Test Data File The different environments request data file and the response data file store the different environments request data and the different environments expected response data for the test cases, respectively.\n// Create a new test data folder mkdir data // Go to the test data folder cd data // Create a new dev request data file touch dev_request_data.json // Create a new dev response data file touch dev_response_data.json // Create a new request data file for the production environment touch prod_request_data.json // Create a new production response data file touch prod_response_data.json Writing test data files for different environments Write the dev environment request data file The dev environment request data file is configured with the request data for the getAPI API and the request data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } } Writing the dev Environment Response Data File The dev environment response data file is configured with the response data for the getAPI API and the response data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 } } Write the prod environment request data file The prod environment request data file is configured with the request data for the getAPI API and the request data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } } Writing the prod Environment Response Data File The prod environment response data file is configured with the response data for the getAPI API and the response data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 } } Configure fixture to support multiple environments The \u0026gt; fixture will be stored in the conftest.py file as an example, other formats such as YAML, CSV, etc. are similar.\nCreate a new conftest.py file in the project root directory. mkdrir conftest.py Writing the conftest.py file import pytest import json import json import os @pytest.fixture(scope=\u0026#34;session\u0026#34;) def env_config(request): # get config file from different env env = os.getenv(\u0026#39;ENV\u0026#39;, \u0026#39;dev\u0026#39;) with open(f\u0026#39;config/{env}_config.json\u0026#39;, \u0026#39;r\u0026#39;) as config_file: config = json.load(config_file) return config @pytest.fixture(scope=\u0026#34;session\u0026#34;) def env_request_data(request): # get request data file from different env env = os.getenv(\u0026#39;ENV\u0026#39;, \u0026#39;dev\u0026#39;) with open(f\u0026#39;data/{env}_request_data.json\u0026#39;, \u0026#39;r\u0026#39;) as request_data_file: request_data = json.load(request_data_file) return request_data @pytest.fixture (scope=\u0026#34;session\u0026#34;) def env_response_data(request): # get response data file from different env env = os.getenv(\u0026#39;ENV\u0026#39;, \u0026#39;dev\u0026#39;) with open(f\u0026#39;data/{env}_response_data.json\u0026#39;, \u0026#39;r\u0026#39;) as response_data_file: response_data = json.load(response_data_file) return response_data Update test case to support multi environment To make a distinction, here is a new test case file named test_demo_multi_environment.py\nimport requests import json class TestPytestMultiEnvDemo: def test_get_demo_multi_env(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] get_api = env_config[\u0026#34;getAPI\u0026#34;] get_api_response_data = env_response_data[\u0026#34;getAPI\u0026#34;] # send request response = requests.get(host+get_api) # assert assert response.status_code == 200 assert response.json() == get_api_response_data def test_post_demo_multi_env(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] post_api = env_config[\u0026#34;postAPI\u0026#34;] post_api_request_data = env_request_data[\u0026#34;postAPI\u0026#34;] post_api_response_data = env_response_data[\u0026#34;postAPI\u0026#34;] # send request response = requests.post(host + post_api, post_api_request_data) # assert assert response.status_code == 201 assert response.json() == post_api_response_data Run this test case to confirm that multi-environment support is in effect Run the dev environment test case ENV=dev pytest test_case/test_demo_multi_environment.py Run the prod environment test case ENV=prod pytest test_case/test_demo_multi_environment.py Integration with allure reporting allure is a lightweight, flexible, and easily extensible test reporting tool that provides a rich set of report types and features to help you better visualize your test results.\nallure reports can be integrated with Pytest to generate detailed test reports.\nRefer to the demo:https://github.com/Automation-Test-Starter/Pytest-API-Test-Demo\nInstall allure-pytest library pip install allure-pytest To avoid conflicts between the previously installed pytest-html-reporter and the allure-pytest package, it is recommended to uninstall the pytest-html-reporter package first.\npip uninstall pytest-html-reporter Configuration allure-pytest library Update the pytest.ini file to specify where allure reports are stored\n[pytest] # allure addopts = --alluredir ./allure-results Adjusting test cases to support allure reporting To differentiate, create a new test case file here, named test_demo_allure.py\nimport allure import requests @allure.feature(\u0026#34;Test example API\u0026#34;) class TestPytestAllureDemo: @allure.story(\u0026#34;Test example get endpoint\u0026#34;) @allure.title(\u0026#34;Verify the get API\u0026#34;) @allure.description(\u0026#34;verify the get API response status code and data\u0026#34;) @allure.severity(\u0026#34;blocker\u0026#34;) def test_get_example_endpoint_allure(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] get_api = env_config[\u0026#34;getAPI\u0026#34;] get_api_request_data = env_request_data[\u0026#34;getAPI\u0026#34;] get_api_response_data = env_response_data[\u0026#34;getAPI\u0026#34;] # send get request response = requests.get(host + get_api) # assert print(\u0026#34;response status code is\u0026#34; + str(response.status_code)) assert response.status_code == 200 print(\u0026#34;response data is\u0026#34; + str(response.json())) assert response.json() == get_api_response_data @allure.story(\u0026#34;Test example POST API\u0026#34;) @allure.title(\u0026#34;Verify the POST API\u0026#34;) @allure.description(\u0026#34;verify the POST API response status code and data\u0026#34;) @allure.severity(\u0026#34;Critical\u0026#34;) def test_post_example_endpoint_allure(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] post_api = env_config[\u0026#34;postAPI\u0026#34;] post_api_request_data = env_request_data[\u0026#34;postAPI\u0026#34;] post_api_response_data = env_response_data[\u0026#34;postAPI\u0026#34;] # send request response = requests.post(host + post_api, json=post_api_request_data) # assert print(\u0026#34;response status code is\u0026#34; + str(response.status_code)) assert response.status_code == 201 print(\u0026#34;response data is\u0026#34; + str(response.json())) assert response.json() == post_api_response_data Run test cases to generate allure reports ENV=dev pytest test_case/test_demo_allure.py View allure report Run the following command to view the allure report in the browser\nallure serve allure-results Adapting CI/CD processes to support allure reporting Github action is an example, other CI tools are similar.\nUpdate the contents of the .github/workflows/pytest.yml file to upload allure reports to GitHub.\nname: Pytest API Testing on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] permissions: contents: read jobs: Pytes-API-Testing: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up Python 3.10 uses: actions/setup-python@v3 with: python-version: \u0026#34;3.10\u0026#34; - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Test with pytest run: | ENV=dev pytest - name: Archive Pytest allure test report uses: actions/upload-artifact@v3 with: name: Pytest-allure-report path: allure-results - name: Upload Pytest allure report to GitHub uses: actions/upload-artifact@v3 with: name: Pytest-allure-report path: allure-results View github action allure report In GitHub, navigate to your repository. Click the Actions tab at the top, and then click the Pytest API Testing workflow on the left. You should see the workflow running, wait for the execution to complete, and then you can view the results.\nReference Pytest docs Allure docs ","permalink":"https://naodeng.tech/posts/api-automation-testing/pytest-tutorial-advance-usage-multiple-environment-support-and-integration-allure-report/","summary":"A deep dive into advanced Pytest usage, focusing on how Pytest is support multiple environment and integration allure report.","title":"Pytest API Automation Testing Tutorial Advance Usage Multiple Environment Support and Integration Allure Report"},{"content":"Advanced Usage Common Assertions Using Pytest During the writing of interface automation test cases, we need to use various assertions to verify the expected results of the tests.\nPytest provides more assertions and a flexible library of assertions to fulfill various testing needs.\nThe following are some of the commonly used Pytest interface automation test assertions:\nEquality assertion: checks whether two values are equal.\nassert actual_value == expected_value Unequality Assertion: checks if two values are not equal.\nassert actual_value != expected_value Containment assertion: checks whether a value is contained in another value, usually used to check whether a string contains a substring.\nassert substring in full_string Membership Assertion: checks whether a value is in a collection, list, or other iterable object.\nassert item in iterable Truth Assertion: checks whether an expression or variable is true.\nassert expression OR\nassert variable False Value Assertion: checks whether an expression or variable is false.\nassert not expression OR\nassert not variable Greater Than, Less Than, Greater Than Equal To, Less Than Equal To Assertion: checks whether a value is greater than, less than, greater than equal to, or less than equal to another value.\nassert value \u0026gt; other_value assert value \u0026lt; other_value assert value \u0026gt;= other_value assert value \u0026lt;= other_value Type Assertion: checks that the type of a value is as expected.\nassert isinstance(value, expected_type) For example, to check if a value is a string:\nassert isinstance(my_string, str) Exception Assertion: checks to see if a specific type of exception has been raised in a block of code.\nwith pytest.raises(ExpectedException): # Block of code that is expected to raise an ExpectedException. Approximate Equality Assertion: checks whether two floating-point numbers are equal within some margin of error.\nassert math.isclose(actual_value, expected_value, rel_tol=1e-9) List Equality Assertion: checks if two lists are equal.\nassert actual_list == expected_list Dictionary Equality Assertion: checks if two dictionaries are equal.\nassert actual_dict == expected_dict Regular Expression Match Assertion: checks if a string matches the given regular expression.\nimport re assert re.match(pattern, string) Null Assertion: checks whether a value is None。\nassert value is None Non-null value assertion: checks if a value is not None。\nassert value is not None Boolean Assertion: checks whether a value of True or False。\nassert boolean_expression Empty Container Assertion: checks if a list, collection or dictionary is empty.\nassert not container # Check if the container is empty Contains Subset Assertion: checks whether a set contains another set as a subset.\nassert subset \u0026lt;= full_set String Beginning or End Assertion: checks whether a string begins or ends with the specified prefix or suffix.\nassert string.startswith(prefix) assert string.endswith(suffix) Quantity Assertion: checks the number of elements in a list, collection, or other iterable object.\nassert len(iterable) == expected_length Range Assertion: checks if a value is within the specified range.\nassert lower_bound \u0026lt;= value \u0026lt;= upper_bound Document Existence Assertion: checking whether a document exists or not。\nimport os assert os.path.exists(file_path) These are some common Pytest assertions, but depending on your specific testing needs, you may want to use other assertions or combine multiple assertions to more fully validate your test results. Detailed documentation on assertions can be found on the official Pytest website at:Pytest - Built-in fixtures, marks, and nodes\nData-driven In the process of API automation testing. The use of data-driven is a regular testing methodology where the input data and expected output data of the test cases are stored in data files, and the testing framework executes multiple tests based on these data files to validate various aspects of the API.\nThe test data can be easily modified without modifying the test case code.\nData-driven testing helps you cover multiple scenarios efficiently and ensures that the API works properly with a variety of input data.\nRefer to the demo:https://github.com/Automation-Test-Starter/Pytest-API-Test-Demo\nCreate the test configuration file Configuration file will be stored in json format for example, other formats such as YAML, CSV, etc. are similar, can be referred to.\n// create a new config folder mkdir config // enter the config folder cd config // create a new configuration file touch config.json Writing Test Configuration Files The configuration file stores the configuration information of the test environment, such as the URL of the test environment, database connection information, and so on.\nThe contents of the test configuration file in the demo are as follows:\nConfigure host information Configure the getAPI interface information. Configure the postAPI interface information. { \u0026#34;host\u0026#34;: \u0026#34;https://jsonplaceholder.typicode.com\u0026#34;, \u0026#34;getAPI\u0026#34;: \u0026#34;/posts/1\u0026#34;, \u0026#34;postAPI\u0026#34;:\u0026#34;/posts\u0026#34; } Create the test data file The request data file and the response data file store the request data and the expected response data of the test case, respectively.\n// create a new data folder mkdir data // enter the data folder cd data // create a new request data file touch request_data.json // create a new response data file touch response_data.json Writing test data files Writing the request data file The request data file is configured with the request data for the getAPI API and the request data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } } Writing the response data file The request data file is configured with the response data for the getAPI API and the response data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 } } Updating test cases to support data driving To differentiate, here is a new test case file named test_demo_data_driving.py\nimport requests import json # get the test configuration information from the configuration file with open(\u0026#34;config/config.json\u0026#34;, \u0026#34;r\u0026#34;) as json_file: config = json.load(json_file) # get the request data from the test data file with open(\u0026#39;data/request_data.json\u0026#39;, \u0026#39;r\u0026#39;) as json_file: request_data = json.load(json_file) # get the response data from the test data file with open(\u0026#39;data/response_data.json\u0026#39;, \u0026#39;r\u0026#39;) as json_file: response_data = json.load(json_file) class TestPytestDemo: def test_get_demo(self): host = config.get(\u0026#34;host\u0026#34;) get_api = config.get(\u0026#34;getAPI\u0026#34;) get_api_response_data = response_data.get(\u0026#34;getAPI\u0026#34;) # send request response = requests.get(host+get_api) # assert assert response.status_code == 200 assert response.json() == get_api_response_data def test_post_demo(self): host = config.get(\u0026#34;host\u0026#34;) post_api = config.get(\u0026#34;postAPI\u0026#34;) post_api_request_data = request_data.get(\u0026#34;postAPI\u0026#34;) post_api_response_data = response_data.get(\u0026#34;postAPI\u0026#34;) # send request response = requests.post(host + post_api, post_api_request_data) # assert assert response.status_code == 201 assert response.json() == post_api_response_data Run the test case to confirm the data driver is working If you run the data driver support test case with demo project: test_demo_data_driving.py, it is recommended to block other test cases first, otherwise it may report errors.\npytest tests/test_demo_data_driving.py Reference Pytest docs ","permalink":"https://naodeng.tech/posts/api-automation-testing/pytest-tutorial-advance-usage-common-assertions-and-data-driven/","summary":"A deep dive into advanced Pytest usage, focusing on how Pytest is commonly asserted and data-driven.","title":"Pytest API Automation Testing Tutorial Advance Usage Common Assertions and Data Driven"},{"content":"Advanced Usage CI/CD integration Integration github action Use github action as an example, and other CI tools similarly\nSee the demo at https://github.com/Automation-Test-Starter/Pytest-API-Test-Demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as pytest.yml, in the .github/workflows directory.\nEdit the pytest.yml file: Copy the following into the file\n# This workflow will install Python dependencies, run tests and lint with a single version of Python # For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python name: Pytest API Testing on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] permissions: contents: read jobs: Pytes-API-Testing: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up Python 3.10 uses: actions/setup-python@v3 with: python-version: \u0026#34;3.10\u0026#34; - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Test with pytest run: | pytest - name: Archive Pytest test report uses: actions/upload-artifact@v3 with: name: SuperTest-test-report path: report - name: Upload Pytest report to GitHub uses: actions/upload-artifact@v3 with: name: Pytest-test-report path: report Commit the code: Add the pytest.yml file to your repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Pytest API Testing workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. reference Pytest official document: https://docs.pytest.org/en/6.2.x/contents.html gitHub action official document: https://docs.github.com/en/actions ","permalink":"https://naodeng.tech/posts/api-automation-testing/pytest-tutorial-advance-usage-integration-ci-cd-and-github-action/","summary":"dive into advanced usage of Pytest, focusing on how to integrate Pytest into a CI/CD process and how to automate tests using GitHub Actions.","title":"Pytest API Automation Testing Tutorial Advance Usage Integration CI CD and Github Action"},{"content":"Build a Pytest API Automation Test Project from 0 to 1 1. Create a project directory mkdir Pytest-API-Testing-Demo 2.Project initialization // Go to the project folder cd Pytest-API-Testing-Demo // Create the project python project virtual environment python -m venv .env // Enable the project python project virtual environment source .env/bin/activate 3.Install project dependencies // Install the requests package pip install requests // Install the pytest package pip install pytest // Save the project dependencies to the requirements.txt file. pip freeze \u0026gt; requirements.txt 4. Create new test files and test cases // Create a new tests folder mkdir tests // Create a new test case file cd tests touch test_demo.py 5. Writing Test Cases The test API can be referred to the demoAPI.md file in the project.\nimport requests class TestPytestDemo: def test_get_demo(self): base_url = \u0026#34;https://jsonplaceholder.typicode.com\u0026#34; # SEND REQUEST response = requests.get(f\u0026#34;{base_url}/posts/1\u0026#34;) # ASSERT assert response.status_code == 200 assert response.json()[\u0026#39;userId\u0026#39;] == 1 assert response.json()[\u0026#39;id\u0026#39;] == 1 def test_post_demo(self): base_url = \u0026#34;https://jsonplaceholder.typicode.com\u0026#34; requests_data = { \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } # SEND REQUEST response = requests.post(f\u0026#34;{base_url}/posts\u0026#34;, requests_data) # ASSERT assert response.status_code == 201 print(response.json()) assert response.json()[\u0026#39;userId\u0026#39;] == \u0026#39;1\u0026#39; assert response.json()[\u0026#39;id\u0026#39;] == 101 6.Run test cases pytest 7.View test report 8.Integration pytest-html-reporter test report https://github.com/prashanth-sams/pytest-html-reporter\n8.1 Install pytest-html-reporter dependency pip install pytest-html-reporter 8.2 Configuring Test Report Parameters Create a new pytest.ini file in the project root directory. Add the following [pytest] addopts = -vs -rf --html-report=./report --title=\u0026#39;PYTEST REPORT\u0026#39; --self-contained-html 8.3 Run test cases pytest 8.4 Viewing the test report The report is located in the report directory in the project root directory, use your browser to open the pytest_html_report.html file to view it.\nreference pytest: https://docs.pytest.org/en/latest/ ","permalink":"https://naodeng.tech/posts/api-automation-testing/pytest-tutorial-building-your-own-project-from-0-to-1/","summary":"dive into how to build a Pytest API automation testing project from scratch.Pytest is a popular Java library for performing REST API testing, providing powerful tools that make it easy to write automated test scripts to validate the API\u0026rsquo;sbehavior.","title":"Pytest API Automation Testing Tutorial: Building a Pytest API Automation Test project from 0 to 1"},{"content":"Introduction Introducing Pytest Pytest is a popular Python testing framework for writing, organizing, and running various types of automated tests. It provides a rich set of features that make it easy to write and manage test cases, as well as generate detailed test reports. Here are some of the key features and benefits of Pytest:\nSimple and easy to use Pytest is designed to make writing test cases simple and easy to understand. You can write test assertions using Python\u0026rsquo;s standard assert statement without having to learn a new assertion syntax.\nAutomatic Discovery of Test Cases Pytest can automatically discover and run test cases in your project without explicitly configuring the test suite. Test case files can be named test_*.py or *_test.py, or use a specific test function naming convention.\nRich plugin ecosystem Pytest can be extended with plugins. There are many third-party plug-ins available to meet different testing needs, such as Allure reporting, parameterization, coverage analysis, and so on.\nParameterized Testing Pytest supports parameterized testing, which allows you to run the same test case multiple times, but with different parameters. This reduces code duplication and improves test coverage.\nException and fault localization Pytest provides detailed error and exception information that helps you locate and resolve problems more easily. It also provides detailed traceback information.\nParallel Test Execution Pytest supports parallel execution of test cases, which increases the speed of test execution, especially in large projects.\nMultiple Report Formats Pytest supports multiple test report formats, including terminal output, JUnit XML, HTML reports and Allure reports. These reports can help you visualize test results.\nCommand Line Options Pytest provides a rich set of command line options to customize the behavior of test runs, including filtering, retrying, coverage analysis, and more.\nIntegration Pytest can be easily integrated with other testing frameworks and tools (e.g. Selenium, Django, Flask, etc.) as well as continuous integration systems (e.g. Jenkins, Travis CI, etc.).\nActive Community Pytest has an active community with extensive documentation and tutorials for learning and reference. You can also get support and solve problems in the community.\nIn short, Pytest is a powerful and flexible testing framework for projects of all sizes and types. Its ease of use, automation capabilities, and rich set of plugins make it one of the go-to tools in Python testing.\nOfficial website: https://docs.pytest.org/en/latest/\nIntroduction to python virtual environments A Python virtual environment is a mechanism for creating and managing multiple isolated development environments within a single Python installation. Virtual environments help resolve dependency conflicts between different projects by ensuring that each project can use its own independent Python packages and libraries without interfering with each other. Here are the steps on how to create and use a Python virtual environment:\nInstall the Virtual Environment Tool Before you begin, make sure you have installed Python\u0026rsquo;s virtual environment tools. In Python 3.3 and later, the venv module is built-in and can be used to create virtual environments. If you\u0026rsquo;re using an older version of Python, you can install the virtualenv tool.\nFor Python 3.3+, the venv tool is built-in and does not require additional installation.\nFor Python 2.x, you can install the virtualenv tool with the following command:\npip install virtualenv Creating a virtual environment Open a terminal, move to the directory where you wish to create the virtual environment, and run the following command to create the virtual environment:\nUse venv (for Python 3.3+):\npython -m venv myenv Use virtualenv (for Python 2.x):\nvirtualenv myenv In the above command, myenv is the name of the virtual environment and you can customize the name.\nActivate virtual environment To start using the virtual environment, you need to activate it. The activation command is slightly different for different operating systems:\non macOS and Linux: source myenv/bin/activate On Windows (using Command Prompt): myenv\\Scripts\\activate On Windows (using PowerShell): .\\myenv\\Scripts\\Activate.ps1 Once the virtual environment is activated, you will see the name of the virtual environment in front of the terminal prompt, indicating that you are in the virtual environment.\nInstalling dependencies in a virtual environment In a virtual environment, you can use pip to install any Python packages and libraries required by your project, and these dependencies will be associated with that virtual environment. Example:\npip install requests Using a virtual environment When working in a virtual environment, you can run Python scripts and use packages installed in the virtual environment. This ensures that your project runs in a separate environment and does not conflict with the global Python installation.\nExiting the virtual environment To exit the virtual environment, simply run the following command in a terminal:\ndeactivate This returns you to the global Python environment.\nBy using a virtual environment, you can maintain clean dependencies between projects and ensure project stability and isolation. This is a good practice in Python development.\nProject dependencies The following environments need to be installed in advance\npython, demo version is v3.11.6 Just install python 3.x or higher.\nProject directory structure The following is an example of the directory structure of a Pytest interface automation test project:\nSubsequent demo projects will introduce allure reports, so there will be an additional allure-report directory.\nPytest-allure-demo/ ├── tests/ # test case files │ ├── test_login.py # Example test case file │ ├── test_order.py # Example test case file │ └── ... ├── data/ # test data files (e.g. JSON, CSV, etc.) │ ├── dev_test_data.json # Test data file for development environment. │ ├── prod_test_data.json # Test data file for prod environment. │ ├── ... ├── config/ │ ├── dev_config.json # Development environment configuration file │ ├── prod_config.json # Production environment configuration file │ ├── ... ├── conftest.py # Pytest\u0026#39;s global configuration file ├── pytest.ini # Pytest configuration file ├── requirements.txt # Project dependencies file └── allure-report/ # Allure reports reference pytest: https://docs.pytest.org/en/latest/ ","permalink":"https://naodeng.tech/posts/api-automation-testing/pytest-tutorial-getting-started-and-own-environment-preparation/","summary":"a tutorial on Pytest, focusing on getting started and preparing the environment to be built.","title":"Pytest API Automation Testing Tutorial: Getting Started and Setting Up Your Environment"},{"content":"Multiple Environment Support When using Jest or Mocha for API testing, you may need to support testing different environments, such as development, test and production environments. This can be achieved by configuring different test scripts and environment variables.\nThe following is a brief description of how to configure multi-environment support in Jest and Mocha, with a demo demonstrating support for two environments.\nMocha version can be found in the demo project: https://github.com/Automation-Test-Starter/SuperTest-Mocha-demo.\nThe Jest version can be found in the demo project: https://github.com/Automation-Test-Starter/SuperTest-Jest-demo.\nThe mocha version is similar to the Jest version, so here is an example of the mocha version.\nCreate Multi-Environment Test Configuration File // create test configuration folder, if already exists, skip this step mkdir Config // create test configuration file for test environment cd Config touch testConfig-test.js // create test configuration file for dev environment touch testConfig-dev.js Edit Multi-Environment Test Configuration File edit test configuration file for test environment: testConfig-test.js based on actual situation, edit test configuration file for test environment\n// Test config file for test environment module.exports = { host: \u0026#39;https://jsonplaceholder.typicode.com\u0026#39;, // Test endpoint getAPI: \u0026#39;/posts/1\u0026#39;, // Test GET API URL postAPI: \u0026#39;/posts\u0026#39;, // Test POST API URL }; edit test configuration file for dev environment: testConfig-dev.js based on actual situation, edit test configuration file for dev environment\n// Test config file for dev environment module.exports = { host: \u0026#39;https://jsonplaceholder.typicode.com\u0026#39;, // Test endpoint getAPI: \u0026#39;/posts/1\u0026#39;, // Test GET API URL postAPI: \u0026#39;/posts\u0026#39;, // Test POST API URL }; Create Multi-Environment Test Data File // create test data folder, if already exists, skip this step mkdir testData // enter test data folder cd testData // create request data file for test environment touch requestData-test.js // create response data file for test environment touch responseData-test.js // create request data file for dev environment touch requestData-dev.js // create response data file for dev environment touch responseData-dev.js Edit Multi-Environment Test Data File edit request data file for test environment: requestData-test.js based on actual situation, edit request data file for test environment\n// Test request data file for test environment module.exports = { getAPI: \u0026#39;\u0026#39;, // request data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }, // request data for POST API }; edit response data file for test environment: responseData-test.js based on actual situation, edit response data file for test environment\n// Test response data file for test environment module.exports = { getAPI: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, // response data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 }, // response data for POST API }; edit request data file for dev environment: requestData-dev.js based on actual situation, edit request data file for dev environment\n// Test request data file for dev environment module.exports = { getAPI: \u0026#39;\u0026#39;, // request data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }, // request data for POST API }; edit response data file for dev environment: responseData-dev.js based on actual situation, edit response data file for dev environment\n// Test response data file for dev environment module.exports = { getAPI: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, // response data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 }, // response data for POST API }; Update test cases to support multiple environments To differentiate, here is a new test case file named multiEnvTest.spec.js\n// Test: multiEnvTest.spec.js const request = require(\u0026#39;supertest\u0026#39;); // import supertest require(\u0026#39;chai\u0026#39;); // import chai const expect = require(\u0026#39;chai\u0026#39;).expect; // import expect const config = process.env.NODE_ENV === \u0026#39;test\u0026#39; ? require(\u0026#39;../Config/testConfig-test\u0026#39;) : require(\u0026#39;../Config/testConfig-dev\u0026#39;); // import test config const requestData = process.env.NODE_ENV === \u0026#39;test\u0026#39; ? require(\u0026#39;../TestData/requestData-test\u0026#39;) : require(\u0026#39;../TestData/requestData-dev\u0026#39;); // import request data const responseData= process.env.NODE_ENV === \u0026#39;test\u0026#39; ? require(\u0026#39;../TestData/responseData-test\u0026#39;) : require(\u0026#39;../TestData/responseData-dev\u0026#39;); // import response data // Test Suite describe(\u0026#39;multiEnv-Verify that the Get and POST API returns correctly\u0026#39;, function(){ // Test case 1 it(\u0026#39;multiEnv-Verify that the GET API returns correctly\u0026#39;, function(done){ request(config.host) // Test endpoint .get(config.getAPI) // API endpoint .expect(200) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(responseData.getAPI.id) expect(res.body.userId).to.equal(responseData.getAPI.userId) expect(res.body.title).to.equal(responseData.getAPI.title) expect(res.body.body).to.equal(responseData.getAPI.body) }) // expected response body .end(done) // end the test case }); // Test case 2 it(\u0026#39;multiEnv-Verify that the POST API returns correctly\u0026#39;, function(done){ request(config.host) // Test endpoint .post(config.postAPI) // API endpoint .send(requestData.postAPI) // request body .expect(201) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(responseData.postAPI.id ) expect(res.body.userId).to.equal(responseData.postAPI.userId ) expect(res.body.title).to.equal(responseData.postAPI.title ) expect(res.body.body).to.equal(responseData.postAPI.body ) }) // expected response body .end(done) // end the test case }); }); Update test scripts to support multiple environments \u0026lsquo;\u0026lsquo;\u0026lsquo;json // package.json \u0026ldquo;scripts\u0026rdquo;: { \u0026ldquo;test\u0026rdquo;: \u0026ldquo;NODE_ENV=test mocha\u0026rdquo; // run test script for test environment \u0026ldquo;dev\u0026rdquo;: \u0026ldquo;NODE_ENV=dev mocha\u0026rdquo; // run test script for dev environment }, \u0026rsquo;\u0026rsquo;\u0026rsquo;\nRun the test case to check if the multi environment support is working. If you use demo project to run multi-environment support test case: multiEnvTest.spec.js, it is recommended to block dataDrivingTest.spec.js and test.spec.js test cases first, otherwise it will report an error.\nRun the test environment test script npm run test Run the dev environment test script npm run dev ","permalink":"https://naodeng.tech/posts/api-automation-testing/supertest-tutorial-advance-usage-multiple-environment-support/","summary":"focuses on advanced usage of SuperTest with an emphasis on multi-environment support. You will learn how to configure and manage multiple test environments for different stages of development and deployment.","title":"SuperTest API Automation Testing Tutorial Advance Usage - Multiple Environment Support"},{"content":"Data Driven Data-driven for API testing is a testing methodology in which the input data and expected output data for test cases are stored in data files, and the testing framework executes multiple tests against these data files to validate various aspects of the API. Data-driven testing can help you effectively cover multiple scenarios and ensure that the API works properly with a variety of input data.\nThe Mocha version can be found in the demo project: https://github.com/Automation-Test-Starter/SuperTest-Mocha-demo.\nThe Jest version can be found in the demo project: https://github.com/Automation-Test-Starter/SuperTest-Jest-demo.\nThe mocha version is similar to the Jest version, so here is an example of the mocha version.\nCreate test configuration files // create test configuration folder mkdir Config // create test configuration file cd Config touch config.js Edit test configuration files // Test config file module.exports = { host: \u0026#39;https://jsonplaceholder.typicode.com\u0026#39;, // Test endpoint getAPI: \u0026#39;/posts/1\u0026#39;, // Test GET API URL postAPI: \u0026#39;/posts\u0026#39;, // Test POST API URL }; Create test data files // create test data folder mkdir testData // enter test data folder cd testData // create request data file touch requestData.js // create response data file touch responseData.js Edit test data files Edit request data files // Test request data file module.exports = { getAPI: \u0026#39;\u0026#39;, // request data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }, // request data for POST API }; Edit response data files // Test response data file module.exports = { getAPI: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, // response data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 }, // response data for POST API }; Update test cases to support data-driven To differentiate, create a new test case file named dataDrivingTest.spec.js.\n// Test: dataDrivingTest.spec.js const request = require(\u0026#39;supertest\u0026#39;); // import supertest require(\u0026#39;chai\u0026#39;); // import chai const expect = require(\u0026#39;chai\u0026#39;).expect; // import expect const config = require(\u0026#39;../Config/testConfig\u0026#39;); // import test config const requestData = require(\u0026#39;../TestData/requestData\u0026#39;); // import request data const responseData = require(\u0026#39;../TestData/responseData\u0026#39;); // import response data // Test Suite describe(\u0026#39;Data Driving-Verify that the Get and POST API returns correctly\u0026#39;, function(){ // Test case 1 it(\u0026#39;Data Driving-Verify that the GET API returns correctly\u0026#39;, function(done){ request(config.host) // Test endpoint .get(config.getAPI) // API endpoint .expect(200) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(responseData.getAPI.id) expect(res.body.userId).to.equal(responseData.getAPI.userId) expect(res.body.title).to.equal(responseData.getAPI.title) expect(res.body.body).to.equal(responseData.getAPI.body) }) // expected response body .end(done) // end the test case }); // Test case 2 it(\u0026#39;Data Driving-Verify that the POST API returns correctly\u0026#39;, function(done){ request(config.host) // Test endpoint .post(config.postAPI) // API endpoint .send(requestData.postAPI) // request body .expect(201) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(responseData.postAPI.id ) expect(res.body.userId).to.equal(responseData.postAPI.userId ) expect(res.body.title).to.equal(responseData.postAPI.title ) expect(res.body.body).to.equal(responseData.postAPI.body ) }) // expected response body .end(done) // end the test case }); }); Run the test case to check whether the data driver is effective. If you run the data driver support test case: dataDrivingTest.spec.js with the demo project, it is recommended to skip the test.spec.js test case first, otherwise it will report an error.\n","permalink":"https://naodeng.tech/posts/api-automation-testing/supertest-tutorial-advance-usage-data-driven/","summary":"advanced usage of Supertest, focusing on data-driven testing. You will learn how to extend and optimize your Supertest test suite with data parameterization to improve test coverage.","title":"SuperTest API Automation Testing Tutorial Advance Usage - Data Driven"},{"content":"Common Assertions The following is an overview of common assertions used by SuperTest, CHAI and Jest.\nSuperTest\u0026rsquo;s built-in assertions Supertest is a more advanced library built on SuperAgent, so Supertest can easily use SuperAgent\u0026rsquo;s HTTP assertions.\nExamples are as follows:\n.expect(status[, fn]) // Assert response status code. .expect(status, body[, fn]) // Assert response status code and body. .expect(body[, fn]) // Assert response body text with a string, regular expression, or parsed body object. .expect(field, value[, fn]) // Assert header field value with a string or regular expression. .expect(function(res) {}) // Pass a custom assertion function. It\u0026#39;ll be given the response object to check. If the check fails, throw an error. Common Assertions for CHAI Equality Assertions expect(actual).to.equal(expected) // Verify that the actual value is equal to the expected value. expect(actual).to.deep.equal(expected) // Verify that the actual value is deeply equal to the expected value, for object and array comparisons. expect(actual).to.eql(expected) // Same as deep.equal for deep-equal comparisons. Inclusion Assertions expect(array).to.include(value) // Verify that the array contains the specified value. expect(string).to.include(substring) // Verify that the string contains the specified substring. expect(object).to.include(key) // Verify that the object contains the specified key. Type Assertions expect(actual).to.be.a(type) // Verify that the type of the actual value is equal to the specified type. expect(actual).to.be.an(type) // Same as to.be.a for type assertions. expect(actual).to.be.an.instanceof(constructor) // Verify that the actual value is an instance of the specified constructor. Truthiness Assertions expect(value).to.be.true // Verify that the value is true. expect(value).to.be.false // Verify that the value is false. expect(value).to.exist // Verify that the value exists, is not null and is not undefined. Length Assertions expect(array).to.have.length(length) // Verify that the length of the array is equal to the specified length. expect(string).to.have.lengthOf(length) // Verify that the length of the string is equal to the specified length. Empty Assertions expect(array).to.be.empty // Verify if the array is empty. expect(string).to.be.empty // Verify that the string is empty. Range Assertions expect(value).to.be.within(min, max) // Verify that the value is within the specified range. expect(value).to.be.above(min) // Verify that the value is greater than the specified value. expect(value).to.be.below(max) // Verify that the value is less than the specified value. Exception Assertions expect(fn).to.throw(error) // Verify that the function throws an exception of the specified type. expect(fn).to.throw(message) // Verify that the function throws an exception containing the specified message. Existence Assertions expect(object).to.have.property(key) // Verify that the object contains the specified property. expect(array).to.have.members(subset) // Verify that the array contains the specified members. For more chai assertions, see https://www.chaijs.com/api/assert/\nCommon Assertions for Jest Equality Assertions expect(actual).toBe(expected) // Verify that the actual value is strictly equal to the expected value. expect(actual).toEqual(expected) // Verify that the actual value is deeply equal to the expected value, for object and array comparisons. Inequality Assertions expect(actual).not.toBe(expected) // Verify that the actual value is not equal to the expected value. Inclusion Assertions expect(array).toContain(value) // Verify that the array contains the specified value. Type Assertions expect(actual).toBeTypeOf(expected) // Verify that the type of the actual value is equal to the specified type. Truthiness Assertions expect(value).toBeTruthy() // Verify that the value is true. expect(value).toBeFalsy() // Verify that the value is false. Asynchronous Assertions await expect(promise).resolves.toBe(expected) // Verify that the asynchronous operation completed successfully and return a result matching the expected value. Exception Assertions expect(fn).toThrow(error) // Verify that the function throws an exception of the specified type. expect(fn).toThrow(message) // Verify that the function throws an exception containing the specified message. Scope Assertions expect(value).toBeGreaterThanOrEqual(min) // Verify that the value is greater than or equal to the specified minimum. expect(value).toBeLessThanOrEqual(max) // Verify that the value is less than or equal to the specified maximum. Object Property Assertions expect(object).toHaveProperty(key, value) // Verify that the object contains the specified property and that the value of the property is equal to the specified value. For more Jest assertions, seehttps://jestjs.io/docs/expect\n","permalink":"https://naodeng.tech/posts/api-automation-testing/supertest-tutorial-advance-usage-common-assertions/","summary":"This blog focuses on advanced usage of Supertest, with a particular focus on commonly used assertions.","title":"SuperTest API Automation Testing Tutorial: Advanced Usage - Common Assertions"},{"content":"CI/CD integration Integration github action Use github action as an example, and other CI tools similarly\nThe mocha version integration github action See the demo at https://github.com/Automation-Test-Starter/SuperTest-Mocha-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as mocha.yml, in the .github/workflows directory.\nEdit the mocha.yml file: Copy the following into the file\nname: RUN SuperTest API Test CI on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: RUN-SuperTest-API-Test: runs-on: ubuntu-latest strategy: matrix: node-version: [ 18.x] # See supported Node.js release schedule at https://nodejs.org/en/about/releases/ steps: - uses: actions/checkout@v3 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v3 with: node-version: ${{ matrix.node-version }} cache: \u0026#39;npm\u0026#39; - name: Installation of related packages run: npm ci - name: RUN SuperTest API Testing run: npm test - name: Archive SuperTest mochawesome test report uses: actions/upload-artifact@v3 with: name: SuperTest-mochawesome-test-report path: Report - name: Upload SuperTest mochawesome report to GitHub uses: actions/upload-artifact@v3 with: name: SuperTest-mochawesome-test-report path: Report Commit the code: Add the mocha.yml file to your repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the RUN SuperTest API Test CI workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. The jest version integration github action See the demo at https://github.com/Automation-Test-Starter/SuperTest-Jest-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as jest.yml, in the .github/workflows directory.\nEdit the jest.yml file: Copy the following into the file\nname: RUN SuperTest API Test CI on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: RUN-SuperTest-API-Test: runs-on: ubuntu-latest strategy: matrix: node-version: [ 18.x] # See supported Node.js release schedule at https://nodejs.org/en/about/releases/ steps: - uses: actions/checkout@v3 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v3 with: node-version: ${{ matrix.node-version }} cache: \u0026#39;npm\u0026#39; - name: Installation of related packages run: npm ci - name: RUN SuperTest API Testing run: npm test - name: Archive SuperTest test report uses: actions/upload-artifact@v3 with: name: SuperTest-test-report path: Report - name: Upload SuperTest report to GitHub uses: actions/upload-artifact@v3 with: name: SuperTest-test-report path: Report Commit the code: Add the jest.yml file to the repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the RUN-SuperTest-API-Test workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. ","permalink":"https://naodeng.tech/posts/api-automation-testing/supertest-tutorial-advance-usage-integration-ci-cd-and-github-action/","summary":"dive into advanced usage of Supertest, focusing on how to integrate Supertest into a CI/CD process and how to automate tests using GitHub Actions.","title":"SuperTest API Automation Testing Tutorial: Advanced Usage - Integration CI CD and Github action"},{"content":"Build a SuperTest API automation test project from 0 to 1 The following is a demo of building a SuperTest API automation test project from 0 to 1, using either Jest or Mocha as the test framework.\nMocha version You can refer to the demo project at https://github.com/Automation-Test-Starter/SuperTest-Mocha-demo.\nCreate a new project folder mkdir SuperTest-Mocha-demo Project Initialization // enter the project folder cd SuperTest-Mocha-demo // nodejs project initialization npm init -y Install dependencies // install supertest library npm install supertest --save-dev // install mocha test framework npm install mocha --save-dev // install chai assertion library npm install chai --save-dev Create new test folder and test cases // create test folder mkdir Specs // create test case file cd Specs touch test.spec.js Writing Test Cases The test API can be found in the demoAPI.md file in the project.\n// Test: test.spec.js const request = require(\u0026#39;supertest\u0026#39;); // import supertest const chai = require(\u0026#39;chai\u0026#39;); // import chai const expect = require(\u0026#39;chai\u0026#39;).expect; // import expect // Test Suite describe(\u0026#39;Verify that the Get and POST API returns correctly\u0026#39;, function(){ // Test case 1 it(\u0026#39;Verify that the GET API returns correctly\u0026#39;, function(done){ request(\u0026#39;https://jsonplaceholder.typicode.com\u0026#39;) // Test endpoint .get(\u0026#39;/posts/1\u0026#39;) // API endpoint .expect(200) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(1 ) expect(res.body.userId).to.equal(1) expect(res.body.title) .to.equal(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;) expect(res.body.body) .to.equal(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;) }) // expected response body .end(done) // end the test case }); // Test case 2 it(\u0026#39;Verify that the POST API returns correctly\u0026#39;, function(done){ request(\u0026#39;https://jsonplaceholder.typicode.com\u0026#39;) // Test endpoint .post(\u0026#39;/posts\u0026#39;) // API endpoint .send({ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }) // request body .expect(201) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(101 ) expect(res.body.userId).to.equal(1) expect(res.body.title).to.equal(\u0026#34;foo\u0026#34;) expect(res.body.body).to.equal(\u0026#34;bar\u0026#34;) }) // expected response body .end(done) // end the test case }); }); Configuring mocha config files Create a new mocha configuration file // create configuration file in the project root directory touch .mocharc.js Updating configuration files // mocha config module.exports = { timeout: 5000, // set the default timeout for test cases (milliseconds) spec: [\u0026#39;Specs/**/*.js\u0026#39;], // specify the location of the test file }; Updating test scripts for mocha Add test scripts to the package.json file\n\u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;mocha\u0026#34; }, Running test cases // run test cases npm run test Test Report Terminal Test Report Integrated mochawesome test report Install mochawesome library npm install --save-dev mochawesome Updating mocha configuration files You can refer to the demo project athttps://github.com/Automation-Test-Starter/SuperTest-Mocha-demo\n// mocha config module.exports = { timeout: 5000, // Set the default timeout for test cases (milliseconds) reporter: \u0026#39;mochawesome\u0026#39;, // Use mochawesome as the test report generator \u0026#39;reporter-option\u0026#39;: [ \u0026#39;reportDir=Report\u0026#39;, // Report directory \u0026#39;reportFilename=[status]_[datetime]-[name]-report\u0026#39;, //Report file name \u0026#39;html=true\u0026#39;, // enable html report \u0026#39;json=false\u0026#39;, // disable json report \u0026#39;overwrite=false\u0026#39;, // disable report file overwrite \u0026#39;timestamp=longDate\u0026#39;, // add timestamp to report file name ], // mochawesome report generator options spec: [\u0026#39;Specs/**/*.js\u0026#39;], // Specify the location of the test file }; Running test cases // Run test cases npm run test View test report Test report folder: Report, click to open the latest html report file with your browser\nJest version You can refer to the demo project athttps://github.com/Automation-Test-Starter/SuperTest-Jest-demo\nCreate a new jest project folder mkdir SuperTest-Jest-demo Jest demo project initialization // enter the project folder cd SuperTest-Mocha-demo // nodejs project initialization npm init -y Jest demo install dependencies // install supertest library npm install supertest --save-dev // install jest test framework npm install jest --save-dev Create new Jest demo project test folder and test cases // create test folder mkdir Specs // enter test folder and create test case file cd Specs touch test.spec.js Writing Jest demo Test Cases The test API can be found in the demoAPI.md file in the project.\nconst request = require(\u0026#39;supertest\u0026#39;); // Test Suite describe(\u0026#39;Verify that the Get and POST API returns correctly\u0026#39;, () =\u0026gt; { // Test case 1 it(\u0026#39;Verify that the GET API returns correctly\u0026#39;, async () =\u0026gt; { const res = await request(\u0026#39;https://jsonplaceholder.typicode.com\u0026#39;) // Test endpoint .get(\u0026#39;/posts/1\u0026#39;) // API endpoint .send() // request body .expect(200); // use supertest\u0026#39;s expect to verify that the status code is 200 // user jest\u0026#39;s expect to verify the response body expect(res.status).toBe(200); // Verify that the status code is 200 expect(res.body.id).toEqual(1); // Verify that the id is 1 expect(res.body.userId).toEqual(1); // Verify that the userId is 1 expect(res.body.title) .toEqual(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;); expect(res.body.body) .toEqual(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;); }); // Test case 2 it(\u0026#39;Verify that the POST API returns correctly\u0026#39;, async() =\u0026gt;{ const res = await request(\u0026#39;https://jsonplaceholder.typicode.com\u0026#39;) // Test endpoint .post(\u0026#39;/posts\u0026#39;) // API endpoint .send({ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }) // request body .expect(201); // use supertest\u0026#39;s expect to verify that the status code is 201 // user jest\u0026#39;s expect to verify the response body expect(res.statusCode).toBe(201); expect(res.body.id).toEqual(101); expect(res.body.userId).toEqual(1); expect(res.body.title).toEqual(\u0026#34;foo\u0026#34;); expect(res.body.body).toEqual(\u0026#34;bar\u0026#34;); }); }); Configuring Jest config files Creating a new configuration file // Create a new configuration file in the project root directory touch jest.config.js Updating configuration files // Desc: Jest configuration file module.exports = { // Specify the location of the test file testMatch: [\u0026#39;**/Specs/*.spec.js\u0026#39;], }; Adapting Jest Test Scripts Add the test script to the package.json file\n\u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;jest\u0026#34; }, Runing test case // run test case npm run test Jest test report Jest terminal Test Report Integrating jest-html-reporters test reports Install jest-html-reporters library npm install --save-dev jest-html-reporters Updating jest configuration files You can refer to the demo project atttps://github.com/Automation-Test-Starter/SuperTest-Jest-demo\n// Desc: Jest configuration file module.exports = { // specify the location of the test file testMatch: [\u0026#39;**/Specs/*.spec.js\u0026#39;], // test report generator reporters: [ \u0026#39;default\u0026#39;, [ \u0026#39;jest-html-reporters\u0026#39;, { publicPath: \u0026#39;./Report\u0026#39;, // report directory filename: \u0026#39;report.html\u0026#39;, // report file name pageTitle: \u0026#39;SuperTest and Jest API Test Report\u0026#39;, // report title overwrite: true, // enable report file overwrite expand: true, // enable report file expansion }, ], ], }; Running test cases // run test case npm run test View test report Test report folder: Report, click on the browser to open the latest html report file\n","permalink":"https://naodeng.tech/posts/api-automation-testing/supertest-tutorial-building-your-own-project-from-0-to-1/","summary":"dive into how to build a Supertest API automation testing project from scratch.Supertest is a popular Java library for performing REST API testing, providing powerful tools that make it easy to write automated test scripts to validate the API\u0026rsquo;sbehavior.","title":"SuperTest API Automation Testing Tutorial: Building a Supertest API Automation Test project from 0 to 1"},{"content":"Introduction This project is a quick start tutorial for API automation testing using SuperTest, and will use Jest or Mocha as the testing framework for demo demonstration.\nWe will introduce SuperTest, Jest and Mocha in turn, so that you can understand the basic usage of these tools in advance.\nIntroduction of SuperTest \u0026ldquo;Supertest\u0026rdquo; is a popular JavaScript library for testing Node.js applications. It is primarily used for end-to-end testing, also known as integration testing, to make sure that your application works properly across different components.Supertest is typically used in conjunction with testing frameworks such as Mocha, Jasmine or Jest to write and run test cases.\nHere are some of the key features and uses of Supertest:\nInitiating HTTP requests: Supertest allows you to easily simulate HTTP requests such as GET, POST, PUT, DELETE, etc. to test your application\u0026rsquo;s routing and endpoints. Chained Syntax: Supertest provides a chained syntax that allows you to build and execute multiple requests in a single test case, which helps simulate different user actions in your application. Assertions and Expectations: You can use Supertest in conjunction with assertion libraries such as Chai to examine the content of the response, status codes, headers, etc. to ensure the expected behavior of your application. Authentication Testing: Supertest can be used to test endpoints that require authentication to ensure that user login and authorization functions properly. Asynchronous support: Supertest can handle asynchronous operations, such as waiting for a response to return before executing further test code. Easy Integration: Supertest can be easily used with different Node.js frameworks (e.g. Express, Koa, Hapi, etc.), so you can test all types of applications. Using Supertest can help you verify that your application is working as expected, as well as quickly catch potential problems when changes are made to your application. Typically, you need to install Supertest and the testing framework in your project, and then write test cases to simulate different requests and check responses. This helps improve code quality and maintainability and ensures that your application remains stable as it evolves.\nOfficial documentation: https://github.com/ladjs/supertest\nNote: Supertest can be used not only for API testing, but also for unit and integration testing.\ncode examples:\n// import supertest const request = require(\u0026#39;supertest\u0026#39;); request({URL}) // request(url) or request(app) .get() or .put() or.post() // http methods .set() // http options .send() // http body .expect() // http assertions .end() // end the request Introduction of Jest Jest is a popular JavaScript testing framework for writing and running unit, integration and end-to-end tests for JavaScript applications. Its goal is to provide simple, fast and easy-to-use testing tools for a wide variety of JavaScript applications, both front-end and back-end.\nHere are some of the key features and uses of Jest:\nBuilt-in Assertion Library: Jest includes a powerful assertion library that makes it easy to write assertions to verify that code behaves as expected. Automated mocks: Jest automatically creates mocks that help you simulate functions, modules, and external dependencies, making testing easier and more manageable. Fast and Parallel: Jest saves time by intelligently selecting which tests to run and executing them in parallel, allowing you to run a large number of test cases quickly. Comprehensive Test Suite: Jest supports unit, integration and end-to-end testing and can test a wide range of application types such as JavaScript, TypeScript, React, Vue, Node.js and more. Snapshot testing: Jest has a snapshot testing feature that can be used to capture UI changes by checking if the rendering of a UI component matches a previous snapshot. Automatic Watch Mode: Jest has a watch mode that automatically re-runs tests as code changes are made, supporting developers in continuous testing. Rich Ecosystem: Jest has a rich set of plug-ins and extensions that can be used to extend its functionality, such as coverage reporting, test reporting, and integration with other tools. Community Support: Jest is a popular testing framework with a large community that provides extensive documentation, tutorials and support resources. Jest is often used in conjunction with other tools such as Babel (for transcoding JavaScript), Enzyme (for React component testing), Supertest (for API testing), etc. to achieve comprehensive test coverage and ensure code quality. Whether you\u0026rsquo;re writing front-end or back-end code, Jest is a powerful testing tool that can help you catch potential problems and improve code quality and maintainability.\nOfficial Documentation: https://jestjs.io/docs/zh-Hans/getting-started\nCode examples:\n// import jest const jest = require(\u0026#39;jest\u0026#39;); describe(): // test scenarios it(): // detailed test case, it() is in the describe() before(): // this action is before all test cases after(): // this action is after all test cases Introduction of Mocha Mocha is a popular JavaScript testing framework for writing and running a variety of tests for JavaScript applications, including unit tests, integration tests, and end-to-end tests.Mocha provides flexibility and extensibility, allowing developers to easily customize the test suite to meet the needs of their projects.\nHere are some of the key features and uses of Mocha:\nMultiple Test Styles: Mocha supports multiple test styles including BDD (Behavior Driven Development) and TDD (Test Driven Development). This allows developers to write test cases according to their preferences. Rich Assertion Library: Mocha does not include an assertion library by itself, but it can be used in conjunction with a variety of assertion libraries (e.g., Chai, Should.js, Expect.js, etc.), allowing you to write tests using your favorite assertion style. Asynchronous Testing: Mocha has built-in support for asynchronous testing, allowing you to test asynchronous code, Promises, callback functions, etc. to ensure that your code is correct in asynchronous scenarios. Parallel Testing: Mocha allows you to run test cases in your test suite in parallel, improving the efficiency of test execution. Rich Plug-ins and Extensions: Mocha has a rich ecosystem of plug-ins that can be used to extend its functionality, such as test coverage reporting, test report generation, and so on. Easy to Integrate: Mocha can be used with various assertion libraries, test runners (such as Karma and Jest), browsers (using the browser test runner), etc. to suit different project and testing needs. Command Line Interface: Mocha provides an easy-to-use command line interface for running test suites, generating reports, and other test-related operations. Continuous Integration Support: Mocha can be easily integrated into Continuous Integration (CI) tools such as Jenkins, Travis CI, CircleCI, etc. to ensure that code is tested after every commit. Mocha\u0026rsquo;s flexibility and extensibility make it a popular testing framework for a variety of JavaScript projects, including front-end and back-end applications. Developers can choose the testing tools, assertion libraries, and other extensions to meet the requirements of their projects based on their needs and preferences. Whether you are writing browser-side code or server-side code, Mocha is a powerful testing tool to help you ensure code quality and reliability.\nOfficial documentation: https://mochajs.org/\nCode examples:\n// import mocha const mocha = require(\u0026#39;mocha\u0026#39;); describe(): // test scenarios it(): // detailed test case, it() is in the describe() before(): // this action is before all test cases after(): // this action is after all test cases Introduction of CHAI Chai is a JavaScript assertion library for assertion and expectation validation when writing and running test cases. It is a popular testing tool that is often used in conjunction with testing frameworks (e.g. Mocha, Jest, etc.) to help developers write and execute various types of tests, including unit tests and integration tests.\nHere are some of the key features and uses of Chai:\nReadable Assertion Syntax: Chai provides an easy to read and write assertion syntax that makes test cases easier to understand. It supports natural language assertion styles such as expect(foo).to.be.a(\u0026lsquo;string\u0026rsquo;) or expect(bar).to.equal(42). Multiple Assertion Styles: Chai provides a number of different assertion styles to suit different developer preferences. The main styles include BDD (Behavior-Driven Development) style, TDD (Test-Driven Development) style and assert style. Plugin extensions: Chai can be extended with plugins to support more assertion types and features. This allows Chai to fulfill a variety of testing needs, including asynchronous testing, HTTP request testing, and more. Easy Integration: Chai can be easily integrated with various testing frameworks such as Mocha, Jest, Jasmine etc. This makes it a powerful tool for writing test cases. Chained Assertions Support: Chai allows you to chain calls to multiple assertions to make complex testing and validation easier. Official documentation: https://www.chaijs.com/\nCode examples:\n// import chai const chai = require(\u0026#39;chai\u0026#39;); const expect = chai.expect; // demo assertions .expect(\u0026lt;actual result\u0026gt;).to.{assert}(\u0026lt;expected result\u0026gt;) // Asserts that the target is strictly equal to value. .expect(‘hello\u0026#39;).to.equal(\u0026#39;hello\u0026#39;); // Asserts that the target is strictly equal to value. .expect({ foo: \u0026#39;bar\u0026#39; }).to.not.equal({ foo: \u0026#39;bar\u0026#39; }); // Asserts that the target is not strictly equal to value. .expect(\u0026#39;foobar\u0026#39;).to.contain(\u0026#39;foo\u0026#39;); // Asserts that the target contains the given substring. .expect(foo).to.exist; // Asserts that the target is neither null nor undefined. .expect(5).to.be.at.most(5); // Asserts that the target is less than or equal to value. Project dependencies The following environments need to be installed in advance\nnodejs, demo version v21.1.0 Project Structure The following is the file structure of a SuperTest Interface Automation Test project, which contains test configuration files, test case files, test tool files, and test report files. It can be used for reference.\nSuperTest-Jest-demo ├── README.md ├── package.json ├── package-lock.json ├── Config // Test configuration │ └── config.js ├── Specs // Test case │ └── test.spec.js ├── Utils // Test tool │ └── utils.js ├── Report // Test report │ └── report.html ├── .gitignore └── node_modules // Project dependencies ├── ... └── ... Next In the next post, we will introduce how to build a SuperTest interface automation test project from 0 to 1 using Supertest, so stay tuned.\n","permalink":"https://naodeng.tech/posts/api-automation-testing/supertest-tutorial-getting-started-and-own-environment-preparation/","summary":"a tutorial on Supertest, focusing on getting started and preparing the environment to be built.","title":"SuperTest API Automation Testing Tutorial: Getting Started and Setting Up Your Environment"},{"content":"CI/CD integration integration github action Use github action as an example, and other CI tools similarly\nThe Gradle version integration github action See the demo at https://github.com/Automation-Test-Starter/RestAssured-gradle-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as gradle.yml, in the .github/workflows directory.\nEdit the gradle.yml file: Copy the following into the file\nname: Gradle and REST Assured Tests on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v3 - name: Setup Java uses: actions/setup-java@v3 with: java-version: \u0026#39;11\u0026#39; distribution: \u0026#39;adopt\u0026#39; - name: Build and Run REST Assured Tests with Gradle uses: gradle/gradle-build-action@bd5760595778326ba7f1441bcf7e88b49de61a25 # v2.6.0 with: arguments: build - name: Archive REST-Assured results uses: actions/upload-artifact@v2 with: name: REST-Assured-results path: build/reports/tests/test - name: Upload REST-Assured results to GitHub uses: actions/upload-artifact@v2 with: name: REST-Assured-results path: build/reports/tests/test Commit the code: Add the gradle.yml file to your repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Gradle and REST Assured Tests workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. The Maven version integration github action See the demo at https://github.com/Automation-Test-Starter/RestAssured-maven-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as maven.yml, in the .github/workflows directory.\nEdit the maven.yml file: Copy the following into the file\nname: Maven and REST Assured Tests on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: Run-Rest-Assured-Tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up JDK 17 uses: actions/setup-java@v3 with: java-version: \u0026#39;17\u0026#39; distribution: \u0026#39;temurin\u0026#39; cache: maven - name: Build and Run REST Assured Tests with Maven run: mvn test - name: Archive REST-Assured results uses: actions/upload-artifact@v3 with: name: REST-Assured-results path: target/surefire-reports - name: Upload REST-Assured results to GitHub uses: actions/upload-artifact@v3 with: name: REST-Assured-results path: target/surefire-reports Commit the code: Add the maven.yml file to the repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Maven and REST Assured Tests workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. Integrating allure test reports allure Introduction Allure is an open source testing framework for generating beautiful, interactive test reports. It can be used with a variety of testing frameworks (e.g. JUnit, TestNG, Cucumber, etc.) and a variety of programming languages (e.g. Java, Python, C#, etc.).\nAllure test reports have the following features:\nAesthetically pleasing and interactive: Allure test reports present test results in an aesthetically pleasing and interactive way, including graphs, charts and animations. This makes test reports easier to read and understand. Multi-language support: Allure supports multiple programming languages, so you can write tests in different languages and generate uniform test reports. Test case level details: Allure allows you to add detailed information to each test case, including descriptions, categories, labels, attachments, historical data, and more. This information helps provide a more complete picture of the test results. Historical Trend Analysis: Allure supports test historical trend analysis, which allows you to view the historical performance of test cases, identify issues and improve test quality. Categories and Tags: You can add categories and tags to test cases to better organize and categorize test cases. This makes reporting more readable. Attachments and Screenshots: Allure allows you to attach files, screenshots, and other attachments to better document information during testing. Integration: Allure seamlessly integrates with a variety of testing frameworks and build tools (e.g. Maven, Gradle), making it easy to generate reports. Open Source Community Support: Allure is an open source project with an active community that provides extensive documentation and support. This makes it the tool of choice for many automated testing teams. The main goal of Allure test reports is to provide a clear, easy-to-read way to present test results to help development teams better understand the status and quality of their tests, quickly identify problems, and take the necessary action. Whether you are a developer, tester, or project manager, Allure test reports provide you with useful information to improve software quality and reliability.\nOfficial Website: https://docs.qameta.io/allure/\nIntegration steps The Maven version integration of allure Add allure dependency in POM.xml Copy the contents of the pom.xml file in this project\n\u0026lt;!-- https://mvnrepository.com/artifact/io.qameta.allure/allure-testng --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.qameta.allure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;allure-testng\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.24.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/io.qameta.allure/allure-rest-assured --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.qameta.allure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;allure-rest-assured\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.24.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Add allure plugin to POM.xml \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.qameta.allure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;allure-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.12.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;resultsDirectory\u0026gt;../allure-results\u0026lt;/resultsDirectory\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; Create test code for testing the REST API under src/test/java. The following is an example of a demo, see the project for details: https://github.com/Automation-Test-Starter/RestAssured-maven-demo.\npackage com.example; import io.qameta.allure.*; import io.qameta.allure.restassured.AllureRestAssured; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; @Epic(\u0026#34;REST API Regression Testing using TestNG\u0026#34;) @Feature(\u0026#34;Verify that the Get and POST API returns correctly\u0026#34;) public class TestDemo { @Test(description = \u0026#34;To get the details of post with id 1\u0026#34;, priority = 1) @Story(\u0026#34;GET Request with Valid post id\u0026#34;) @Severity(SeverityLevel.NORMAL) @Description(\u0026#34;Test Description : Verify that the GET API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .filter(new AllureRestAssured()) // Set up the AllureRestAssured filter to display request and response information in the test report .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .statusCode(200) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(1)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } @Test(description = \u0026#34;To create a new post\u0026#34;, priority = 2) @Story(\u0026#34;POST Request\u0026#34;) @Severity(SeverityLevel.NORMAL) @Description(\u0026#34;Test Description : Verify that the post API returns correctly\u0026#34;) public void verifyPostAPI() { // Given given() .filter(new AllureRestAssured()) // Set up the AllureRestAssured filter to display request and response information in the test report .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .body(\u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;foo\\\u0026#34;, \\\u0026#34;body\\\u0026#34;: \\\u0026#34;bar\\\u0026#34;, \\\u0026#34;userId\\\u0026#34;: 1\\n}\u0026#34;) .post(\u0026#34;/posts\u0026#34;) // Then .then() .statusCode(201) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(101)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;foo\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;bar\u0026#34;)); } } Run tests and generate Allure reports mvn clean test The generated Allure report is in the allure-results file in the project root directory.\nPreview of the Allure Report mvn allure:serve Running the command automatically opens a browser to preview the Allure report.\nThe Gradle version of allure integration Add the allure plugin to your build.gradle. Copy the contents of the build.gradle file in this project\nid(\u0026#34;io.qameta.allure\u0026#34;) version \u0026#34;2.11.2\u0026#34; Add allure dependency to build.gradle Copy the contents of the build.gradle file in this project\nimplementation \u0026#39;io.qameta.allure:allure-testng:2.24.0\u0026#39; // Add allure report dependency implementation \u0026#39;io.qameta.allure:allure-rest-assured:2.24.0\u0026#39; // Add allure report dependency Create test code for testing the REST API under src/test/java. The following is an example of a demo, see the project for details: https://github.com/Automation-Test-Starter/RestAssured-gradle-demo.\npackage com.example; import io.qameta.allure.*; import io.qameta.allure.restassured.AllureRestAssured; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; @Epic(\u0026#34;REST API Regression Testing using TestNG\u0026#34;) @Feature(\u0026#34;Verify that the Get and POST API returns correctly\u0026#34;) public class TestDemo { @Test(description = \u0026#34;To get the details of post with id 1\u0026#34;, priority = 1) @Story(\u0026#34;GET Request with Valid post id\u0026#34;) @Severity(SeverityLevel.NORMAL) @Description(\u0026#34;Test Description : Verify that the GET API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .filter(new AllureRestAssured()) // Set up the AllureRestAssured filter to display request and response information in the test report .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .statusCode(200) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(1)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } @Test(description = \u0026#34;To create a new post\u0026#34;, priority = 2) @Story(\u0026#34;POST Request\u0026#34;) @Severity(SeverityLevel.NORMAL) @Description(\u0026#34;Test Description : Verify that the post API returns correctly\u0026#34;) public void verifyPostAPI() { // Given given() .filter(new AllureRestAssured()) // Set up the AllureRestAssured filter to display request and response information in the test report .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .body(\u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;foo\\\u0026#34;, \\\u0026#34;body\\\u0026#34;: \\\u0026#34;bar\\\u0026#34;, \\\u0026#34;userId\\\u0026#34;: 1\\n}\u0026#34;) .post(\u0026#34;/posts\u0026#34;) // Then .then() .statusCode(201) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(101)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;foo\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;bar\u0026#34;)); } } Run the test and generate the Allure report gradle clean test `` \u0026gt; The generated Allure report is in the build/allure-results file in the project root directory. - Preview the Allure report ```bash gradle allureServe Running the command automatically opens a browser to preview the Allure report.\nReference Rest assured official documentation: https://rest-assured.io/\nRest assured official github:https://github.com/rest-assured/rest-assured\nRest assured official docs in Chinese: https://github.com/RookieTester/rest-assured-doc\n","permalink":"https://naodeng.tech/posts/api-automation-testing/rest-assured-tutorial-advance-usage-integration-ci-cd-and-allure-report/","summary":"dive into advanced applications of REST Assured, focusing on how to integrate CI/CD (Continuous Integration/Continuous Delivery) tools and integrate Allure test reports.","title":"REST Assured API Automation Testing Tutorial: Advanced Usage - Integration CI CD and Integration Allure Report"},{"content":"Advanced Usage Verifying Response Data You can verify Response status code, Response status line, Response cookies, Response headers, Response content type and Response body.\nresponse body assertion json assertion Assume that the GET request (to http://localhost:8080/lotto) returns JSON as:\n{ \u0026#34;lotto\u0026#34;:{ \u0026#34;lottoId\u0026#34;:5, \u0026#34;winning-numbers\u0026#34;:[2,45,34,23,7,5,3], \u0026#34;winners\u0026#34;:[{ \u0026#34;winnerId\u0026#34;:23, \u0026#34;numbers\u0026#34;:[2,45,34,23,3,5] },{ \u0026#34;winnerId\u0026#34;:54, \u0026#34;numbers\u0026#34;:[52,3,12,11,18,22] }] } } REST assured makes it easy to make get requests and process response messages.\nAsserts whether the value of lottoId is equal to 5. For example: get(\u0026#34;/lotto\u0026#34;).then().body(\u0026#34;lotto.lottoId\u0026#34;, equalTo(5)); Assertion The values for winnerId include 23 and 54. For example: get(\u0026#34;/lotto\u0026#34;).then().body(\u0026#34;lotto.winners.winnerId\u0026#34;, hasItems(23, 54)); Note: equalTo and hasItems are Hamcrest matchers which you should statically import from org.hamcrest.Matchers.\nXML assertion XML can be verified in a similar way. Imagine that a POST request to http://localhost:8080/greetXML returns:\n\u0026lt;greeting\u0026gt; \u0026lt;firstName\u0026gt;{params(\u0026#34;firstName\u0026#34;)}\u0026lt;/firstName\u0026gt; \u0026lt;lastName\u0026gt;{params(\u0026#34;lastName\u0026#34;)}\u0026lt;/lastName\u0026gt; \u0026lt;/greeting\u0026gt; Asserts whether the firstName is returned correctly. For example: given(). parameters(\u0026#34;firstName\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;lastName\u0026#34;, \u0026#34;Doe\u0026#34;). when(). post(\u0026#34;/greetXML\u0026#34;). then(). body(\u0026#34;greeting.firstName\u0026#34;, equalTo(\u0026#34;John\u0026#34;)). Assert that firstname and lastname are returned correctly. For example: given(). parameters(\u0026#34;firstName\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;lastName\u0026#34;, \u0026#34;Doe\u0026#34;). when(). post(\u0026#34;/greetXML\u0026#34;). then(). body(\u0026#34;greeting.firstName\u0026#34;, equalTo(\u0026#34;John\u0026#34;)). body(\u0026#34;greeting.lastName\u0026#34;, equalTo(\u0026#34;Doe\u0026#34;)); with().parameters(\u0026#34;firstName\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;lastName\u0026#34;, \u0026#34;Doe\u0026#34;) .when().post(\u0026#34;/greetXML\u0026#34;) .then().body(\u0026#34;greeting.firstName\u0026#34;, equalTo(\u0026#34;John\u0026#34;), \u0026#34;greeting.lastName\u0026#34;, equalTo(\u0026#34;Doe\u0026#34;)); Cookie assertion Asserts whether the value of the cookie is equal to cookieValue. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().cookie(\u0026#34;cookieName\u0026#34;, \u0026#34;cookieValue\u0026#34;) Asserts whether the value of multiple cookies is equal to the cookieValue at the same time. For example: get(\u0026#34;/x\u0026#34;).then() .assertThat().cookies(\u0026#34;cookieName1\u0026#34;, \u0026#34;cookieValue1\u0026#34;, \u0026#34;cookieName2\u0026#34;, \u0026#34;cookieValue2\u0026#34;) Asserts whether the value of the cookie contains a cookieValue. For example: get(\u0026#34;/x\u0026#34;).then() .assertThat().cookies(\u0026#34;cookieName1\u0026#34;, \u0026#34;cookieValue1\u0026#34;, \u0026#34;cookieName2\u0026#34;, containsString(\u0026#34;Value2\u0026#34;)) Status Code Assertion Assertion Whether the status code is equal to 200. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().statusCode(200) Assertion Whether the status line is something. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().statusLine(\u0026#34;something\u0026#34;) Assertion Whether the status line contains some. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().statusLine(containsString(\u0026#34;some\u0026#34;)) Header Assertion Asserts whether the value of Header is equal to HeaderValue. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().header(\u0026#34;headerName\u0026#34;, \u0026#34;headerValue\u0026#34;) Asserts whether the value of multiple Headers is equal to HeaderValue at the same time. For example: get(\u0026#34;/x\u0026#34;).then() .assertThat().headers(\u0026#34;headerName1\u0026#34;, \u0026#34;headerValue1\u0026#34;, \u0026#34;headerName2\u0026#34;, \u0026#34;headerValue2\u0026#34;) Asserts whether the value of the Header contains a HeaderValue. For example: get(\u0026#34;/x\u0026#34;).then().assertThat() .headers(\u0026#34;headerName1\u0026#34;, \u0026#34;headerValue1\u0026#34;, \u0026#34;headerName2\u0026#34;, containsString(\u0026#34;Value2\u0026#34;)) Assert that the \u0026ldquo;Content-Length\u0026rdquo; of the Header is less than 1000. For example: The header can be first converted to int using the mapping function, and then asserted using the \u0026ldquo;integer\u0026rdquo; matcher before validation with Hamcrest:\nget(\u0026#34;/something\u0026#34;).then().assertThat().header(\u0026#34;Content-Length\u0026#34;, Integer::parseInt, lessThan(1000)); Content-Type Assertion Asserts whether the value of Content-Type is equal to application/json. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().contentType(ContentType.JSON) Full body/content matching Assertion Assertion Whether the response body is exactly equal to something. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().body(equalTo(\u0026#34;something\u0026#34;)) Measuring Response Time As of version 2.8.0 REST Assured has support measuring response time. For example:\nlong timeInMs = get(\u0026#34;/lotto\u0026#34;).time() or using a specific time unit:\nlong timeInSeconds = get(\u0026#34;/lotto\u0026#34;).timeIn(SECONDS); where \u0026lsquo;SECONDS\u0026rsquo; is just a standard \u0026lsquo;TimeUnit\u0026rsquo;. You can also validate it using the validation DSL:\nwhen(). get(\u0026#34;/lotto\u0026#34;). then(). time(lessThan(2000L)); // Milliseconds or\nwhen(). get(\u0026#34;/lotto\u0026#34;). then(). time(lessThan(2L), SECONDS); Note that you can only referentially correlate these measurements to server request processing times (as response times will include HTTP roundtrips, REST Assured processing times, etc., and cannot be very accurate).\nFile Upload Often we use the multipart form data technique when transferring large amounts of data to the server, such as files. rest-assured provides a multiPart method to recognize whether this is a file, a binary sequence, an input stream, or uploaded text.\nUpload only one file in the form. For example: given(). multiPart(new File(\u0026#34;/path/to/file\u0026#34;)). when(). post(\u0026#34;/upload\u0026#34;); Uploading a file in the presence of a control name. For example: given(). multiPart(\u0026#34;controlName\u0026#34;, new File(\u0026#34;/path/to/file\u0026#34;)). when(). post(\u0026#34;/upload\u0026#34;); Multiple \u0026ldquo;multi-parts\u0026rdquo; entities in the same request. For example: byte[] someData = .. given(). multiPart(\u0026#34;controlName1\u0026#34;, new File(\u0026#34;/path/to/file\u0026#34;)). multiPart(\u0026#34;controlName2\u0026#34;, \u0026#34;my_file_name.txt\u0026#34;, someData). multiPart(\u0026#34;controlName3\u0026#34;, someJavaObject, \u0026#34;application/json\u0026#34;). when(). post(\u0026#34;/upload\u0026#34;); MultiPartSpecBuilder use cases. For example: For more usage referencesMultiPartSpecBuilder：\nGreeting greeting = new Greeting(); greeting.setFirstName(\u0026#34;John\u0026#34;); greeting.setLastName(\u0026#34;Doe\u0026#34;); given(). multiPart(new MultiPartSpecBuilder(greeting, ObjectMapperType.JACKSON_2) .fileName(\u0026#34;greeting.json\u0026#34;) .controlName(\u0026#34;text\u0026#34;) .mimeType(\u0026#34;application/vnd.custom+json\u0026#34;).build()). when(). post(\u0026#34;/multipart/json\u0026#34;). then(). statusCode(200); MultiPartConfig use cases. For example: MultiPartConfigYou can specify the default control name and file name.\ngiven().config(config().multiPartConfig(multiPartConfig().defaultControlName(\u0026#34;something-else\u0026#34;))) By default, the control name is configured as \u0026ldquo;something-else\u0026rdquo; instead of \u0026ldquo;file\u0026rdquo;. For more usage references blog introduction\nLogging When we are writing interface test scripts, we may need to print some logs during the test process so that we can view the request and response information of the interface and some other information during the test process.RestAssured provides some methods to print logs.\nRestAssured provides a global logging configuration method that allows you to configure logging before the test starts and then print the logs during the test. This method is applicable to all test cases, but it can only print request and response information, not other information.\nRestAssured also provides a localized log configuration method that prints logs during the test. This method prints request and response information as well as other information.\nGlobal logging configuration Steps to add global logging configuration Importing logging-related dependency classes import io.restassured.config.LogConfig; import io.restassured.filter.log.LogDetail; import io.restassured.filter.log.RequestLoggingFilter; import io.restassured.filter.log.ResponseLoggingFilter; Adding logging configuration to the setup() method Use LogConfig configuration to enable logging of requests and responses, as well as to enable nice output formatting. Enabled logging filters for requests and responses, which will log details of requests and responses.\n// Setting the Global Request and Response Logging Configuration RestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL) .enablePrettyPrinting(true)); Enabled global logging filters in the setup() method // Enable global request and response logging filters RestAssured.filters(new RequestLoggingFilter(), new ResponseLoggingFilter()); Global Logging Code Example package com.example; import io.restassured.RestAssured; // Importing logging-related dependency classes import io.restassured.config.LogConfig; import io.restassured.filter.log.LogDetail; import io.restassured.filter.log.RequestLoggingFilter; import io.restassured.filter.log.ResponseLoggingFilter; import org.testng.annotations.BeforeClass; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; public class TestDemo { @BeforeClass public void setup() { // Setting the Global Request and Response Logging Configuration RestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL) .enablePrettyPrinting(true)); // Enable global request and response logging filters RestAssured.filters(new RequestLoggingFilter(), new ResponseLoggingFilter()); } @Test(description = \u0026#34;Verify that the Get Post API returns correctly\u0026#34;) public void verifyGetAPI() { // Test cases have been omitted, refer to the demo } @Test(description = \u0026#34;Verify that the publish post API returns correctly\u0026#34;) public void verifyPostAPI() { // Test cases have been omitted, refer to the demo } } Viewing Global Log Output Open the Terminal window for this project and run the test script by executing the following command Viewing Log Output Localized logging configuration In RestAssured, you can make localized logging configurations to enable or disable logging for specific test methods or requests without affecting the global configuration.\nSteps to add Localized logging configuration Add logging configuration is enabled in the test method for which you want to print logs @Test(description = \u0026#34;Verify that the Get Post API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .log().everything(true) // Output request-related logs .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .log().everything(true) // Output response-related logs .statusCode(200) } Viewing Localized Log Output Open the Terminal window for this project and run the test script by executing the following command Viewing Log Output LogConfig Configuration Description In Rest-Assured, you can use the LogConfig class to configure logging of requests and responses. The LogConfig allows you to define the level of logging detail, the output format, the location of the output, and so on. The following are some common LogConfig configuration examples:\nEnable logging of requests and responses:\nRestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL));; This will enable logging of requests and responses only if validation fails.\nConfigure the output level:\nRestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.HEADERS));; This will log only the request and response headers.\nConfigure the location of the output:\nRestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL) .enablePrettyPrinting(true) .defaultStream(FileOutputStream(\u0026#34;log.txt\u0026#34;))); This outputs the log records to a file named \u0026ldquo;log.txt\u0026rdquo;.\nConfigure the nice output format:\nRestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL) .enablePrettyPrinting(true)); This will enable nice output formatting and make the logs easier to read.\nYou can combine these configuration options according to your specific needs and set it to RestAssured.config to configure global request and response logging. This will help log and review requests and responses in RestAssured for debugging and analyzing issues.\nRequest Logging Starting with version 1.5, REST Assured supports logging request specifications before they are sent to the server using RequestLoggingFilter. Note that HTTP Builder and HTTP Client may add headers other than what is printed in the log. The filter will only log the details specified in the request specification. That is, you cannot consider the details logged by the RequestLoggingFilter to be the details actually sent to the server. In addition, subsequent filters may change the request after logging has occurred. If you need to log what is actually sent over the network, see the HTTP Client Logging documentation or use an external tool such as fiddler.\nExamples：\ngiven().log().all() // Log all request specification details including parameters, headers and body given().log().params() // Log only the parameters of the request given().log().body() // Log only the request body given().log().headers() // Log only the request headers given().log().cookies() // Log only the request cookies given().log().method() // Log only the request method given().log().path() // Log only the request path Response Logging Wanting to print only the body of the response, regardless of the status code, you can do the following. , for example: get(\u0026#34;/x\u0026#34;).then().log().body() The response body will be printed whether or not an error occurs. If only interested in printing the response body when an error occurs, for example: get(\u0026#34;/x\u0026#34;).then().log().ifError() Record all details in the response, including status lines, headers, and cookies, for example: get(\u0026#34;/x\u0026#34;).then().log().all() Record only the status line, header, or cookie in the response, for example: get(\u0026#34;/x\u0026#34;).then().log().statusLine() // Only log the status line get(\u0026#34;/x\u0026#34;).then().log().headers() // Only log the response headers get(\u0026#34;/x\u0026#34;).then().log().cookies() // Only log the response cookies Configured to log a response only when the status code matches a value. for example: get(\u0026#34;/x\u0026#34;).then().log().ifStatusCodeIsEqualTo(302) // Only log if the status code is equal to 302 get(\u0026#34;/x\u0026#34;).then().log().ifStatusCodeMatches(matcher) // Only log if the status code matches the supplied Hamcrest matcher Log if validation fails Since REST Assured 2.3.1 you can log the request or response only if the validation fails. To log the request do. for example: given().log().ifValidationFails() To log the response. for example: then().log().ifValidationFails() It can be enabled for both requests and responses using LogConfig, for example: given().config(RestAssured.config().logConfig(logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(HEADERS))) If authentication fails, the log only records the request header.\nAnother shortcut to enable request and response logging for all requests if authentication fails, for example: RestAssured.enableLoggingOfRequestAndResponseIfValidationFails(); Starting with version 4.5.0, you can also use specify the message that will be displayed if the onFailMessage test fails, for example: when(). get(). then(). onFailMessage(\u0026#34;Some specific message\u0026#34;). statusCode(200); Header Blacklist Configuration Starting with REST Assured 4.2.0, it is possible to blacklist headers so that they do not show up in request or response logs. Instead, the header value will be replaced with [ BLACKLISTED ] . You can enable this feature on a per-header basis using LogConfig, for example:\ngiven().config(config().logConfig(logConfig().blacklistHeader(\u0026#34;Accept\u0026#34;))) Filters In RestAssured, you can use filters to modify requests and responses. Filters allow you to modify requests and responses at different stages of the request and response process. For example, you can modify the request before the request or the response after the response. You can use filters to add request headers, request parameters, request bodies, response headers, response bodies, and so on.\nFilters can be used to implement custom authentication schemes, session management, logging, and so on. To create a filter, you need to implement the io.restassured.filter.Filter interface. To use a filter, you can do the following:\ngiven().filter(new MyFilter()) There are a couple of filters provided by REST-Assured that are ready to use:\nio.restassured.filter.log.RequestLoggingFilter: A filter that\u0026rsquo;ll print the request specification details. io.restassured.filter.log.ResponseLoggingFilter: A filter that\u0026rsquo;ll print the response details if the response matches a given status code. io.restassured.filter.log.ErrorLoggingFilter: A filter that\u0026rsquo;ll print the response body if an error occurred (status code is between 400 and 500). Ordered Filters As of REST Assured 3.0.2 you can implement the io.restassured.filter.OrderedFilter interface if you need to control the filter ordering. Here you implement the getOrder method to return an integer representing the precedence of the filter. A lower value gives higher precedence. The highest precedence you can define is Integer.MIN_VALUE and the lowest precedence is Integer.MAX_VALUE. Filters not implementing io.restassured.filter.OrderedFilter will have a default precedence of 1000.\nexamples\nResponse Builder If you need to change the Response from a filter you can use the ResponseBuilder to create a new Response based on the original response. For example if you want to change the body of the original response to something else you can do:\nResponse newResponse = new ResponseBuilder() .clone(originalResponse).setBody(\u0026#34;Something\u0026#34;).build(); ","permalink":"https://naodeng.tech/posts/api-automation-testing/rest-assured-tutorial-advance-usage-verifying-response-and-logging/","summary":"provide an in-depth look at advanced uses of REST Assured, with a focus on validating API responses, logging, and the application of filters.","title":"REST Assured API Automation Testing Tutorial: Advanced Usage - Validating Responses and Logging, Filters, File Uploads"},{"content":"Building a REST Assured API test project from 0 to 1 REST Assured supports both Gradle and Maven build tools, you can choose one of them according to your preference. Below is a description of the initialization process for Gradle and Maven build tools.\nThis project is built using Gradle 8.44 and Maven 3.9.5, if you are using other versions, it may be different.\nGradle version See the demo project at https://github.com/Automation-Test-Starter/RestAssured-gradle-demo.\nInitialize an empty Gradle project mkdir RestAssured-gradle-demo cd RestAssured-gradle-demo gradle init Configuration build.gradle The demo project introduces the testNG testing framework. For reference only.\nCreate a build.gradle file in the project root directory to configure the project. For reference, the following is a sample configuration // plugins configuration plugins { id \u0026#39;java\u0026#39; // use java plugin } // repositories configuration repositories { mavenCentral() // user maven central repository } // dependencies configuration dependencies { testImplementation \u0026#39;io.rest-assured:rest-assured:5.3.1\u0026#39; // add rest-assured dependency testImplementation \u0026#39;org.testng:testng:7.8.0\u0026#39; // add testng testing framework dependency implementation \u0026#39;org.uncommons:reportng:1.1.4\u0026#39; // add testng reportng dependency implementation \u0026#39;org.slf4j:slf4j-api:2.0.9\u0026#39; // add slf4j dependency for test logging implementation \u0026#39;org.slf4j:slf4j-simple:2.0.9\u0026#39; // add slf4j dependency for test logging implementation group: \u0026#39;com.google.inject\u0026#39;, name: \u0026#39;guice\u0026#39;, version: \u0026#39;7.0.0\u0026#39; } // test configuration test { reports.html.required = false // set gradle html report to false reports.junitXml.required = false // set gradle junitXml report to false // use testng testing framework useTestNG() { useDefaultListeners = true suites \u0026#39;src/test/resources/testng.xml\u0026#39; // set testng.xml file path } testLogging.showStandardStreams = true // output test log to console testLogging.events \u0026#34;passed\u0026#34;, \u0026#34;skipped\u0026#34;, \u0026#34;failed\u0026#34; // deny output test log to console } You can copy the contents of the build.gradle file in this project. For more configuration refer to Official Documentation\ntestng.xml configuration Create a resources directory under the src/test directory to store test configuration files.\nCreate a testng.xml file in the resources directory to configure the TestNG test framework.\nFor reference, the following is a sample configuration\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE suite SYSTEM \u0026#34;http://testng.org/testng-1.0.dtd\u0026#34;\u0026gt; \u0026lt;suite name=\u0026#34;restAssured-gradleTestSuite\u0026#34;\u0026gt; \u0026lt;test thread-count=\u0026#34;1\u0026#34; name=\u0026#34;Demo\u0026#34;\u0026gt; \u0026lt;classes\u0026gt; \u0026lt;class name=\u0026#34;com.example.TestDemo\u0026#34;/\u0026gt; \u0026lt;!-- test case class--\u0026gt; \u0026lt;/classes\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;/suite\u0026gt; \u0026lt;!-- Suite --\u0026gt; gradle build project and initialize Open the Terminal window of the project with an editor and execute the following command to confirm that the project build was successful gradle build Initialization complete: After completing the wizard, Gradle will generate a basic Gradle project structure in the project directory initialization project directory The directory structure can be found in \u0026raquo; Project structure\nCreate a new test class in the project\u0026rsquo;s test source directory. By default, Gradle usually places the test source code in the src/test/java directory. You can create a package of test classes in that directory and create a new test class in the package\nTo create a test class for TestDemo, you can create files with the following structure\nsrc └── test └── java └── com └── example └── TestDemo.java Introduction of demo test API Get API HOST: https://jsonplaceholder.typicode.com API path: /posts/1 Request method: GET Request Parameters: None Request header: \u0026ldquo;Content-Type\u0026rdquo;: \u0026ldquo;application/json; charset=utf-8\u0026rdquo; Request Body: None Response status code: 200 Response header: \u0026ldquo;Content-Type\u0026rdquo;: \u0026ldquo;application/json; charset=utf-8\u0026rdquo; Response body: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; } Post API HOST: https://jsonplaceholder.typicode.com API path:/posts Request method: POST Request Parameters: None Request header:\u0026ldquo;Content-Type\u0026rdquo;: \u0026ldquo;application/json; charset=utf-8\u0026rdquo; Request Body:raw json format Request Body: { \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } Response status code: 201 Response header:\u0026ldquo;Content-Type\u0026rdquo;: \u0026ldquo;application/json; charset=utf-8\u0026rdquo; Response body: { \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 } Writing Test cases Open the TestDemo.java file and start writing the test script.\nThe example script is as follows. For reference\npackage com.example; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; public class TestDemo { @Test(description = \u0026#34;Verify that the Get Post API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .statusCode(200) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(1)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } @Test(description = \u0026#34;Verify that the publish post API returns correctly\u0026#34;) public void verifyPostAPI() { // Given given() .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .body(\u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;foo\\\u0026#34;, \\\u0026#34;body\\\u0026#34;: \\\u0026#34;bar\\\u0026#34;, \\\u0026#34;userId\\\u0026#34;: 1\\n}\u0026#34;) .post(\u0026#34;/posts\u0026#34;) // Then .then() .statusCode(201) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(101)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;foo\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;bar\u0026#34;)); } } Debugging test cases Open the Terminal window for this project and run the test script by executing the following command gradle test Viewing Test Reports Command Line Report testng html Report Open the project build/reports/tests/test directory. Click on the index.html file to view the test report. Maven version See the demo project at https://github.com/Automation-Test-Starter/RestAssured-maven-demo\nInitialize an empty Maven project mvn archetype:generate -DgroupId=com.example -DartifactId=RestAssured-maven-demo -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false Initialization complete: After completing the wizard, Maven will create a new project directory and a basic Maven project structure\nConfiguration pom.xml Add the following to the pom.xml file in your project\nYou can copy the contents of the pom.xml file in this project. For more information on configuration, please refer to the official documentation.\n\u0026lt;!-- dependencies config --\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/io.rest-assured/rest-assured --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.rest-assured\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rest-assured\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.3.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.testng/testng --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.testng\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;testng\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;7.8.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;!-- plugin config --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;suiteXmlFiles\u0026gt; \u0026lt;suiteXmlFile\u0026gt;src/test/resources/testng.xml\u0026lt;/suiteXmlFile\u0026gt; \u0026lt;/suiteXmlFiles\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; Configuration testng.xml Create a resources directory under the src/test directory to store test configuration files.\nCreate a testng.xml file in the resources directory to configure the TestNG test framework.\nFor reference, the following is a sample configuration\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE suite SYSTEM \u0026#34;http://testng.org/testng-1.0.dtd\u0026#34;\u0026gt; \u0026lt;suite name=\u0026#34;restAssured-gradleTestSuite\u0026#34;\u0026gt; \u0026lt;test thread-count=\u0026#34;1\u0026#34; name=\u0026#34;Demo\u0026#34;\u0026gt; \u0026lt;classes\u0026gt; \u0026lt;class name=\u0026#34;com.example.TestDemo\u0026#34;/\u0026gt; \u0026lt;!-- test case class--\u0026gt; \u0026lt;/classes\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;/suite\u0026gt; \u0026lt;!-- Suite --\u0026gt; initialization maven project directory The directory structure can be found in \u0026raquo; Project structure\nCreate a new test class in the project\u0026rsquo;s test source directory. By default, Gradle usually places the test source code in the src/test/java directory. You can create a package of test classes in that directory and create a new test class in the package\nTo create a test class for TestDemo, you can create files with the following structure\nsrc └── test └── java └── com └── example └── TestDemo.java The api used by Demo referable to \u0026raquo; Introduction of demo test API\nWriting Test cases Open the TestDemo.java file and start writing the test script.\nThe example script is as follows. For reference\npackage com.example; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; public class TestDemo { @Test(description = \u0026#34;Verify that the Get Post API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .statusCode(200) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(1)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } @Test(description = \u0026#34;Verify that the publish post API returns correctly\u0026#34;) public void verifyPostAPI() { // Given given() .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .body(\u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;foo\\\u0026#34;, \\\u0026#34;body\\\u0026#34;: \\\u0026#34;bar\\\u0026#34;, \\\u0026#34;userId\\\u0026#34;: 1\\n}\u0026#34;) .post(\u0026#34;/posts\u0026#34;) // Then .then() .statusCode(201) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(101)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;foo\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;bar\u0026#34;)); } } Debugging test cases Open the Terminal window for this project and run the test script by executing the following command mvn test Viewing Test Reports terminal report testng html report Open the project target/surefire-reports directory. Click on the index.html file to view the test report. More info Visit my personal blog: https://naodeng.tech/ My QA automation quickstart project page: https://github.com/Automation-Test-Starter ","permalink":"https://naodeng.tech/posts/api-automation-testing/rest-assured-tutorial-building-your-own-project-from-0-to-1/","summary":"dive into how to build a REST Assured API automation testing project from scratch.","title":"REST Assured API Automation Testing Tutorial: Building a REST Assured API Automation Test project from 0 to 1"},{"content":"Introduction of RestAssured REST Assured is a Java testing framework for testing RESTful APIs that enables developers/testers to easily write and execute API tests. It is designed to make API testing simple and intuitive, while providing rich functionality and flexibility. The following are some of the key features and uses of REST Assured:\nInitiating HTTP requests: REST Assured allows you to easily build and initiate HTTP GET, POST, PUT, DELETE and other types of requests. You can specify the request\u0026rsquo;s URL, headers, parameters, body, and other information.\nChained Syntax: REST Assured uses chained syntax to make test code more readable and easy to write. You can describe your test cases in a natural way without writing tons of code.\nAssertions and Checksums: REST Assured provides a rich set of checksums that can be used to validate API response status codes, response bodies, response headers, and so on. You can add multiple assertions according to your testing needs.\nSupport for multiple data formats: REST Assured supports a variety of data formats, including JSON, XML, HTML, Text and so on. You can use appropriate methods to handle different formats of response data.\nIntegration with BDD (Behavior-Driven Development): REST Assured can be used in conjunction with BDD frameworks (such as Cucumber), allowing you to better describe and manage test cases.\nSimulate HTTP Server: REST Assured also includes a simulation of an HTTP server, allowing you to simulate the behavior of an API for end-to-end testing.\nExtensibility: REST Assured can be customized with plug-ins and extensions to meet specific testing needs.\nOverall, REST Assured is a powerful and easy-to-use API testing framework that helps you easily perform RESTful API testing and provides many tools to verify the correctness and performance of an API. Whether you are a beginner or an experienced developer/tester, REST Assured is a valuable tool for quickly getting started with API automation testing.\nProject structure Gradle-built versions - src - main - java - (The main source code of the application) - test - test - api - (REST Assured test code) - UsersAPITest.java - ProductsAPITest.java - TestConfig.java - TestConfig.java - resources - (configuration files, test data, etc.) - (other project files and resources) - build.gradle (Gradle project configuration file) In this example directory structure:\nsrc/test/java/api directory is used to hold REST Assured test classes, each of which typically involves tests for one or more related API endpoints. For example, UsersAPITest.java and ProductsAPITest.java could contain tests for user management and product management. The src/test/java/util directory can be used to store tool classes that are shared among tests, such as TestConfig.java for configuring REST Assured. The src/test/resources directory can contain test data files, configuration files, and other resources that can be used in tests. build.gradle is the gradle project\u0026rsquo;s configuration file, which is used to define the project\u0026rsquo;s dependencies, build configuration, and other project settings. Maven-built versions - src - main - java - (The main source code of the application) - test - java - api - (REST Assured test code) - UsersAPITest.java - ProductsAPITest.java - util - TestConfig.java - resources - (configuration files, test data, etc.) - (other project files and resources) - pom.xml (Maven project configuration file) In this example directory structure:\nsrc/test/java/api directory is used to hold REST Assured test classes, each of which typically involves tests for one or more related API endpoints. For example, UsersAPITest.java and ProductsAPITest.java could contain tests for user management and product management. The src/test/java/util directory can be used to store tool classes that are shared among tests, such as TestConfig.java for configuring REST Assured. The src/test/resources directory can contain test data files, configuration files, and other resources that can be used in the tests. pom.xml is a Maven project configuration file that is used to define project dependencies, build configurations, and other project settings. Project dependency JDK 1.8+, I\u0026rsquo;m using JDK 19 Gradle 6.0+ or Maven 3.0+, I\u0026rsquo;m using Gradle 8.44 and Maven 3.9.5 RestAssured 4.3.3+, I\u0026rsquo;m using the latest version 5.3.2 Syntax Example Here\u0026rsquo;s a simple example of RestAssured syntax that demonstrates how to perform a GET request and validate the response: First, make sure you have added a RestAssured dependency to your Gradle or Maven project.\nFirst, make sure you have added a RestAssured dependency to your Gradle or Maven project.\nGradle dependency:\ndependency { implementation \u0026#39;io.rest-assured:rest-assured:5.3.1\u0026#39; } Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.rest-assured\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rest-assured\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.3.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Next, create a test class and write the following code:\nimport io.restassured.RestAssured; import io.restassured.response.Response; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; public class RestAssuredDemo { @Test public void testGetRequest() { // Set the base URI, using JSONPlaceholder as an example RestAssured.baseURI = \u0026#34;https://jsonplaceholder.typicode.com\u0026#34;; // Send a GET request and save the response Response response = given() .when() .get(\u0026#34;/posts/1\u0026#34;) .then() .extract() .response(); // Print the JSON content of the response System.out.println(\u0026#34;Response JSON: \u0026#34; + response.asString()); // Verify that the status code is 200. // Validate that the status code is 200 response.then().statusCode(200); // validate that the response has a status code of 200. // Validate a specific field value in the response response.then().body(\u0026#34;userId\u0026#34;, equalTo(1)); response.then().body(\u0026#34;id\u0026#34;, equalTo(1)); response.then().body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)); response.then().body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } } The above code executes a GET request to JSONPlaceholder\u0026rsquo;s /posts/1 endpoint and validates the response with a status code and values for specific fields. You can modify the base URI and validation conditions to suit your needs.\nIn this example, we\u0026rsquo;re using the TestNG testing framework, but you can also use other testing frameworks such as JUnit. make sure your test classes contain the appropriate import statements and configure them appropriately as needed.\nThis is a simple example of RestAssured syntax for performing a GET request and validating the response. You can build more complex test cases based on the needs of your project and the complexity of your interface.\n","permalink":"https://naodeng.tech/posts/api-automation-testing/rest-assured-tutorial-and-environment-preparation/","summary":"a tutorial on REST Assured, focusing on getting started and preparing the environment to be built.","title":"REST Assured API Automation Testing Tutorial: Getting Started and Setting Up Your Environment"},{"content":"CI/CD Integration Accessing github action Take github action as an example, and other CI tools as well.\nGradle + Scala version See the demo at https://github.com/Automation-Test-Starter/gatling-gradle-scala-demo.\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate the workflow file: Create a YAML-formatted workflow file, such as gatling.yml, in the .github/workflows directory.\nEdit the gatling.yml file: Copy the following into the file.\nname: Gatling Performance Test on: push: branches: - main jobs: performance-test: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Java uses: actions/setup-java@v2 with: java-version: 11 distribution: \u0026#39;adopt\u0026#39; - name: Run Gatling tests run: | ./gradlew gatlingRun env: GATLING_SIMULATIONS_FOLDER: src/gatling/scala - name: Archive Gatling results uses: actions/upload-artifact@v2 with: name: gatling-results path: build/reports/gatling - name: Upload Gatling results to GitHub uses: actions/upload-artifact@v2 with: name: gatling-results path: build/reports/gatling Commit the code: Add the gatling.yml file to your repository and commit. View the test report: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Performance Test workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. Maven + Scala version See the demo at https://github.com/Automation-Test-Starter/gatling-maven-scala-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate the workflow file: Create a YAML-formatted workflow file, such as gatling.yml, in the .github/workflows directory.\nEdit the gatling.yml file: Copy the following into the file.\nname: Gatling Performance Test on: push: branches: - main jobs: performance-test: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Java uses: actions/setup-java@v2 with: java-version: 11 distribution: \u0026#39;adopt\u0026#39; - name: Run Gatling tests run: | mvn gatling:test env: GATLING_SIMULATIONS_FOLDER: src/test/scala - name: Archive Gatling results uses: actions/upload-artifact@v2 with: name: gatling-results path: target/gatling - name: Upload Gatling results to GitHub uses: actions/upload-artifact@v2 with: name: gatling-results path: target/gatling Commit the code: Add the gatling.yml file to your repository and commit. View the test report: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Performance Test workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. reference galting official website: https://gatling.io/ galting official documentation: https://gatling.io/docs/gatling/ galting official github: https://github.com/gatling/ ","permalink":"https://naodeng.tech/posts/performance-testing/gatling-tool-tutorial-ci-cd-integration/","summary":"This article introduces the advanced usage of the performance testing tool gatling: CI/CD integration, using github action as an example to introduce how to integrate gatling into the CI/CD process.","title":"Gatling Performance Testing Tutorial advanced usage: CI/CD Integration"},{"content":"Test report analysis Overview Overall view Open the detailed html report after the performance test execution is finished; Your report can be analyzed by metrics, active users and requests/responses over time, as well as distributions\nThe name of Simulation is displayed in the center of the page in the header The list on the left side shows a menu of different types of reports, which can be switched by clicking on them. The middle of the page shows an overview of the performance test report, including: total number of requests, total number of successful requests, total number of unsuccessful requests, shortest response time, longest response time, average response time, throughput, standard deviation, percentage distribution, etc. It also shows the version of gatling and the time and duration of this report. The version of gatling and the time and duration of this report run are also displayed. The Global menu points to aggregate statistics. The Details menu points to statistics for each request type. Response time ranges This chart shows the distribution of response times within the standard range The list on the left shows all requests and the distribution of request response times, with the red color representing failed requests. On the right, Number of requests represents the number of concurrent users, as well as the number of requests for each request and their success and failure status.\nThese ranges can be configured in the gatling.conf file\nSummary This chart shows some standard statistics such as minimum, maximum, average, standard deviation and percentile for global and per request. stats shows the specific success and failure of all requests OK for success, KO for failure, and 99th pct for 99th percentile response time for total requests for this API.\nThese percentiles can be configured in the gatling.conf file.\nActive users over time This chart shows that the number of active users refers to the number of users who are making requests during the test time period. At the beginning of the test, the number of active users is 0. When users start sending requests, the number of active users starts to increase. When a user completes a request, the number of active users begins to decrease. The maximum number of active users is the number of users sending requests at the same time during the test period.\nResponse time distribution This chart shows the distribution of response times, including response times for successful requests and response times for failed requests.\nResponse time percentiles over time This chart shows various response time percentiles over time, but only for successful requests. Since failed requests may end early or be caused by timeouts, they can have a huge impact on the percentile calculation.\nRequests per second over time This chart shows the number of requests per second, including the number of successful requests and the number of failed requests.\nResponse per second over time This chart shows the number of responses per second, including the number of successful responses and the number of failed responses.\nSingle request analysis report You can click the details menu on the report page to switch to the details tab and view a detailed report for a single request.\nThe Details page primarily shows per-request statistics, and similarly to the global report includes a graph of response time distribution, response time percentile, requests per second, and responses per second. The difference is that there is a graph at the bottom that depicts the response time of a single request relative to all requests globally. The horizontal coordinate of this graph is the number of all requests per second globally, and the vertical coordinate is the response time of a single request.\nPerformance Scenario Setting Injection What is Injection In Gatling performance testing, \u0026ldquo;Injection\u0026rdquo; refers to a method of introducing virtual users (or load) into the system. It defines how simulated users are introduced into a test scenario, including the number, rate, and manner of users.Injection is a key concept used in Gatling to control load and concurrency, allowing you to simulate different user behaviors and load models.\nUser injection profiles are defined using the injectOpen and injectClosed methods (inject in Scala). This method takes as arguments a sequence of injection steps that are processed sequentially. Each step defines a set of users and how these users are injected into the scene.\nMore from the web site: https://gatling.io/docs/gatling/reference/current/core/injection/\nCommon Injection Scenario Open Model Scenario setUp( scn.inject( nothingFor(4), // 1 atOnceUsers(10), // 2 rampUsers(10).during(5), // 3 constantUsersPerSec(20).during(15), // 4 constantUsersPerSec(20).during(15).randomized, // 5 rampUsersPerSec(10).to(20).during(10.minutes), // 6 rampUsersPerSec(10).to(20).during(10.minutes).randomized, // 7 stressPeakUsers(1000).during(20) // 8 ).protocols(httpProtocol) ) nothingFor(duration): set a period of time to stop, this time to do nothing atOnceUsers(nbUsers): immediately inject a certain number of virtual users rampUsers(nbUsers) during(duration): set a certain number of virtual users to be injected gradually during a specified period of time. constantUsersPerSec(rate) during(duration): Define a constant number of concurrent users per second for a specified period of time. constantUsersPerSec(rate) during(duration) randomized: defines a randomized concurrency increase/decrease around a specified number of concurrencies per second, for a specified period of time rampUsersPerSec(rate1) to (rate2) during(duration): defines a concurrency interval that runs for the specified time, with the concurrency growth period being a regular value. rampUsersPerSec(rate1) to (rate2) during(duration) randomized: define a concurrency interval, run for a specified time, the concurrency growth period is a random value stressPeakUsers(nbUsers).during(duration) : injects a given number of users according to a smooth approximation of a step function that stretches to a given duration. users. Closed Model Scenario setUp( scn.inject( constantConcurrentUsers(10).during(10), // 1 rampConcurrentUsers(10).to(20).during(10) // 2 ) ) constantConcurrentUsers(fromNbUsers).during(duration) : inject to make the number of concurrent users in the system constant rampConcurrentUsers(fromNbUsers).to(toNbUsers).during(duration) : inject so that the number of concurrent users in the system increases linearly from one number to the next Meta DSL Scenario \u0026ldquo;Meta DSL is a special Domain Specific Language (DSL) for describing the metadata and global configuration of performance test scenarios.Meta DSL allows you to define a number of global settings and parameters in a performance test that affect the entire test process, rather than being specific to a particular scenario.\nThe elements of the Meta DSL can be used to write tests in a simpler way. If you want to link levels and ramps to reach the limits of your application (a test sometimes referred to as a capacity load test), you can do this manually using the regular DSL and looping with map and flatMap.\nincrementUsersPerSec setUp( // Generate an open workload injection profile // 10, 15, 20, 25 and 30 users arrive every second // Each level lasts 10 seconds // Each level lasts 10 seconds scn.inject( incrementUsersPerSec(5.0) .times(5) .eachLevelLasting(10) .separatedByRampsLasting(10) .startingFrom(10) // Double ) incrementConcurrentUsers setUp( // Generate a closed workload injection profile // Concurrent users at levels 10, 15, 20, 25, and 30 // Each level lasts 10 seconds // Each level lasts 10 seconds scn.inject( incrementConcurrentUsers(5) .times(5) .eachLevelLasting(10) .separatedByRampsLasting(10) .startingFrom(10) // Int ) ) incrementUsersPerSec is used for open workloads, incrementConcurrentUsers is used for closed workloads (users/sec vs concurrent users).\nseparatedByRampsLasting and startingFrom are both optional. If you do not specify a ramp, the test jumps from one level to another as soon as it finishes. If you do not specify the number of starting users, the test will start with 0 concurrent users or 0 users per second and move to the next step immediately.\nConcurrent Scenario setUp( scenario1.inject(injectionProfile1), scenario2.inject(injectionProfile2) ) You can configure multiple scenes to start simultaneously and execute concurrently in the same setUp block.\nOther Scenarios Check out the website: https://gatling.io/docs/gatling/reference/current/core/injection/\n","permalink":"https://naodeng.tech/posts/performance-testing/gatling-tool-tutorial-advanced-usage/","summary":"This article introduces the advanced usage of the performance testing tool gatling: analysis of performance test reports, introduction of different types of test report reports, and configuration of performance test scenarios under different business types.","title":"Gatling Performance Testing Tutorial advanced usage: Test report analysis and Performance Scenario Setting"},{"content":"Build your own Gatling project from 0 to 1 Gradle + Scala versions Create an empty Gradle project mkdir gatling-gradle-demo cd gatling-gradle-demo gradle init Configure the project build.gradle Add the following to the build.gradle file in the project\nYou can copy the content of the build.gradle file in this project, for more configurations, please refer to the official documentation.\n// Plugin Configuration plugins { id \u0026#39;scala\u0026#39; // scala plugin declaration (based on the development tools plugin) id \u0026#39;io.gatling.gradle\u0026#39; version \u0026#39;3.9.5.6\u0026#39; // declaration of the version of the gradle-based gatling framework plugin } // Repository source configuration repositories { // Use the maven central repository source mavenCentral() } // gatling configuration gatling { // logback root level, defaults to the Gatling console log level if logback.xml does not exist in the configuration folder logLevel = \u0026#39;WARN\u0026#39; // Enforce logging of HTTP requests at a level of detail // set to \u0026#39;ALL\u0026#39; for all HTTP traffic in TRACE, \u0026#39;FAILURES\u0026#39; for failed HTTP traffic in DEBUG logHttp = \u0026#39;FAILURES\u0026#39; // Simulations filter simulations = { include \u0026#34;**/simulation/*.scala\u0026#34; } } // Dependencies dependencies { // Charts library for generating report charts gatling \u0026#39;io.gatling.highcharts:gatling-charts-highcharts:3.8.3\u0026#39; } gradle build project and initialize Open the Terminal window of the project with an editor and execute the following command to confirm that the project build was successful gradle build Initialization complete: After completing the wizard, Gradle will generate a basic Gradle project structure in the project directory Initialization Directory Create a simulation directory in the src/gatling/scala directory to hold test scripts\nGatling tests are usually located in the src/gatling directory. You need to manually create the src directory in the project root, and then create the gatling directory under the src directory. In the gatling directory, you can create your test simulation folder simulation, as well as other folders such as data, bodies, resources, and so on.\nWriting Scripts Create a demo.scala file in the simulation directory to write your test scripts.\nFor reference, the following is a sample script\nThe script contains two scenarios, one for get requests and one for post requests. The get interface validates that the interface returns a status code of 200 and the post interface validates that the interface returns a status code of 201. The get interface uses rampUsers, the post interface uses constantConcurrentUsers. rampUsers: incrementally increase the number of concurrent users over a specified period of time, constantConcurrentUsers: keep the number of concurrent users constant over a specified period of time. The number of concurrent users is 10 for both interfaces, and the duration is 10 seconds for both interfaces. The request interval is 2 seconds for both interfaces.\npackage simulation import scala.concurrent.duration._ import io.gatling.core.Predef._ import io.gatling.http.Predef._ class demo extends Simulation { val httpProtocol = http .baseUrl(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) // 5 val scn = scenario(\u0026#34;GetSimulation\u0026#34;) .exec(http(\u0026#34;get_demo\u0026#34;) .get(\u0026#34;/posts/1\u0026#34;) .check(status.is(200))) .pause(2) val scn1 = scenario(\u0026#34;PostSimulation\u0026#34;) .exec(http(\u0026#34;post_demo\u0026#34;) .post(\u0026#34;/posts\u0026#34;) .body(StringBody(\u0026#34;\u0026#34;\u0026#34;{\u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;,\u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;,\u0026#34;userId\u0026#34;: 1}\u0026#34;\u0026#34;\u0026#34;)).asJson .check(status.is(201))) .pause(2) setUp( scn.inject(rampUsers(10) during(10 seconds)), scn1.inject(constantConcurrentUsers(10) during(10 seconds)) ).protocols(httpProtocol) } Debugging Scripts Execute the following command to run the test script and view the report\ngradle gatlingRun Maven + Scala version Create an empty Maven project mvn archetype:generate -DgroupId=demo.gatlin.maven -DartifactId=gatling-maven-demo Initialization complete: After completing the wizard, Maven will create a new project directory and generate a basic Maven project structure in the\nConfigure the project pom.xml Add the following contents to the pom.xml file in the project\nYou can copy the contents of the pom.xml file in this project, for more configuration, please refer to the official documentation.\n\u0026lt;!-- dependencies Configuration --\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.gatling.highcharts\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;gatling-charts-highcharts\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.9.5\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;!-- Plugin Configuration --\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.gatling\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;gatling-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.6.0\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.alchim31.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scala-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.8.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;scalaVersion\u0026gt;2.13.12\u0026lt;/scalaVersion\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;jvmArgs\u0026gt; \u0026lt;jvmArg\u0026gt;-Xss100M\u0026lt;/jvmArg\u0026gt; \u0026lt;/jvmArgs\u0026gt; \u0026lt;args\u0026gt; \u0026lt;arg\u0026gt;-deprecation\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;-feature\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;-unchecked\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;-language:implicitConversions\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;-language:postfixOps\u0026lt;/arg\u0026gt; \u0026lt;/args\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; Initialization Directory Create a simulation directory in the src/test/scala directory to hold the test scripts\nscala tests are usually located in the src/test directory. You need to create a scala directory under the project test directory. In the scala directory, you can create your test simulation folder simulation, as well as other folders such as data, bodies, resources, and so on.\nWriting Scripts Create a demo.scala file in the simulation directory to write your test scripts.\nFor reference, the following is a sample script\nThe script contains two scenarios, one for get requests and one for post requests. The get interface validates that the interface returns a status code of 200 and the post interface validates that the interface returns a status code of 201. The get interface uses rampUsers, the post interface uses constantConcurrentUsers. rampUsers: incrementally increase the number of concurrent users over a specified period of time, constantConcurrentUsers: keep the number of concurrent users constant over a specified period of time. The number of concurrent users is 10 for both interfaces, and the duration is 10 seconds for both interfaces. The request interval is 2 seconds for both interfaces.\npackage simulation import scala.concurrent.duration._ import io.gatling.core.Predef._ import io.gatling.http.Predef._ class demo extends Simulation { val httpProtocol = http .baseUrl(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) // 5 val scn = scenario(\u0026#34;GetSimulation\u0026#34;) .exec(http(\u0026#34;get_demo\u0026#34;) .get(\u0026#34;/posts/1\u0026#34;) .check(status.is(200))) .pause(2) val scn1 = scenario(\u0026#34;PostSimulation\u0026#34;) .exec(http(\u0026#34;post_demo\u0026#34;) .post(\u0026#34;/posts\u0026#34;) .body(StringBody(\u0026#34;\u0026#34;\u0026#34;{\u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;,\u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;,\u0026#34;userId\u0026#34;: 1}\u0026#34;\u0026#34;\u0026#34;)).asJson .check(status.is(201))) .pause(2) setUp( scn.inject(rampUsers(10) during(10 seconds)), scn1.inject(constantConcurrentUsers(10) during(10 seconds)) ).protocols(httpProtocol) } Debugging Scripts Execute the following command to run the test script and view the report\nmvn gatling:test ","permalink":"https://naodeng.tech/posts/performance-testing/gatling-tool-tutorial2/","summary":"The article introduces the performance testing tool gatling advanced introduction: from 0 to 1 build your own Gatling project, introduces the basic use of Gatling, and how to build your own Gatling project, write performance test scripts, view the test report and so on.","title":"gatling Performance Testing Tutorial: building your own gatling project from 0 to 1"},{"content":"Gatling Introduction Gatling is an open source tool for performance and load testing, especially for testing web applications. It is a high-performance tool based on the Scala programming language for simulating and measuring the performance of applications under different loads.\nHere are some of the key features and benefits of Gatling:\nBased on Scala programming language: Gatling\u0026rsquo;s test scripts are written in Scala, which makes it a powerful programming tool that allows users to write complex test scenarios and logic. High Performance: Gatling is designed as a high performance load testing tool. It uses non-blocking I/O and an asynchronous programming model that is capable of simulating large numbers of concurrent users to better mimic real-world load situations. Easy to learn and use: Although Gatling\u0026rsquo;s test scripts are written in Scala, its DSL (Domain Specific Language) is very simple and easy to learn. Even if you are not familiar with Scala, you can quickly learn how to create test scripts. Rich Features: Gatling provides a rich set of features, including request and response processing, data extraction, conditional assertions, performance report generation, and more. These features enable you to create complex test scenarios and perform comprehensive evaluation of application performance. Multi-Protocol Support: In addition to HTTP and HTTPS, Gatling supports other protocols such as WebSocket, JMS, and SMTP, making it suitable for testing a wide range of different types of applications. Real-time results analysis: Gatling provides real-time performance data and graphical reports during test runs to help you quickly identify performance issues. Open source and active community: Gatling is an open source project with an active community that constantly updates and improves the tool. CI/CD Integration Support: Gatling can be integrated with CI/CD tools such as Jenkins to perform performance testing in continuous integration and continuous delivery processes. Overall, Gatling is a powerful performance testing tool for testing a wide range of application types, helping development teams identify and resolve performance issues to ensure consistent performance and scalability of applications in production environments.\nEnvironment setup Since I\u0026rsquo;m a macbook user, I\u0026rsquo;ll use the macbook demo as an example in the introduction, but windows users can refer to it on their own.\nVSCode + Gradle + Scala Version Preparation Development tool: VSCode Install Gradle version \u0026gt;= 6.0, I am using Gradle 8.44. Install JDK version \u0026gt;= 8, I use JDK 19 install plugins VSCode search for Scala (Metals) plugin for installation VSCode search for Gradle for Java plugin for installation official demo initialization \u0026amp; debugging We will use the official demo project for initialization and debugging first, and then we will introduce how to create your own project later.\nClone the official demo project git clone git@github.com:gatling/gatling-gradle-plugin-demo-scala.git Open the cloned official demo project with VSCode.\nOpen the project\u0026rsquo;s Terminal window with VSCode and execute the following command\ngradle build Run the demo in the project gradle gatlingRun Viewing the results of a command line run Click on the html report link in the command line report and open it with your browser to view the detailed report information VSCode + Maven + Scala version Preparation Development tool: VSCode Install Maven, I use Maven 3.9.5 JDK version \u0026gt;= 8, I use JDK 19 install plugins VSCode search for Scala (Metals) plugins to install VSCode search for Maven for Java plugins to install Official demo initialization \u0026amp; debugging We will use the official demo project for initialization and debugging first, and then we will introduce how to create your own project.\nClone the official demo project git clone git@github.com:gatling/gatling-maven-plugin-demo-scala.git Use VSCode to open the cloned official demo project.\nOpen the Terminal window of this project with VSCode and execute the following command to run the demo in the project\nmvn gatling:test Viewing the results of a command line run Click on the html report link in the command line report and open it with your browser to view the detailed report information IDEA + Gradle + Scala version This is similar to the VSCode version, so I won\u0026rsquo;t repeat it here.\nThe differences are as follows:\nIDEA searches for Scala plugins to install New way to run: right click and select Engine.scala file in the project directory, select Run \u0026lsquo;Engine\u0026rsquo; to run the demo (you need to press enter to confirm the run). IDEA + Maven + Scala version This is similar to the VSCode version, so I won\u0026rsquo;t repeat it here.\nThe differences are as follows:\nIDEA searches for Scala plugins to install New way to run: right-click the Engine.scala file in the project directory and select Run \u0026lsquo;Engine\u0026rsquo; to run the demo (you need to press enter to confirm during the run). ","permalink":"https://naodeng.tech/posts/performance-testing/gatling-tool-tutorial1/","summary":"This article describes how to get started with the performance testing tool gatling, how to set up the environment, and how to get the official demo up and running.","title":"Gatling Performance Testing Tutorial: Getting Started"},{"content":"Since Postman announced in May 2023 that it will phase out the Scratch Pad model with offline capabilities, teams that need to isolate API workspace data from third-party servers have been looking for alternatives. Teams that need to isolate API workspace data from third-party servers have had to look for alternatives. bruno is one of those alternatives: an open source desktop application designed for API testing, development, and debugging.\nBruno is one of those alternatives: an open source desktop application designed for API testing, development, and debugging. Why bruno, how to get started, and how to migrate postman scripts are all covered in this article!\nwhy bruno Official description: https://github.com/usebruno/bruno/discussions/269\nComparison with postman: https://www.usebruno.com/compare/bruno-vs-postman\nOpen source, MIT License\nClient platform support (Mac/linux/Windows)\nOffline client, no cloud synchronization plan\nSupports Postman/insomina script import (only API request scripts can be imported, not test scripts)\nRelatively active community and clear product development roadmap.\nInstall bruno Download link: https://www.usebruno.com/downloads\nMac computer recommended brew command download\n​ brew install Bruno\nGetting Started Default main interface API collection Create API collection On the home page, click on the \u0026lsquo;Create Collection\u0026rsquo; link to open the Create API Request Collection pop-up window.\nOn the popup window, enter\nName: input the name of the API request collection\nLocation: input the path of the folder where you want to save the API request collection files (we suggest you choose the path where this project is located).\nFolder Name: you can enter the name of the API request set (a folder with the corresponding name will be created under the path you just selected).\nClick Create button to finish creating the API request set and display it on the interface (the list of newly created API request set will be displayed on the left side).\nOpen API collection Click on the \u0026lsquo;Open Collection\u0026rsquo; link on the home page to open the folder of the selected API request collection in bruno format. Click open to complete the selection and display it in the interface (the collection list on the left side will display the selected API request collection information). Import API collection Click the \u0026lsquo;Import Collection\u0026rsquo; link on the home page to open the popup window for importing API collections (Bruno/Postman/Insomnia are supported). On the popup window, select the link of the corresponding format, and then select the path of the existing file of the corresponding format. Click open to complete the selection and display it on the interface (the collection list on the left side will display the information of the selected API collection). RUN API collection Select the API request set you want to run from the collection list on the left side of the main interface. Select Run on the menu, the Runner tab will be opened on the right side of the interface, it will show some information about the requests in the selected API request collection. Click on the Run Collection button to run it locally (you will see the allowed results on the screen after running). Export API collection Select the API request set you want to run from the collection list on the left side of the main interface, and right-click to open the menu. Select Export on the menu, and then select the path of the file you want to export to complete the export (the exported file is also in json format). API request Create API request Pre-requisite: An API request collection has already been created (see Creating an API Request Collection above). Select the API request set you want to create a new API request from the collection list on the left side of the main interface. Select New Request on the menu, the right interface will open the Request tab, it will show some information of requests in the selected API request set. On the new Request window, first select the request type: HTTP/GraphQL. In the new Request window, first select the request type: HTTP/GraphQL. Name: Enter the name of the API request. URL: enter the URL of the API request Method: Select the Method of the API request. Click Create button to finish creating the API request and display it on the interface (the left request set list will display the information of the newly created API request). Edit API request Pre-requisite: you have already created an API request collection and an API request (refer to Creating an API request collection and New API request above).\nSelect the API request collection you want to edit in the collection list on the left side of the main interface, and then select the API request you want to edit.\nThen you can edit different fields of the request according to the type of API request. Body: Enter the Body of the API request.\nHeaders: Enter the headers of the API request.\nParams: Enter the Params of the API request.\nAuth: enter the Auth of the API request\nVars: enter the Vars of the API request\nScript: enter the Script of the API request\nAssert: Enter the Assert of the API request.\nTests: Enter the Tests of the API request.\nClick the Save button to finish editing the API request and display it on the interface (the list of request sets on the left side will display the information of the edited API request).\nRUN API request Pre-requisite: you have already created an API request collection and an API request (refer to Creating an API request collection and New API request above). In the collection list on the left side of the main interface, select the API request set that you want to edit the API request, and then select the API request that you want to edit. Click the right button after the API url edit box to finish running the API request and display it on the interface (the Request tab on the right side will display the information of the running API request). API request generate code Pre-requisite: you have already created an API request collection and an API request (refer to Creating an API request collection and New API request above). In the collection list on the left side of the main interface, select the API request set that you want to edit the API request, and then select the API request that you want to edit. Right click on the menu and select Generate Code, then select the language you want to generate code for. The Generate Code window will show the request code of different languages. Write API request test scripts API request Assert Introducing Assert Open any API request and switch to the Assert tab.\nThe Assert tab displays the Assert information of the API request.\nAssert is used to determine whether the result of the API request meets the expectation.\nExpr: input the expression of expected result, it can be the value of a field of the API request, two types can be input: Status Code and Response Body. Status Code: determine whether the returned status code of the API request meets the expectation (default is 200). Response Body: determine whether the returned result of the API request meets the expectation (default is true).\nOperator: the validation method for inputting the expected result. Supports multiple judgment methods: Equal and Not Equal, etc. Equal: determine whether the returned result of the API request is equal to the expected result. Not Equal: determine if the returned result of the API request is not equal to the expected result.\nValue: input the value of the expected result, supports two ways of inputting the expected result: Static and Dynamic. Static: input the static value of the expected result. Dynamic: input the dynamic value of the expected result, which can be the value of a field in the return result of the API request.\nAssert demo Assert status code is 200 Taking https://jsonplaceholder.typicode.com/posts/1 as an example (the API request returns https://jsonplaceholder.typicode.com/posts/1) I want to verify that the API request returns a status is 200. Open the API request and switch to the Assert tab. Enter the following information Expr: res.status Operator: Equal Value: 200 Assert repsponse body as expected Using https://jsonplaceholder.typicode.com/posts/1 as an example (the API request returned https://jsonplaceholder.typicode.com/posts/1) I want to verify that the API request\u0026rsquo;s repsponse body is as expected Open the API request and switch to the Assert tab. Assert1 Enter the following information in order Expr: res.body.id Operator: Equal Value: 1 Assert2 Input the following information in order Expr: res.body.title Operator: contains Value: provider Debug Assert Pre-requisite: you have already created an API request set and an API request (refer to Creating an API request set and New API request above), and you have also written the corresponding Assert according to the demo. Select the API request set you want to edit in the collection list on the left side of the main interface, and then select the API request you want to edit. Click the right button after the API url edit box to finish running the API request and display it on the interface (the Request tab on the right side will display the information of the running API request). Switch to the Tests tab to display the Tests information of the API request, which also includes the Assert information of the request. API request Tests Introduction Tests Open any API request and switch to the Tests tab. Tests tab will show the Tests information of the API request. Tests are used to write test scripts for API requests, currently javascript language is supported. You can write multiple test scripts inside Tests, each test script can be run separately. Tests demo Verify status code is 200 Taking https://jsonplaceholder.typicode.com/posts/1 as an example (the API request returns https://jsonplaceholder.typicode.com/posts/1), I want to verify that the API request returns a status is 200. Open the API request and switch to the Tests tab. Enter the following script test(\u0026#34;res.status should be 200\u0026#34;, function() { const data = res.getBody(); expect(res.getStatus()).to.equal(200); }); Verify repsponse body as expected Taking https://jsonplaceholder.typicode.com/posts/1 as an example (the API request returned https://jsonplaceholder.typicode.com/posts/1) I want to verify that the repsponse body is as expected Open the API request and switch to the Tests tab. Enter the following script test(\u0026#34;res.body should be correct\u0026#34;, function() { const data = res.getBody(); expect(data.id).to.equal(1); expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); Debugging Tests Prerequisites: You have already created an API request set and an API request (refer to Creating an API Request Set and New API Request above), and you have also written the corresponding Tests according to the demo. Select the API request set you want to edit in the collection list on the left side of the main interface, and then select the API request you want to edit. Click the right button after the API url edit box to finish running the API request and display it on the interface (the Request tab on the right side will display the information of the running API request). Switch to the Tests tab, it will show the Tests information of the API request, which will also include the requested Tests information. environment variables Creating Environment Variables Prerequisites: An API request set and an API request have already been created (see Creating an API request set and New API request above). Select the API request for which you want to create an environment variable Click the \u0026lsquo;No Environment\u0026rsquo; link in the upper right corner of the page (default is No Environment) and select the configure button in the menu to open the environment variable management popup window (supports creating new environment variables and importing existing environment variables). Click Create Environment button on the popup window, enter the name of the environment variable and click create button to create the environment variable. Then click Add Variable button on the popup window, enter the key and value of the environment variable, and click Save button to add the environment variable. environment variable demo Requirement: Create a demo environment variable that contains an environment variable with key host and value https://jsonplaceholder.typicode.com.\nSelect the API request for which you want to create the environment variable Click the \u0026lsquo;No Environment\u0026rsquo; link in the upper right corner of the page (default is No Environment), and select the configure button in the menu to open the environment variable management popup. Click the Create Environment button on the popup window, enter the name of the environment variable demo, and click the create button to create the environment variable demo. Select the demo environment variable, and then click Add Variable button on the page, enter the key of the environment variable as host and the value as https://jsonplaceholder.typicode.com, and click Save button to add the environment variable. As shown in the following figure ! env-intro Using Environment Variables Prerequisites: You have already created an API request set and an API request (see Creating an API request set and creating a new API request above), and you have also created a demo environment variable. Select the API request for which you want to use environment variables Click the \u0026lsquo;No Environment\u0026rsquo; link in the top right corner of the page (default is No Environment), and select the demo button in the menu to use the demo environment variable. Then change the URL of the API request to {{host}}/posts/1 to use the environment variable. Test script automation Pre-conditions API request set has been created (example named :api-collects) API request has been created (example name: api request1) an environment variable has been created (example name: demo) has also written an assert or tests script for the API request api automation project demo Installed node.js Install npm create a new project folder (example name: bruno-test) Execute npm init in the project folder to initialize the project as an npm project Install @usebruno/cli dependency (script: npm install @usebruno/cli) Open the folder directory where the API request sets are stored, and copy all the files in the api-collects directory to the bruno-test project directory The project directory looks like this bruno-test //项目主文件夹 api request1.bru //api 请求 enviroments //环境变量 demo.bru bruno.json node_modules //node 包依赖 package-lock.json package.json //npm 项目配置文件 Run the following command in the project directory to run the API request bruno run --env demo The result is as follows Getting into CI Getting into github action Take github action as an example, other CI tools are similar.\nPrepare: Add the following script to the project package.json file \u0026#34;test\u0026#34;: \u0026#34;bru run --env demo\u0026#34; Create .github/workflows folder in the project root folder Create main.yml file under .github/workflows folder The contents of the main.yml file are as follows name: bruno cli CI on: push: branches: [ main ] pull_request: branches: [ main ] jobs: run_bruno_api_test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - run: npm install - name: run tests run: npm run test submit code to github, will automatically trigger github action View the result of the github action, as shown in the example: The code for this project can be pulled for reference:https://github.com/dengnao-tw/Bruno-API-Test-Starter\nTest report\u0026mdash;TODO bruno More usage\u0026mdash;TODO Postman script migration API Request Collection Migration Click on the \u0026lsquo;Import Collection\u0026rsquo; link on the home page to open the Import API collection popup window. Click on the Select Postman Collection link and select the path to an existing Postman request collection file. Then you can import Postman request collection. However, only API requests can be imported, not test scripts, as shown in the figure (but it doesn\u0026rsquo;t affect the request invocation). Environment Variable Migration Select the Postman request you just imported on the home page. Click the \u0026lsquo;No Environment\u0026rsquo; link in the upper right corner of the page (default is No Environment), and select the configure button in the menu to open the environment variable management popup window. Click on the \u0026lsquo;Import Environment\u0026rsquo; link to open the Import Environment popup. Click on the \u0026lsquo;Postman Environment\u0026rsquo; link to open the Import Environment popup window Click on the \u0026lsquo;Postman Environment\u0026rsquo; link and select the path to an existing Postman environment file You can import Postman environment variables. Test Script Migration Reference The syntax of the test scripts for the two tools is partially different and needs to be modified manually\nPostman test script syntax reference: https://learning.postman.com/docs/writing-scripts/test-scripts/ Postman test script example pm.test(\u0026#34;res.status should be 200\u0026#34;, function () { pm.response.to.have.status(200); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(1); pm.expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); Bruno test script syntax reference: https://docs.usebruno.com/testing/introduction.html Bruno test script example test(\u0026#34;res.status should be 200\u0026#34;, function() { const data = res.getBody(); expect(res.getStatus()).to.equal(200); }); test(\u0026#34;res.body should be correct\u0026#34;, function() { const data = res.getBody(); expect(data.id).to.equal(1); expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); ","permalink":"https://naodeng.tech/posts/api-automation-testing/introduction_of_bruno/","summary":"Article introduces postman replacement tool bruno beginner\u0026rsquo;s introduction, how to migrate postman scripts to bruno","title":"Introducing Bruno for Replacement Postman"}]