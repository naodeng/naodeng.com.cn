[{"content":"Introduction Why Not Use Postman and Insomnia? Regarding Postman: In May 2023, Postman announced the gradual phasing out of the Scratch Pad model with offline capabilities. Most functions will be shifted to the cloud, requiring users to log in for access. (Limited functionality is available without logging in, but the extent of data upload to the cloud during testing, compromising security, remains uncertain.) About Insomnia: With the release of version 8.0 on September 28, 2023, Insomnia intensified its reliance on the cloud. Users must log in to utilize the full functionality of Insomnia. The existing Scratch Pad features are restricted without login. (The security implications of potential data transmission to the cloud during testing without confirmation remain unclear.) Therefore, an alternative solution that isolates API workspace data from third-party servers is necessary, with Bruno emerging as one feasible substitute.\nWhy Choose Bruno Official Documentation: https://github.com/usebruno/bruno/discussions/269\nComparison with Postman: https://www.usebruno.com/compare/bruno-vs-postman\nOpen source, MIT License\nCross-platform support (Mac/Linux/Windows)\nOffline client with no plans for cloud synchronization\nSupports Postman/Insomnia script import (limited to API request scripts, excluding test scripts)\nRelatively active community, with a clear product development roadmap\nBuilding a Bruno API Automation Test project from 0 to 1 This article focuses on leveraging Bruno\u0026rsquo;s features to construct an API automation test project from scratch.\nFor Bruno installation and basic usage, please refer to: Introduction to using Bruno as a postman replacement\nProject Structure The structure of a Bruno API automation test project is as follows:\nBruno-demo ├── README.md // Project documentation file ├── package.json ├── package-lock.json ├── Testcase // Test case folder │ └── APITestDemo1.bru // Test case file 1 │ └── APITestDemo2.bru // Test case file 2 │ └── bruno.json // Bruno COLLECTION configuration file │ └── environments // Different test environment folder │ └── dev.bru // Test environment configuration file │ └── Report // Test report files │ └── report.json // JSON format report file ├── .gitignore └── node_modules // Project dependencies Project Setup Preparation Create Project Folder mkdir Bruno-demo Project Initialization // Navigate to the project folder cd Bruno-demo // Initialize the Node.js project npm init -y Install Bruno CLI Dependencies // Install Bruno CLI npm install @usebruno/cli --save-dev Bruno CLI is the official command-line tool provided by Bruno. It allows easy execution of API collections through simple command-line commands. This tool facilitates testing APIs in different environments, automating testing workflows, and integrating API testing with continuous integration and deployment workflows.\nWriting API Test Cases with Bruno Create Test Case Directory Run Bruno app to the homepage Create a COLLECTION named Testcase, and choose the project folder created above as the directory for the COLLECTION. Create a GET Request Test Case Click the ADD REQUEST button under the Testcase COLLECTION to create a new GET request. Enter the request name as GetDemo and the request URL as https://jsonplaceholder.typicode.com/posts/1. Adding Test Assertions to the GET Request Using Bruno\u0026rsquo;s Built-in Assert for Test Assertions Click the Assert button under the GetDemo request to enter the test assertion editing page.\nEnter Assertion 1: Response status code equals 200. Assertion 2: The title in the response body contains \u0026ldquo;provident.\u0026rdquo; Debugging Assertions: Click the Run button in the upper right corner to execute the assertions and check if the results meet expectations. Writing Test Assertions Using JavaScript Click the Tests button under the GetDemo request to enter the test script editing page. Enter the script code, Assertion 1: Response status code equals 200. Assertion 2: The title in the response body contains \u0026ldquo;provident.\u0026rdquo; test(\u0026#34;res.status should be 200\u0026#34;, function() { const data = res.getBody(); expect(res.getStatus()).to.equal(200); }); test(\u0026#34;res.body should be correct\u0026#34;, function() { const data = res.getBody(); expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); Debugging Assertions: Click the Run button in the upper right corner to execute the assertions and check if the results meet expectations. Creating a New POST Request Test Case Click the ADD REQUEST button under the Testcase COLLECTION to create a new POST request.\nEnter the request name as PostDemo, and the request URL as https://jsonplaceholder.typicode.com/posts. Click the Body button under the newly created PostDemo request to enter the request body editing page.\nSelect the body type as JSON and enter the request body content:\n{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } Adding Test Assertions to the Post Request Using Bruno\u0026rsquo;s Built-in Assert for Post Request Test Assertions Click the Assert button under the PostDemo request to enter the test assertion editing page.\nEnter Assertion 1: Response status code equals 201. Assertion 2: The title in the response body equals \u0026ldquo;foo.\u0026rdquo; Debugging Assertions: Click the Run button in the upper right corner to execute the assertions and check if the results meet expectations. Writing Test Assertions Using JavaScript for the Post Request Click the Tests button under the PostDemo request to enter the test script editing page. Enter the script code, Assertion 1: Response status code equals 201. Assertion 2: The title in the response body equals \u0026ldquo;foo.\u0026rdquo; test(\u0026#34;res.status should be 200\u0026#34;, function() { const data = res.getBody(); expect(res.getStatus()).to.equal(201); }); test(\u0026#34;res.body should be correct\u0026#34;, function() { const data = res.getBody(); expect(data.title).to.equal(\u0026#39;foo\u0026#39;); }); Debugging Assertions: Click the Run button in the upper right corner to execute the assertions and check if the results meet expectations. Running Two Test Cases Locally Click the Run button under the Testcase COLLECTION to run all test cases. Confirm if the results meet expectations. This concludes the writing and assertion of test cases for two interfaces.\nEnvironment Variable Configuration By reviewing the results of the two test cases, we found that the request addresses for both test cases are https://jsonplaceholder.typicode.com. If we need to run these two test cases in different testing environments, we need to modify the request addresses for both test cases. This could be tedious if there are many test cases. Bruno provides the functionality of environment variables, allowing us to configure request addresses in test cases as environment variables. This way, we only need to configure different environment variables in different testing environments to run test cases.\nCreating Environment Variable Configuration Files Click the Environments button under the Testcase COLLECTION to enter the environment variable configuration page. Click the ADD ENVIRONMENT button in the upper right corner to create a new environment variable configuration file. Enter the name as dev and click the SAVE button to save the configuration file. Click the newly created dev environment variable configuration file to enter the environment variable configuration page. Click the ADD VARIABLE button in the upper right corner to create a new environment variable. Enter the name as host and the value as https://jsonplaceholder.typicode.com. Click the SAVE button to save the environment variable. Using Environment Variables in Test Cases Click the GetDemo request under the Testcase COLLECTION to enter the GetDemo request editing page. Modify the request address of the GetDemo request to {{host}}/posts/1 and click the SAVE button to save the GetDemo request. Click the PostDemo request under the Testcase COLLECTION to enter the PostDemo request editing page. Modify the request address of the PostDemo request to {{host}}/posts and click the SAVE button to save the PostDemo request. Debugging Environment Variables Click the Environments button under the Testcase COLLECTION, select the dev environment variable. Click the RUN button in the upper right corner to run all test cases. Confirm if the results meet expectations. This concludes the configuration and debugging of environment variables.\nRunning Test Cases from the Command Line Pre-check We have set the storage directory for the test cases to the project folder created earlier. We need to check if the test case files and environment variable configuration files have been successfully created in the project folder.\nCurrently, our project folder directory structure is as follows:\nBruno-demo ├── package.json ├── package-lock.json ├── Testcase // Test case folder │ └── APITestDemo1.bru // Test case file 1 │ └── APITestDemo2.bru // Test case file 2 │ └── bruno.json // Bruno COLLECTION configuration file │ └── environments // Different test environment folder │ └── dev.bru // Test environment configuration file └── node_modules // Project dependencies Debugging and Running Test Cases from the Command Line In the Testcase folder under the project file, run the command bru run --env dev to run all test cases. Confirm if the results meet expectations. Generating JSON Format Reports In the Testcase folder under the project file, create a Report folder to store the test report files. In the Testcase folder, run the command bru run --env dev --output Report/results.json to run all test cases. Confirm that the test report file is generated successfully. At this point, the construction of the Bruno API automation testing project is complete.\nIntegration into CI/CD Processes For Bruno installation and basic usage, please refer to: Introduction to using Bruno as a postman replacement#CI/CD Integration\nReferences Bruno Official Documentation https://docs.usebruno.com/ Introduction to using Bruno as a postman replacement https://naodeng.com.cn/en/posts/api-automation-testing/introduction_of_bruno/ ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/bruno-tutorial-building-your-own-project-from-0-to-1/","summary":"This blog post serves as a tutorial on Bruno API automation testing, guiding readers on constructing a Bruno API automation test project from scratch. The article provides detailed instructions on establishing the foundational structure of a test project, configuring the environment, and writing the first API test case. Through this tutorial, readers will progressively grasp the usage of the Bruno framework, building a comprehensive API automation test project from inception to completion. This process aims to enhance testing efficiency and maintainability.","title":"Bruno API Automation Testing Tutorial: Building a Bruno API Automation Test project from 0 to 1"},{"content":"How to Sponsor 如何赞助 buy me a coffee\n微信\n支付宝\nSupport me by becoming a sponsor\nIf you like my Website content, you can buy me a coffee.\n支持我来成为一个赞助者\n如果您喜欢我的网站内容，可以请我喝咖啡。\nSponsor List 赞助者列表 The stage is set, thanks for the support!\n虚位以待，感谢支持！\nWeekly manual update.每周手动更新。\n","permalink":"https://naodeng.com.cn/sponsor/","summary":"How to Sponsor 如何赞助 buy me a coffee\n微信\n支付宝\nSupport me by becoming a sponsor\nIf you like my Website content, you can buy me a coffee.\n支持我来成为一个赞助者\n如果您喜欢我的网站内容，可以请我喝咖啡。\nSponsor List 赞助者列表 The stage is set, thanks for the support!\n虚位以待，感谢支持！\nWeekly manual update.每周手动更新。","title":"Sponsor 赞助者"},{"content":"K6 common function HTTP Requests The first step in performance testing with K6 is to define the HTTP requests to be tested.\nGET Request Example A simple HTTP request for the GET method is already included in the demo test script created with the k6 new command:\nimport http from \u0026#39;k6/http\u0026#39;; import { sleep } from \u0026#39;k6\u0026#39;; export default function() { http.get(\u0026#39;https://test.k6.io\u0026#39;); sleep(1); } POST Request Example This POST request example shows the application of some complex scenarios (POST request with email/password authentication load)\nimport http from \u0026#39;k6/http\u0026#39;; export default function () { const url = \u0026#39;http://test.k6.io/login\u0026#39;; const payload = JSON.stringify({ email: \u0026#39;aaa\u0026#39;, password: \u0026#39;bbb\u0026#39;, }); const params = { headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, }; http.post(url, payload, params); } The above is taken from K6 Official Documentation\nSupported HTTP Methods The HTTP module provided by K6 can handle various HTTP requests and methods. The following is a list of supported HTTP methods:\nNAME VALUE batch() makes multiple HTTP requests in parallel (like e.g. browsers tend to do). del() makes an HTTP DELETE request. get() makes an HTTP GET request. head() makes an HTTP HEAD request. options() makes an HTTP OPTIONS request. patch() makes an HTTP PATCH request. post() makes an HTTP POST request. put() makes an HTTP PUT request. request() makes any type of HTTP request. HTTP Request Tags K6 allows you to add tags to each HTTP request. Combining tags and grouping makes it easy to better organize in test results, group requests and filter results to organize analysis.\nThe following is a list of supported tags:\nNAME DESCRIPTION expected_response By default, response statuses between 200 and 399 are true. Change the default behavior with setResponseCallback. group When the request runs inside a group, the tag value is the group name. Default is empty. name Defaults to URL requested method Request method (GET, POST, PUT etc.) scenario When the request runs inside a scenario, the tag value is the scenario name. Default is default. status response status url defaults to URL requested Examples of HTTP requests using tag and group tags will be shown in subsequent demos.\nYou can also refer to the official examples:https://grafana.com/docs/k6/latest/using-k6/http-requests/\nMetrics The metrics are used to measure the performance of the system under test conditions. By default, k6 automatically collects built-in metrics. In addition to the built-in metrics, you can create custom metrics.\nMetrics generally fall into four categories:\nCounters: Summing values. Gauges: Tracking the smallest, largest, and most recent values. Rates: Tracking how often non-zero values occur. Trends: Calculating statistical information (such as mean, mode, or percentile) for multiple values. To ensure that test assertions meet the criteria, thresholds can be written based on the conditions of the metrics required by the performance test (the specifics of the expression depend on the type of metric).\nFor subsequent filtering of metrics, labels and groupings can be used, allowing for better organization of test results.\nThe test results output file can export metrics in a variety of summary and fine-grained formats. For more information, refer to the results output documentation. (This section will be covered in more detail in the later part of the test results output documentation.)\nK6 Built-in Metrics Every k6 test execution emits both built-in and custom metrics. Each supported protocol also has its specific metrics.\nStandard Built-in Metrics Regardless of the protocol used in the test, k6 always collects the following metrics:\nMetric Name Type Description vus Gauge Current number of active virtual users vus_max Gauge Max possible number of virtual users (VU resources are pre-allocated, to avoid affecting performance when scaling up load ) iterations Counter The aggregate number of times the VUs execute the JS script (the default function). iteration_duration Trend The time to complete one full iteration, including time spent in setup and teardown. To calculate the duration of the iteration’s function for the specific scenario, try this workaround dropped_iterations Counter The number of iterations that weren’t started due to lack of VUs (for the arrival-rate executors) or lack of time (expired maxDuration in the iteration-based executors). About dropped iterations data_received Counter The amount of received data. This example covers how to track data for an individual URL. data_sent Counter The amount of data sent. Track data for an individual URL to track data for an individual URL. checks Rate The rate of successful checks. HTTP-specific built-in metrics HTTP-specific built-in metrics are generated only when the test makes HTTP requests.Other types of requests, such as WebSocket, do not generate these metrics.\nNote: For all http_req_* metrics, the timestamp is emitted at the end of the request. In other words, the timestamp occurs when k6 receives the end of the response body or when the request times out.\nThe following table lists HTTP-specific built-in metrics:\nMetric Name Type Description http_reqs Counter How many total HTTP requests k6 generated. http_req_blocked Trend Time spent blocked (waiting for a free TCP connection slot) before initiating the request. float http_req_connecting Trend Time spent establishing TCP connection to the remote host. float http_req_tls_handshaking Trend Time spent handshaking TLS session with remote host http_req_sending Trend Time spent sending data to the remote host. float http_req_waiting Trend Time spent waiting for response from remote host (a.k.a. “time to first byte”, or “TTFB”). float http_req_receiving Trend Time spent receiving response data from the remote host. float http_req_duration Trend Total time for the request. It’s equal to http_req_sending + http_req_waiting + http_req_receiving (i.e. how long did the remote server take to process the request and respond, without the initial DNS lookup/connection times). float http_req_failed Rate The rate of failed requests according to setResponseCallback. Other built-in metrics In addition to the standard built-in metrics and HTTP-specific built-in metrics, K6 built-in metrics also have other built-in metrics:\nBrowser metrics : https://grafana.com/docs/k6/latest/using-k6/metrics/reference/#browser Built-in WebSocket metrics : https://grafana.com/docs/k6/latest/using-k6/metrics/reference/#websockets Built-in gRPC metrics : https://grafana.com/docs/k6/latest/using-k6/metrics/reference/#grpc custom metrics Besides the built-in metrics, you can create custom metrics. For example, you can compute a metric for your business logic, or use the Response.timings object to create a metric for a specific set of endpoints.\nEach metric type has a constructor to create a custom metric. The constructor creates a metric object of the declared type. Each type has an add method to take metric measurements.\nNote: Custom metrics must be created in the init context. This limits memory and ensures that K6 can verify that all thresholds evaluate the defined metrics.\ncustom metrics demo The following example demonstrates how to create a custom trend metrics for wait time:\nThe demo_custom_metrics.js file in the project file already contains this demo example, which can be run directly to view the results.\n1.Import the Trend constructor from the k6/metrics module import { Trend } from \u0026#39;k6/metrics\u0026#39;; The waiting time trend metrics is a Trends metrics, so the Trend constructor needs to be introduced from the k6/metrics module.\n2.Constructs a new custom metric Trend object in the init context const myTrend = new Trend(\u0026#39;waiting_time\u0026#39;); Construct a new custom metric Trend object in the init context, the object in the script is myTrend, and its metric is displayed as waiting_time in the resulting output.\n3.Use the add method in a script to record metric measurements export default function() { const res = http.get(\u0026#39;https://test.k6.io\u0026#39;); myTrend.add(res.timings.waiting); } Use the add method in the script to record the metric measurement values. Here, res.timings.waiting is used, which is the waiting time.\n4.demo_custom_metrics.js Complete code of custom metric import http from \u0026#39;k6/http\u0026#39;; import { Trend } from \u0026#39;k6/metrics\u0026#39;; const myTrend = new Trend(\u0026#39;waiting_time\u0026#39;); export default function () { const res = http.get(\u0026#39;https://httpbin.test.k6.io\u0026#39;); myTrend.add(res.timings.waiting); console.log(myTrend.name); // waiting_time } 5.Run demo_custom_metrics.js and view automated trending metrics k6 run demo_custom_metrics.js The running results are as follows:\nAs you can see, the custom metric waiting_time has been displayed in the result output.\nFor more information about custom metrics, please refer to the official documentation: https://k6.io/docs/using-k6/metrics/#custom-metrics\nChecks This can also be understood as assertions, which verify the test results.\nChecks are used to verify whether specific test conditions in different tests are correctly responded to, similar to how we conventionally verify test results in other types of tests to ensure that the system responds as expected.\nFor example, a check can ensure that the response status of a POST request is 201, or that the size of the response body matches expectations.\nChecks are similar to the concept of assertions in many testing frameworks, but K6 does not abort the test or end it in a failed state when verifications fail. Instead, k6 tracks the failure rate of failed verifications as the test continues to run.\nEach check creates a rate metric. To make a check abort or cause the test to fail, it can be combined with thresholds.\nBelow, we will introduce how to use different types of checks and how to view check results in test results.\n1. Check HTTP Response Status K6 checks are particularly useful for response assertions related to HTTP requests.\nFor example, the following code snippet checks that the HTTP response code is 200:\nimport { check } from \u0026#39;k6\u0026#39;; import http from \u0026#39;k6/http\u0026#39;; export default function () { const res = http.get(\u0026#39;https://httpbin.test.k6.io\u0026#39;); check(res, { \u0026#39;HTTP response code is status 200\u0026#39;: (r) =\u0026gt; r.status === 200, }); } Running this script, you can see the following results:\nWhen a script contains checks, the summary report shows how many test checks have passed.\nIn this example, note that the check \u0026ldquo;HTTP response code is status 200\u0026rdquo; is 100% successful when called.\n2. Check HTTP Response Body In addition to checking the HTTP response status, you can also check the HTTP response body.\nFor example, the following code snippet checks that the HTTP response body size is 9591 bytes:\nimport { check } from \u0026#39;k6\u0026#39;; import http from \u0026#39;k6/http\u0026#39;; export default function () { const res = http.get(\u0026#39;https://httpbin.test.k6.io\u0026#39;); check(res, { \u0026#39;HTTP response body size is 9591 bytes\u0026#39;: (r) =\u0026gt; r.body.length == 9591, }); } Running this script, you can see the following results:\nWhen a script contains checks, the summary report shows how many test checks have passed.\nIn this example, note that the check \u0026ldquo;HTTP response body size is 9591 bytes\u0026rdquo; is 100% successful when called.\n3. Adding Multiple Checks Sometimes, multiple checks need to be added in a single test script. You can directly add multiple checks in a single check() statement, as shown in the script below:\nimport { check } from \u0026#39;k6\u0026#39;; import http from \u0026#39;k6/http\u0026#39;; export default function () { const res = http.get(\u0026#39;https://httpbin.test.k6.io\u0026#39;); check(res, { \u0026#39;HTTP response code is status 200\u0026#39;: (r) =\u0026gt; r.status === 200, \u0026#39;HTTP response body size is 9591 bytes\u0026#39;: (r) =\u0026gt; r.body.length == 9591, }); } Running this script, you can see the following results:\nIn this example, both checks pass successfully (the call is 100% successful).\nNote: When a check fails, the script will continue to execute successfully and will not return a \u0026ldquo;failed\u0026rdquo; exit status. If you need to fail the entire test based on check results, you must combine checks with thresholds. This is particularly useful in specific environments, such as integrating k6 into a CI pipeline or receiving alerts when scheduling performance tests.\nReferences Official K6 documentation: https://k6.io/docs/ Official website: https://k6.io/ K6 Performance Test quick starter: https://github.com/Automation-Test-Starter/K6-Performance-Test-starter/ ","permalink":"https://naodeng.com.cn/posts/performance-testing/k6-tutorial-common-functions-1-http-request-metrics-and-checks/","summary":"The article provides a detailed exploration of the HTTP request functionality in K6, dissecting common performance metrics and check features. Learn how to leverage K6 for robust performance testing, simulating user behavior through HTTP requests, and assessing system response by understanding performance metrics. The tutorial delves into configuring and executing checks, ensuring performance aligns with expected standards. Whether you\u0026rsquo;re a beginner or an experienced performance testing professional, this guide equips you with practical knowledge to harness the full potential of K6 in performance testing. Click the link to embark on an efficient journey into performance testing with K6!","title":"K6 Performance Testing Tutorial: Common Functions (1) - HTTP Request, Metrics and Checks"},{"content":"Introduction of K6 k6 is an open source tool for performance testing and load testing, primarily used to evaluate and validate the performance and stability of applications. Here are some key features and information about k6:\nOpen Source: k6 is a completely open source performance testing tool with code stored on GitHub. This means that users are free to access, use and modify the tool\u0026rsquo;s source code.\nJavaScript scripting: k6 uses the JavaScript language to write test scripts, which makes writing test cases relatively easy and more developer-friendly. Scripts can contain HTTP requests, WebSocket connections, script execution logic, and more.\nSupport for multiple protocols: k6 supports a variety of common protocols, including HTTP, WebSocket, Socket.IO, gRPC and so on, so it can be widely used in various types of applications. 4.\nDistributed Testing: k6 has distributed testing capabilities, allowing tests to be run on multiple nodes to simulate a more realistic production environment load.\nReal-time results and reports: k6 provides real-time results, including request response time, throughput, etc., and is able to generate detailed HTML reports to help users better understand the performance status of their applications.\nContainerization Support: k6 adapts to containerized environments, can be easily integrated into CI/CD pipelines, and works with common container orchestration tools such as Kubernetes.\nPlugin ecosystem: k6 supports plugins that allow users to extend its functionality to meet specific needs.\nActive Community: Since k6 is an open source project, there is an active community that provides support, documentation, and examples to make it easier for users to get started and solve problems.\nOverall, k6 is a flexible, powerful and easy-to-use performance testing tool for applications and systems of all sizes.\nOfficial website and documentation Official website Official Documentation Installation Installation on Mac systems Mac systems can install k6 via Homebrew:\nbrew install k6 Windows installation Windows systems can install k6 via Chocolatey:\nchoco install k6 Or you can install k6 via winget:\nwinget install k6 Docker installation k6 can also be installed via Docker:\ndocker pull grafana/k6 Installation on other systems In addition to the above systems, K6 also supports Linux (Debian/Ubuntu/Fedora/CentOS), and can be installed by downloading the K6 binaries and K6 extensions, please refer to the [official documentation](https://k6.io/docs/get-started/ For details on how to install K6, please refer to the official documentation ().\nConfirming a successful K6 installation After the installation is complete, you can confirm that K6 has been installed successfully by using the following command:\nk6 version If the installation was successful, the k6 version information will be displayed:\nFirst k6 test script Write the first test script Create a new K6 performance testing project directory and go to mkdir k6-demo cd k6-demo Create a file named demo.js for writing test scripts A test script file can be created with the k6 new command: k6 new demo.js You can also create a test script file called demo.js directly touch demo.js Editing Test Scripts If the test script file is created with the k6 new command, a simple test script is automatically generated as shown below:\nimport http from \u0026#39;k6/http\u0026#39;; import { sleep } from \u0026#39;k6\u0026#39;; export const options = { // A number specifying the number of VUs to run concurrently. vus: 10, // A string specifying the total duration of the test run. duration: \u0026#39;30s\u0026#39;, // The following section contains configuration options for execution of this // test script in Grafana Cloud. // // See https://grafana.com/docs/grafana-cloud/k6/get-started/run-cloud-tests-from-the-cli/ // to learn about authoring and running k6 test scripts in Grafana k6 Cloud. // // ext: { // loadimpact: { // // The ID of the project to which the test is assigned in the k6 Cloud UI. // // By default tests are executed in default project. // projectID: \u0026#34;\u0026#34;, // // The name of the test in the k6 Cloud UI. // // Test runs with the same name will be grouped. // name: \u0026#34;demo.js\u0026#34; // } // }, // Uncomment this section to enable the use of Browser API in your tests. // // See https://grafana.com/docs/k6/latest/using-k6-browser/running-browser-tests/ to learn more // about using Browser API in your test scripts. // // scenarios: { // // The scenario name appears in the result summary, tags, and so on. // // You can give the scenario any name, as long as each name in the script is unique. // ui: { // // Executor is a mandatory parameter for browser-based tests. // // Shared iterations in this case tells k6 to reuse VUs to execute iterations. // // // // See https://grafana.com/docs/k6/latest/using-k6/scenarios/executors/ for other executor types. // executor: \u0026#39;shared-iterations\u0026#39;, // options: { // browser: { // // This is a mandatory parameter that instructs k6 to launch and // // connect to a chromium-based browser, and use it to run UI-based // // tests. // type: \u0026#39;chromium\u0026#39;, // }, // }, // }, // } }; // The function that defines VU logic. // // See https://grafana.com/docs/k6/latest/examples/get-started-with-k6/ to learn more // about authoring k6 scripts. // export default function() { http.get(\u0026#39;https://test.k6.io\u0026#39;); sleep(1); } If the test script file was created directly, you can copy the above into the demo.js file.\nRunning the Test Script In the directory where the demo.js file is located, run the following command:\nk6 run demo.js Check the test results If all is well, you will see output similar to the following:\nContains the following information:\nexecution: execution information, including start time, end time, duration, number of VUs, number of iterations, etc. scenarios: Scenario information, including scenario name, number of VUs, number of iterations, duration, average response time, throughput, and so on. http_reqs: HTTP request information, including request name, number of requests, number of failures, average response time, throughput, and so on. Parsing demo test script import http from 'k6/http';: import k6\u0026rsquo;s HTTP module, used to send HTTP request.\nimport { sleep } from 'k6';: Import k6\u0026rsquo;s sleep method to wait for script execution.\nexport const options = { ... }: Define the configuration items of the test script, including the number of VUs, duration, etc.\nvus: 10,: define the number of VUs to be 10 (specify the number of VUs running concurrently).\nduration: '30s',: define the duration as 30 seconds (specify the total duration of the test run).\nexport default function() { ... }: defines the logic of the test script, including sending HTTP requests, executing waits, and so on.\nhttp.get('https://test.k6.io');: send a GET request to https://test.k6.io.\nsleep(1);: wait 1 second for execution.\nThe other comments can be ignored, they are about some advanced features of k6, which will be introduced later.\nReferences Official K6 documentation: https://k6.io/docs/ Official website: https://k6.io/ K6 Performance Test quick starter: https://github.com/Automation-Test-Starter/K6-Performance-Test-starter/ ","permalink":"https://naodeng.com.cn/posts/performance-testing/k6-tutorial-getting-started-and-your-first-k6-test-script/","summary":"This article will take you into the world of K6 performance testing. The blog post covers the introductory knowledge of K6 performance testing, environment setup steps, and how to write your first test script. Whether you are a beginner or an experienced performance testing professional, this tutorial will provide you with clear guidance to help you quickly get started with K6 and start building efficient performance testing scripts","title":"K6 Performance Testing Tutorial: Getting Started and Write your first k6 test script"},{"content":"Dear readers,\nRecently, while checking the indexing status of my personal blog articles on search engines, I regret to inform you about a disheartening discovery. I found that my blog articles were blatantly plagiarized by a CSDN blogger who not only copied them verbatim but also failed to provide proper attribution.\nI am angered and disappointed by this unethical behavior. I have consistently strived to deliver original and valuable content to all of you, and such plagiarism is a severe disrespect to my hard work and dedication. To protect my rights, I find it necessary to issue this declaration to ensure everyone is aware of the facts.\nFirstly, I want to make it clear that I vehemently oppose all forms of plagiarism and infringement. My blog is my personal creative space, intended to be a platform for sharing and communication rather than a target for unauthorized appropriation.\nUpon confirming the actions of the CSDN blogger, I feel deep regret and have decided to take all necessary legal measures to safeguard my legitimate rights. Simultaneously, I call upon all bloggers and creators to collaborate in maintaining a positive creative environment and eradicating instances of plagiarism.\nLastly, I want to express my gratitude to all the readers who have supported me throughout. Your support fuels my creativity and empowers me to overcome challenges. I will continue to deliver authentic and valuable content for all of you.\nPlagiarized article link:https://blog.csdn.net/2301_76387166?type=blog\nI have contacted CSDN to take it down.\nThank you once again for your attention and support.\nSincerely.\n","permalink":"https://naodeng.com.cn/posts/others/article-plagiarism-statement/","summary":"This blog post is a statement on the plagiarism of my articles.","title":"Declaration Regarding Plagiarism of My Articles"},{"content":"Implementation of API Automation Projects with Java and REST Assured Framework REST Assured Framework Tutorial Table of Contents The table of contents is not clickable, only for displaying the structure.\nQuick Start Project for RestAssured API Test Introduction to RestAssured Project Structure Versions for Gradle Build Versions for Maven Build Project Dependencies Building REST Assured API Test Project from 0 to 1 Gradle Version Maven Version Advanced Usage Verify Response Data File Upload Logging Filters Continuous Integration Integrate with GitHub Action Integrate Allure Test Report Data-Driven Multi-Environment Support Corresponding Articles for REST Assured Framework Tutorial REST Assured API Test Tutorial: Advanced Usage - Integration with CI/CD and Allure Report:https://naodeng.tech/en/posts/api-automation-testing/rest-assured-tutorial-advance-usage-integration-ci-cd-and-allure-report/ REST Assured API Test Tutorial: Advanced Usage - Verify Response and Logging, Filters, File Upload:https://naodeng.tech/en/posts/api-automation-testing/rest-assured-tutorial-advance-usage-verifying-response-and-logging/ REST Assured API Test Tutorial: Building Your Own Project from 0 to 1:https://naodeng.tech/en/posts/api-automation-testing/rest-assured-tutorial-building-your-own-project-from-0-to-1/ REST Assured API Test Tutorial: Introduction and Environment Setup Preparation:https://naodeng.tech/en/posts/api-automation-testing/rest-assured-tutorial-and-environment-preparation/ Reference Documents for REST Assured Framework Tutorial Demo Project Repository: RestAssured-API-Test-Starterhttps://github.com/Automation-Test-Starter/RestAssured-API-Test-Starter/ Rest Assured Official Documentation: https://rest-assured.io/ Rest Assured Official GitHub: https://github.com/rest-assured/rest-assured Rest Assured Official Chinese Translation: https://github.com/RookieTester/rest-assured-doc Allure Documentation: https://docs.qameta.io/allure/ GitHub Action Documentation: https://docs.github.com/en/actions Implementation of API Automation Projects with JavaScript and SuperTest Framework SuperTest Framework Tutorial Table of Contents The table of contents is not clickable, only for displaying the structure.\nQuick Start Project for SuperTest API Test Introduction Project Dependencies Project File Structure Building SuperTest API Test Project from 0 to 1 Mocha Version Jest Version Advanced Usage Continuous Integration Integrate with GitHub Action Common Assertions Built-in Assertions in SuperTest Common Assertions in CHAI Common Assertions in Jest Data-Driven Multi-Environment Support Corresponding Articles for SuperTest Framework Tutorial SuperTest API Test Tutorial: Advanced Usage - Multi-Environment Support:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-advance-usage-multiple-environment-support/ SuperTest API Test Tutorial: Advanced Usage - Data-Driven:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-advance-usage-data-driven/ SuperTest API Test Tutorial: Advanced Usage - Common Assertions:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-advance-usage-common-assertions/ SuperTest API Test Tutorial: Advanced Usage - Integration with CI/CD and GitHub Action:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-advance-usage-integration-ci-cd-and-github-action/ SuperTest API Test Tutorial: Building Your Own Project from 0 to 1:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-building-your-own-project-from-0-to-1/ SuperTest API Test Tutorial: Getting Started and Own Environment Preparation:https://naodeng.tech/en/posts/api-automation-testing/supertest-tutorial-getting-started-and-own-environment-preparation/ Reference Documents for SuperTest Framework Tutorial Demo Project Repository: SuperTest-API-Test-Starterhttps://github.com/Automation-Test-Starter/SuperTest-API-Test-Starter SuperTest Documentation: https://github.com/ladjs/supertest Jest Documentation: https://jestjs.io/docs/en/getting-started Mocha Documentation: https://mochajs.org/ Chai Documentation: https://www.chaijs.com/ Allure Documentation: https://docs.qameta.io/allure/ GitHub Action Documentation: https://docs.github.com/en/actions Implementation of API Automation Projects with Python and Pytest Framework Pytest Framework Tutorial Table of Contents The table of contents is not clickable, only for displaying the structure.\nQuick Start Project for Pytest API Test Introduction Introduction to Pytest Introduction to Python Virtual Environment Project Dependencies Project Directory Structure Building Pytest API Test Project from 0 to 1 Advanced Usage Continuous Integration Integrate with GitHub Action Common Assertions Data-Driven Multi-Environment Support and Integration with Allure Report Concurrent Testing and Distributed Testing Filtering Test Case Execution Corresponding Articles for Pytest Framework Tutorial Pytest API Test Tutorial: Advanced Usage - Filtering Test Case Execution, Concurrent Testing, and Distributed Testing:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-advance-usage-filter-testcase-and-concurrent-testing-distributed-testing/ Pytest API Test Tutorial: Advanced Usage - Multi-Environment Support and Integration with Allure Report:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-advance-usage-multiple-environment-support-and-integration-allure-report/ Pytest API Test Tutorial: Advanced Usage - Common Assertions and Data-Driven:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-advance-usage-common-assertions-and-data-driven/ Pytest API Test Tutorial: Advanced Usage - Integration with CI/CD and GitHub Action:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-advance-usage-integration-ci-cd-and-github-action/ Pytest API Test Tutorial: Building Your Own Project from 0 to 1:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-building-your-own-project-from-0-to-1/ Pytest API Test Tutorial: Getting Started and Own Environment Preparation:https://naodeng.tech/en/posts/api-automation-testing/pytest-tutorial-getting-started-and-own-environment-preparation/ Reference Documents for Pytest Framework Tutorial Demo Project Repository: Pytest-API-Test-Starter Pytest Documentation: https://docs.pytest.org/en/stable/ Pytest-html Documentation: https://pypi.org/project/pytest-html/ Pytest-xdist Documentation: https://pypi.org/project/pytest-xdist/ Allure-pytest Documentation: https://pypi.org/project/allure-pytest/ Allure Documentation: https://docs.qameta.io/allure/ GitHub Action Documentation: https://docs.github.com/en/actions Implementation of API Automation Testing with Testing Tools Postman API Automation Testing Postman Framework Tutorial Directory The directory is not clickable, only for displaying the structure\nImplementation of API Automation Projects with Java and REST Assured Framework REST Assured Framework Tutorial Table of Contents Corresponding Articles for REST Assured Framework Tutorial Reference Documents for REST Assured Framework Tutorial Implementation of API Automation Projects with JavaScript and SuperTest Framework SuperTest Framework Tutorial Table of Contents Corresponding Articles for SuperTest Framework Tutorial Reference Documents for SuperTest Framework Tutorial Implementation of API Automation Projects with Python and Pytest Framework Pytest Framework Tutorial Table of Contents Corresponding Articles for Pytest Framework Tutorial Reference Documents for Pytest Framework Tutorial Implementation of API Automation Testing with Testing Tools Postman API Automation Testing Postman Framework Tutorial Directory Postman Framework Tutorial Articles Postman Framework Tutorial Reference Documents Bruno API Automation Testing Bruno Framework Tutorial Directory Bruno Framework Tutorial Articles Bruno Framework Tutorial Reference Documents Recommended Reading Postman Framework Tutorial Articles Postman API Automation Testing Tutorial: Advanced Usage - Common Command Line Options, File Upload Scenarios, and SSL Certificate Scenarios: https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-advance-usage-common-command-line-options-and-file-upload/ Postman API Automation Testing Tutorial: Advanced Usage - Data-Driven:https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-advance-usage-data-driven-and-environment-data-driven/ Postman API Automation Testing Tutorial: Advanced Usage - Common Test Scripts and Examples of Commonly Used Third-Party Packages: https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-advance-usage-common-test-scripts-and-commonly-used-third-party-packages/ Postman API Automation Testing Tutorial: Advanced Usage - Integration with CI/CD and GitHub Action, Allure Report: https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-advance-usage-integration-html-report-and-allure-report-integration-github-action/ Postman API Automation Testing Tutorial: Getting Started and Building Your Own Project from 0 to 1: https://naodeng.tech/zh/posts/api-automation-testing/postman-tutorial-getting-started-and-building-your-own-project-from-0-to-1/ Postman Framework Tutorial Reference Documents Demo Project Repository: Link Postman Official Documentation: Link Newman Official Documentation: Link GitHub Action Documentation: Link Allure Documentation: Link Bruno API Automation Testing Bruno Framework Tutorial Directory The directory is not clickable, only for displaying the structure\nbruno-user-guide Why Choose Bruno Installing Bruno Getting Started with the Client Default Main API API Request Collections API Requests Writing API Request Test Scripts Environment Variables API Script API Automation Preconditions Demo of API Automation Project Integration with CI Integration with GitHub Action Migration from Postman Scripts API Request Collection Migration Environment Variable Migration Reference for Test Script Migration Bruno Framework Tutorial Articles Introduction to Bruno, a Postman Replacement Tool: https://naodeng.tech/zh/posts/api-automation-testing/introduction_of_bruno/ Bruno Framework Tutorial Reference Documents Demo Project Repository: https://github.com/Automation-Test-Starter/Bruno-API-Test-Starter Bruno Documentation: https://docs.usebruno.com/ GitHub Action Documentation: https://docs.github.com/en/actions Recommended Reading Quick Start Series for API Automation Testing Using Postman Quick Start Series for API Automation Testing Using Pytest Quick Start Series for API Automation Testing Using SuperTest Quick Start Series for API Automation Testing Using Rest Assured Quick Start Series for Performance Testing Using Gatling ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/a-collection-of-tutorials-on-api-automation-testing-for-different-frameworks-and-different-development-languages/","summary":"This blog post compiles tutorials on API automation testing using various frameworks and programming languages, providing readers with comprehensive learning resources. It covers a range of popular testing frameworks and programming languages, enabling you to choose the best solution for your project. Whether you\u0026rsquo;re a developer in Python, Java, JavaScript, or any other language, and whether you prefer using REST Assured, SuperTest, or other frameworks, this collection will offer you in-depth learning guides to help you navigate the field of API automation testing with ease. A must-read resource to master the various tools and techniques in API automation testing.","title":"API Testing Tutorial for Beginners: different frameworks and different development languages"},{"content":"Advanced Usage This section will introduce some advanced features of Postman and Newman, including common command-line options, file upload scenarios, and SSL certificate configurations.\nFile Upload Scenarios When performing interface automation with Postman and Newman, file uploads can be achieved using the form-data method.\nThe file must exist in the current working directory, and the \u0026ldquo;src\u0026rdquo; attribute in the request must also include the filename.\nIn this collection, the file \u0026ldquo;demo.txt\u0026rdquo; should be present in the current working directory.\n{ \u0026#34;info\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;file-upload\u0026#34; }, \u0026#34;item\u0026#34;: [ { \u0026#34;request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;https://postman-echo.com/post\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, \u0026#34;body\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;formdata\u0026#34;, \u0026#34;formdata\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;src\u0026#34;: \u0026#34;demo.txt\u0026#34; } ] } } } ] } Note: Adjust the path for file uploads to ensure that the file exists in the project\u0026rsquo;s root directory or use an absolute path.\nCommon Newman Command-Line Options Newman is a command-line tool used to run Postman collections. It provides many options that can be used when running collections.\nHere are some common Newman command-line options along with examples:\nBasic Commands newman run \u0026lt;collection\u0026gt;： Used to run a Postman collection.\nnewman run collection.json -e, --environment \u0026lt;environment\u0026gt;： Specify an environment file.\nnewman run collection.json -e environment.json -g, --globals \u0026lt;globals\u0026gt;： Specify a global variables file.\nnewman run collection.json -g globals.json -d, --iteration-data \u0026lt;data\u0026gt;： Specify a data file for data-driven testing.\nnewman run collection.json -d data-file.csv Output and Reporting -r, --reporters \u0026lt;reporters\u0026gt;： Specify reporters to generate multiple reports, such as cli, json, html, etc.\nnewman run collection.json -r cli,json --reporter-json-export \u0026lt;file\u0026gt;： Export test results as a JSON file.\nnewman run collection.json --reporters json --reporter-json-export output.json --reporter-html-export \u0026lt;file\u0026gt;： Export test results as an HTML file.\nnewman run collection.json --reporters html --reporter-html-export output.html --reporter-html-template \u0026lt;file\u0026gt;： Use a custom HTML template to generate HTML reports.\nnewman run collection.json --reporters html --reporter-html-template custom-template.hbs Other Options -h, --help： Display help information, listing all command-line options.\nnewman run --help -v, --version： Display Newman version information.\nnewman --version -x, --suppress-exit-code： Do not return a non-zero exit code on failure.\nnewman run collection.json -x --delay-request \u0026lt;ms\u0026gt;： Set a delay between requests to simulate real-world scenarios.\nnewman run collection.json --delay-request 1000 --timeout \u0026lt;ms\u0026gt;： Set the timeout for requests.\nnewman run collection.json --timeout 5000 --no-color： Disable colored output in the console.\nnewman run collection.json --no-color --bail： Stop running on the first failed test.\nnewman run collection.json --bail These are just some common Newman command-line options. You can run newman run --help to see all available options and their descriptions. Depending on your testing needs, you may need to adjust and combine these options.\nSSL Certificate Configuration Client certificates are an alternative to traditional authentication mechanisms. They allow users to send authenticated requests to servers using public certificates and optional private keys to verify certificate ownership. In some cases, the private key may also be protected by a secret passphrase, providing an additional layer of authentication security.\nNewman supports SSL client certificates through the following CLI options:\nUsing a Single SSL Client Certificate Add the following options directly after the newman command based on your certificate situation.\n--ssl-client-cert Followed by the path to the public client certificate file.\n--ssl-client-key Followed by the path to the client private key (optional).\n--ssl-client-passphrase Followed by the secret passphrase used to protect the private client key (optional).\nUsing Multiple SSL Client Certificates Applicable when you need to support multiple certificates for each run.\n--ssl-client-cert-list Path to the SSL client certificate list configuration file (in JSON format). Reference example/ssl-client-cert-list.json.\n[ { \u0026#34;name\u0026#34;: \u0026#34;domain1\u0026#34;, \u0026#34;matches\u0026#34;: [\u0026#34;https://test.domain1.com/*\u0026#34;, \u0026#34;https://www.domain1/*\u0026#34;], \u0026#34;key\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;./client.domain1.key\u0026#34;}, \u0026#34;cert\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;./client.domain1.crt\u0026#34;}, \u0026#34;passphrase\u0026#34;: \u0026#34;changeme\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;domain2\u0026#34;, \u0026#34;matches\u0026#34;: [\u0026#34;https://domain2.com/*\u0026#34;], \u0026#34;key\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;./client.domain2.key\u0026#34;}, \u0026#34;cert\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;./client.domain2.crt\u0026#34;}, \u0026#34;passphrase\u0026#34;: \u0026#34;changeme\u0026#34; } ] Additionally, this JSON configuration is suitable for different certificates in different environments based on matches for different URLs and hostnames.\nNote: This option allows setting different SSL client certificates based on the URL or hostname. This option takes precedence over \u0026ndash;ssl-client-cert, \u0026ndash;ssl-client-key, and \u0026ndash;ssl-client-passphrase options. If there are no matching URLs in the list, these options will be used as fallback.\nTrusted CA Certificates Applicable when you need to trust custom CA certificates.\nIf you don\u0026rsquo;t want to use the \u0026ndash;insecure option, you can provide additional trusted CA certificates like this:\n--ssl-extra-ca-certs Followed by a list of file paths to one or more PEM format trusted CA certificates. Reference Documents Postman Official Documentation https://learning.postman.com/docs/getting-started/introduction/ Newman Official Documentation https://github.com/postmanlabs/newman?tab=readme-ov-file#command-line-options ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/postman-tutorial-advance-usage-common-command-line-options-and-file-upload/","summary":"This blog post takes a deep dive into the advanced usage of Postman API automation testing, focusing on common command line options, file upload scenarios, and SSL certificate scenarios. Learn how to use common command line options to optimize the testing process and solve the testing challenges of special scenarios such as file upload and SSL certificate.","title":"Postman API Automation Testing Tutorial Advance Usage common command line options and file upload"},{"content":"Advanced Usage This section explores some advanced features of Postman and Newman, including data-driven testing and environment variable data-driven testing.\nData-Driven Testing In the process of API automation testing, data-driven testing is a common approach where the input and expected output data of test cases are stored in data files. The testing framework executes multiple tests based on these data files to validate various aspects of the API.\nData-driven testing allows for easy modification of test data without altering the test case code, providing flexibility in testing scenarios and ensuring the API functions correctly under various input data.\nRefer to the demo: Postman-Newman-demo\nFor data-driven testing in Postman, especially using JSON data as test data, it can be achieved by combining environment variables and data files with the testing scripts provided by Postman. The usage of environment variables and data files is illustrated with simple examples.\nUsing Environment Variables The general steps are to store test data in environment variables and then read the data from these variables in the testing scripts.\n1. Create Environment Variables In Postman, you can create environment variables in the \u0026ldquo;Manage Environments\u0026rdquo; window. Each environment can have a set of variables. For example, in the DemoEnv environment, variables such as baseURL, getAPI, getAPIResponseStatus, and others can be added to store various test data.\n2. Use Environment Variables In the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; sections, you can use environment variables to store and retrieve data. In the request body, you can use pm.environment.get to fetch the value of an environment variable.\nEdit the get-demo API:\nModify the URL to {{baseURL}}/{{getAPI}}. Edit the Tests script to validate the response data. // Fetch data from environment variables const getAPIResponseStatus = parseInt(pm.environment.get(\u0026#34;getAPIResponseStatus\u0026#34;)); const getAPIResponseData = JSON.parse(pm.environment.get(\u0026#39;getAPIResponseData\u0026#39;)); pm.test(\u0026#34;res.status should be 200\u0026#34;, function () { pm.response.to.have.status(getAPIResponseStatus); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(getAPIResponseData.id); pm.expect(data.userId).to.equal(getAPIResponseData.userId); pm.expect(data.title).to.equal(getAPIResponseData.title); pm.expect(data.body).to.equal(getAPIResponseData.body); }); Edit the post-demo API:\nModify the URL to {{baseURL}}/{{postAPI}}. Edit the Tests script to validate the response data. // Fetch data from environment variables const postAPIResponseStatus = parseInt(pm.environment.get(\u0026#34;postAPIResponseStatus\u0026#34;)); const postAPIResponseData = JSON.parse(pm.environment.get(\u0026#39;postAPIResponseData\u0026#39;)); pm.test(\u0026#34;res.status should be 201\u0026#34;, function () { pm.response.to.have.status(postAPIResponseStatus); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(postAPIResponseData.id); pm.expect(data.userId).to.equal(postAPIResponseData.userId); pm.expect(data.title).to.equal(postAPIResponseData.title); pm.expect(data.body).to.equal(postAPIResponseData.body); }); Click Save and then click Send to be shown that the test passes. 3. Debugging Environment Variable Data-Driven Scripts Select the corresponding environment variable and the updated test case, run the entire demo collection, and confirm that the tests pass.\n4.Automated Execution of Environment Variable Data-Driven Scripts Export the updated test cases to the test case folder of the automation test project. Adjust the package.json file: In the package.json file, update the test script to run the environment variable data-driven test cases:\n\u0026#34;environment-driven-test\u0026#34;: \u0026#34;newman run Testcase/Environment-Driven.postman_collection.json -e Env/Environment-Driven-DemoEnv.postman_environment.json -r cli,allure --reporter-allure-export ./allure-results\u0026#34;, Run the test: npm run environment-driven-test Using Data Files The general steps are to store test data in data files and then read the data from these files in the testing scripts. Postman supports various data file formats such as JSON, CSV, and TXT. The following example uses JSON format.\n1. Create Data Files Create a Data folder under the Postman API automation testing project. mkdir Data Create a JSON format data file named testdata.json under the Data folder. cd Data touch testdata.json Update the test data file testdata.json. [ { \u0026#34;getAPI\u0026#34;: \u0026#34;posts/1\u0026#34;, \u0026#34;postAPI\u0026#34;: \u0026#34;posts\u0026#34;, \u0026#34;getAPIResponseStatus\u0026#34;: 200, \u0026#34;getAPIResponseData\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, \u0026#34;postAPIResponseStatus\u0026#34;: 201, \u0026#34;postAPIResponseData\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 } } ] 2. Update Test Cases Update the get-demo API:\nEdit the Pre-request Script to fetch the request URL from the test data file. const getAPI = pm.iterationData.get(\u0026#39;getAPI\u0026#39;); Modify the URL to {{baseURL}}/{{getAPI}}.\nEdit the Tests script to fetch test data from the test data file.\nconst getAPIResponseStatus = pm.iterationData.get(\u0026#39;getAPIResponseStatus\u0026#39;); const getAPIResponseData = pm.iterationData.get(\u0026#39;getAPIResponseData\u0026#39;); pm.test(\u0026#34;res.status should be 200\u0026#34;, function () { pm.response.to.have.status(getAPIResponseStatus); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(getAPIResponseData.id); pm.expect(data.userId).to.equal(getAPIResponseData.userId); pm.expect(data.title).to.equal(getAPIResponseData.title); pm.expect(data.body).to.equal(getAPIResponseData.body); }); Update the post-demo API:\nEdit the Pre-request Script to fetch the request URL from the test data file. const postAPI = pm.iterationData.get(\u0026#39;postAPI\u0026#39;); Modify the URL to {{baseURL}}/{{postAPI}}.\nEdit the Tests script to fetch test data from the test data file.\nconst postAPIResponseStatus = pm.iterationData.get(\u0026#39;postAPIResponseStatus\u0026#39;); const postAPIResponseData = pm.iterationData.get(\u0026#39;postAPIResponseData\u0026#39;); pm.test(\u0026#34;res.status should be 201\u0026#34;, function () { pm.response.to.have.status(postAPIResponseStatus); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(postAPIResponseData.id); pm.expect(data.userId).to.equal(postAPIResponseData.userId); pm.expect(data.title).to.equal(postAPIResponseData.title); pm.expect(data.body).to.equal(postAPIResponseData.body); }); 3. Debugging In the Postman application, select the get-demo and post-demo requests in the demo collection, click the three dots in the upper right corner, choose \u0026ldquo;Run Collection.\u0026rdquo; In the runner preparation page, click the \u0026ldquo;Select File\u0026rdquo; button on the right side of Data, choose the previous test data file testdata.json. Click \u0026ldquo;Run demo,\u0026rdquo; confirm a successful run, and then export the test case file. 4. Automated Execution of Data-Driven Scripts Export the updated test cases to the test case folder of the automation test project. Adjust the package.json file: In the package.json file, update the test script to run the data-driven test cases:\n\u0026#34;data-driven-test\u0026#34;: \u0026#34;newman run Testcase/Data-Driven.postman_collection.json -e Env/DemoEnv.postman_environment.json -d Data/testdata.json -r cli,allure --reporter-allure-export ./allure-results\u0026#34; Run the test: npm run data-driven-test Reference Documents Postman Official Documentation: https://learning.postman.com/docs/getting-started/introduction/ Newman Official Documentation: https://learning.postman.com/docs/running-collections/using-newman-cli/command-line-integration-with-newman/ ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/postman-tutorial-advance-usage-data-driven-and-environment-data-driven/","summary":"This blog post dives into advanced techniques for Postman API automation testing, focusing on data file driving and environment variable data driving. Learn how to elegantly perform test data driving and improve test coverage with external data files and flexible environment variables. The blog post will show you how to manage and utilize data in a smarter way to make test cases more scalable and flexible.","title":"Postman API Automation Testing Tutorial Advance Usage Data Driven"},{"content":"Advanced Usage This section will introduce some advanced features of Postman and Newman, including commonly used response test scripts, pre-request scripts, and third-party packages available for test scripts.\nCommon Test Scripts Postman provides a test script feature that allows you to write JavaScript scripts to validate the response and behavior of your API. These scripts can be added under the \u0026ldquo;Tests\u0026rdquo; tab of a request and are divided into pre-request scripts (Pre-request Script) and post-response scripts (Tests). Here are some common Postman and Newman test scripts:\nResponse Test Scripts Status Code Check:\npm.test(\u0026#34;Status code is 200\u0026#34;, function () { pm.response.to.have.status(200); }); Response Time Check:\npm.test(\u0026#34;Response time is less than 200ms\u0026#34;, function () { pm.expect(pm.response.responseTime).to.be.below(200); }); Response Body JSON Format Check:\npm.test(\u0026#34;Response body is a valid JSON\u0026#34;, function () { pm.response.to.be.json; }); Response Body Field Value Check:\npm.test(\u0026#34;Response body contains expected value\u0026#34;, function () { pm.expect(pm.response.json().key).to.eql(\u0026#34;expectedValue\u0026#34;); }); Response Body Array Length Check:\npm.test(\u0026#34;Response body array has correct length\u0026#34;, function () { pm.expect(pm.response.json().arrayKey).to.have.lengthOf(3); }); Response Body Property Existence Check:\npm.test(\u0026#34;Response body has required properties\u0026#34;, function () { pm.expect(pm.response.json()).to.have.property(\u0026#34;key\u0026#34;); }); Pre-request Scripts Dynamically Set Request Parameters:\npm.variables.set(\u0026#34;dynamicVariable\u0026#34;, \u0026#34;dynamicValue\u0026#34;); Set Request Header Using Global Variable:\npm.request.headers.add({ key: \u0026#39;Authorization\u0026#39;, value: pm.globals.get(\u0026#39;authToken\u0026#39;) }); Generate Random Number:\nconst randomNumber = Math.floor(Math.random() * 1000); pm.variables.set(\u0026#34;randomNumber\u0026#34;, randomNumber); Generate Signature or Encryption:\n// Example: Use CryptoJS for HMAC SHA256 signature const CryptoJS = require(\u0026#39;crypto-js\u0026#39;); const secretKey = \u0026#39;yourSecretKey\u0026#39;; const message = \u0026#39;dataToSign\u0026#39;; const signature = CryptoJS.HmacSHA256(message, secretKey).toString(CryptoJS.enc.Base64); pm.variables.set(\u0026#34;signature\u0026#34;, signature); Third-Party Libraries in Test Scripts The provided require method allows you to use built-in library modules in the sandbox. Here are some common libraries and examples. More available libraries can be found here.\nChai.js Assertion Library Methods In Postman\u0026rsquo;s test scripts, you can use the Chai assertion library to write assertions to validate the response of your API. Chai provides various assertion styles, including BDD (Behavior-Driven Development) and TDD (Test-Driven Development). Here are some basic usage examples:\n1. Install Chai In the Postman script environment, you don\u0026rsquo;t need to install Chai separately as Postman already includes Chai by default.\n2. Use BDD Style Assertions In the \u0026ldquo;Tests\u0026rdquo; section of Postman, you can use Chai\u0026rsquo;s BDD style assertions, for example:\n// Include Chai library const chai = require(\u0026#39;chai\u0026#39;); // Use BDD style assertions const expect = chai.expect; // Example: Verify the response status code is 200 pm.test(\u0026#39;Status code is 200\u0026#39;, function() { expect(pm.response.code).to.equal(200); }); // Example: Verify the response body is JSON pm.test(\u0026#39;Response body is JSON\u0026#39;, function() { expect(pm.response.headers.get(\u0026#39;Content-Type\u0026#39;)).to.include(\u0026#39;application/json\u0026#39;); }); 3. Use TDD Style Assertions // Include Chai library const chai = require(\u0026#39;chai\u0026#39;); // Use TDD style assertions const assert = chai.assert; // Example: Use assert to verify the response status code is 200 assert.equal(pm.response.code, 200, \u0026#39;Status code should be 200\u0026#39;); 4. Common Assertions Supported by Chai Equality:\nexpect(actual).to.equal(expected); Inclusion:\nexpect(actual).to.include(expected); Type Checking:\nexpect(actual).to.be.a(\u0026#39;string\u0026#39;); Greater Than/Less Than:\nexpect(actual).to.be.above(expected); expect(actual).to.be.below(expected); Null/Not Null:\nexpect(actual).to.be.null; expect(actual).to.not.be.null; Deep Equality:\nexpect(actual).to.deep.equal(expected); The above are just some basic usage of the Chai assertion library. You can use more assertion methods and combinations based on your needs. Chai provides a rich set of assertion features to meet various testing requirements. For more detailed information, please refer to the Chai Documentation.\nUsing Cheerio to Manipulate HTML Files In Postman, Cheerio is a jQuery-based library for server-side manipulation of HTML documents. It allows you to use jQuery-like syntax to select and manipulate HTML elements on the server side, making it suitable for parsing and extracting information from HTML pages. In Postman, you can use the Cheerio library for parsing HTML responses. Here are the basic usage steps for Cheerio in Postman:\nInstall Cheerio:\nSince Postman uses the Node.js runtime environment, you can install Cheerio in Postman scripts. In the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; section of your request, you can install Cheerio as follows: // Install Cheerio const cheerio = require(\u0026#39;cheerio\u0026#39;); Parse HTML with Cheerio:\nIn the \u0026ldquo;Tests\u0026rdquo; section of your request, you can use Cheerio to parse HTML. Here\u0026rsquo;s a simple example: // Get HTML content from the response const htmlContent = pm.response.text(); // Parse HTML with Cheerio const $ = cheerio.load(htmlContent); // Example: Extract text from the title tag const titleText = $(\u0026#39;title\u0026#39;).text(); console.log(\u0026#39;Title:\u0026#39;, titleText); // Example: Extract the href attribute from all links const links = []; $(\u0026#39;a\u0026#39;).each(function () { const link = $(this).attr(\u0026#39;href\u0026#39;); links.push(link); }); console.log(\u0026#39;Links:\u0026#39;, links); In the example above, cheerio.load(htmlContent) is used to load HTML content, and jQuery-like syntax is used to select and manipulate elements.\nConsiderations:\nCheerio is primarily used for parsing static HTML. It may not work well with content generated dynamically using JavaScript. In such cases, you might consider using Puppeteer or other tools that support JavaScript execution. This is just the basic usage of Cheerio in Postman. You can use various selectors and methods provided by Cheerio according to your specific needs. Refer to the Cheerio Documentation for more detailed information.\nValidating JSON Schema with tv4 In Postman, tv4 is a JSON Schema validation library used to validate whether JSON data conforms to a given JSON Schema. JSON Schema is a specification for describing the structure of JSON objects, defining properties, types, and other constraints.\nHere are the basic steps for using tv4 to validate JSON Schema in Postman:\nInstall tv4 Library:\nSince Postman uses the Node.js runtime environment, you can install tv4 in Postman scripts. In the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; section of your request, you can install tv4 as follows: // Install tv4 const tv4 = require(\u0026#39;tv4\u0026#39;); Define JSON Schema:\nIn Postman, you can define the JSON Schema in the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; section. JSON Schema can be defined as a JavaScript object. Here\u0026rsquo;s a simple example: // Define JSON Schema const jsonSchema = { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;number\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;] }; Validate with tv4:\nIn the \u0026ldquo;Tests\u0026rdquo; section of your request, you can use tv4 to validate JSON data against the defined JSON Schema. Here\u0026rsquo;s a simple example: // Get JSON data from the response const jsonResponse = pm.response.json(); // Validate JSON against the schema using tv4 const isValid = tv4.validate(jsonResponse, jsonSchema); // Check the validation result pm.test(\u0026#39;JSON is valid according to the schema\u0026#39;, function() { pm.expect(isValid).to.be.true; }); In the example above, tv4.validate(jsonResponse, jsonSchema) is used to validate whether the JSON response conforms to the specified schema. The validation result is stored in the isValid variable, and pm.test is used to check the validation result.\nThis is just the basic usage of tv4 in Postman. You can define more complex JSON Schemas and use other features of tv4 for flexible validation according to your specific requirements. Refer to the tv4 Documentation for more detailed information.\nGenerating UUIDs In Postman, you can use the uuid module to generate UUIDs (Universally Unique Identifiers), also known as GUIDs. Here\u0026rsquo;s the basic usage of the uuid module in Postman:\n1. Install the uuid Module In the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; section of your Postman request, you can install the uuid module as follows:\n// Install the uuid module const uuid = require(\u0026#39;uuid\u0026#39;); 2. Generate UUID // Generate UUID const generatedUUID = uuid.v4(); console.log(\u0026#39;Generated UUID:\u0026#39;, generatedUUID); In the example above, uuid.v4() is used to generate a UUID based on random numbers. You can use the generated UUID in your Postman script, such as setting it as the value for a request header or parameter.\nExample Here\u0026rsquo;s an example of generating a UUID and setting it as a request header in the \u0026ldquo;Pre-request Script\u0026rdquo; of a Postman request:\n// Install the uuid module const uuid = require(\u0026#39;uuid\u0026#39;); // Generate UUID const generatedUUID = uuid.v4(); // Set request header pm.request.headers.add({ key: \u0026#39;X-Request-ID\u0026#39;, value: generatedUUID }); In the example above, X-Request-ID is a common request header used to identify the uniqueness of the request. The generated UUID is set as the value for this request header to ensure a unique identifier for each request.\nNote that Postman automatically performs the steps to install dependencies\nwhen running scripts, so manual installation of the uuid module is not necessary.\nConverting XML to JavaScript Objects with xml2js In Postman, xml2js is a library used to convert XML into JavaScript objects. In the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; section of your Postman request, you can use xml2js to handle XML responses and transform them into JavaScript objects. Here are the basic steps for using xml2js in Postman:\nInstall the xml2js Library:\nSince Postman uses the Node.js runtime environment, you can install xml2js in Postman scripts. In the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; section of your request, you can install xml2js as follows: // Install xml2js const xml2js = require(\u0026#39;xml2js\u0026#39;); Parse XML Response:\nAfter getting the XML response, you can use xml2js to parse it into a JavaScript object. Here\u0026rsquo;s a simple example: // Get the content of the response as XML const xmlContent = pm.response.text(); // Use xml2js to parse XML xml2js.parseString(xmlContent, function (err, result) { if (err) { console.error(\u0026#39;Error parsing XML:\u0026#39;, err); return; } // result is the parsed JavaScript object console.log(\u0026#39;Parsed XML:\u0026#39;, result); }); In the example above, xml2js.parseString(xmlContent, function (err, result) {...} is used to asynchronously parse the XML content. The parsed JavaScript object is stored in the result variable.\nHandle Parsed JavaScript Object:\nOnce you have the parsed JavaScript object, you can access and manipulate its properties using regular JavaScript object handling techniques. // Example: Access a property of the parsed JavaScript object const value = result.root.element[0].subelement[0]._; console.log(\u0026#39;Value from parsed XML:\u0026#39;, value); In the example above, result.root.element[0].subelement[0]._ is an example of accessing a property of the parsed object. The structure depends on your XML structure.\nThis is just the basic usage of xml2js in Postman. You can use other features of xml2js, such as setting parsing options or handling namespaces, based on your specific needs. Refer to the xml2js Documentation for more detailed information.\nCommon Utility Functions with util In Postman, util is a global object that provides some common utility functions for use in Postman scripts. Here are some common util functions and their usage:\n1. util.guid() - Generate a Globally Unique Identifier (GUID) // Generate a globally unique identifier const uniqueId = util.guid(); console.log(\u0026#39;Unique ID:\u0026#39;, uniqueId); 2. util.timestamp() - Get the Current Timestamp // Get the current timestamp (in milliseconds) const timestamp = util.timestamp(); console.log(\u0026#39;Timestamp:\u0026#39;, timestamp); 3. util.randomInt(min, max) - Generate a Random Integer in a Specified Range // Generate a random integer between 1 and 100 const randomInt = util.randomInt(1, 100); console.log(\u0026#39;Random Integer:\u0026#39;, randomInt); 4. util.unixTimestamp() - Get the Current Timestamp in Unix Timestamp (seconds) // Get the current timestamp (in seconds) const unixTimestamp = util.unixTimestamp(); console.log(\u0026#39;Unix Timestamp:\u0026#39;, unixTimestamp); 5. util.encodeBase64(str) and util.decodeBase64(base64Str) - Base64 Encoding and Decoding // Base64 encoding const encodedString = util.encodeBase64(\u0026#39;Hello, World!\u0026#39;); console.log(\u0026#39;Encoded String:\u0026#39;, encodedString); // Base64 decoding const decodedString = util.decodeBase64(encodedString); console.log(\u0026#39;Decoded String:\u0026#39;, decodedString); 6. util.each(obj, callback) - Iterate Over an Object or Array // Iterate over an array const array = [1, 2, 3, 4]; util.each(array, function (value, index) { console.log(`Index ${index}: ${value}`); }); // Iterate over an object const obj = { a: 1, b: 2, c: 3 }; util.each(obj, function (value, key) { console.log(`Key ${key}: ${value}`); }); Notes:\nIn Postman scripts, you can directly use these utility functions via the util object. These methods provided by the util object simplify some common tasks in Postman scripts, such as generating random numbers, handling timestamps, and encoding/decoding strings. Please refer to the Postman official documentation, as Postman continues to update and improve its script environment, and new utility functions may be introduced. Stream Operations with stream In Node.js, streams are often used to handle large amounts of data, effectively reducing memory usage and improving performance. Here are some basic usage examples of streams in Node.js that you can refer to for data or file processing.\n1. Readable Streams: const fs = require(\u0026#39;fs\u0026#39;); // Create a readable stream const readableStream = fs.createReadStream(\u0026#39;input.txt\u0026#39;); // Set encoding (if it\u0026#39;s a text file) readableStream.setEncoding(\u0026#39;utf-8\u0026#39;); // Handle data readableStream.on(\u0026#39;data\u0026#39;, function(chunk) { console.log(\u0026#39;Received chunk:\u0026#39;, chunk); }); // Handle end readableStream.on(\u0026#39;end\u0026#39;, function() { console.log(\u0026#39;Stream ended.\u0026#39;); }); // Handle error readableStream.on(\u0026#39;error\u0026#39;, function(err) { console.error(\u0026#39;Error:\u0026#39;, err); }); 2. Writable Streams: const fs = require(\u0026#39;fs\u0026#39;); // Create a writable stream const writableStream = fs.createWriteStream(\u0026#39;output.txt\u0026#39;); // Write data writableStream.write(\u0026#39;Hello, World!\\n\u0026#39;); writableStream.write(\u0026#39;Another line.\u0026#39;); // End writing writableStream.end(); // Handle finish writableStream.on(\u0026#39;finish\u0026#39;, function() { console.log(\u0026#39;Write completed.\u0026#39;); }); // Handle error writableStream.on(\u0026#39;error\u0026#39;, function(err) { console.error(\u0026#39;Error:\u0026#39;, err); }); 3. Transform Streams: const { Transform } = require(\u0026#39;stream\u0026#39;); // Create a transform stream const myTransform = new Transform({ transform(chunk, encoding, callback) { // Transform data const transformedData = chunk.toString().toUpperCase(); this.push(transformedData); callback(); } }); // Pipe connecting readable stream, transform stream, and writable stream readableStream.pipe(myTransform).pipe(writableStream); This is just some basic usage of streams in Node.js. In Postman, you can use these methods in the scripts of your requests, such as the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; sections, by executing these scripts in the Node.js runtime environment. Please note that the stream API in Node.js can be more complex, for example, by using the pipeline function to handle the connection of multiple streams.\nTimers: timers In Postman, you can use the timer functionality of Node.js to handle scheduled tasks or operations with a delay. Here are some basic usages of Node.js timers that can be used in Postman scripts.\n1. setTimeout - Delayed Execution // Delayed execution of an operation setTimeout(function() { console.log(\u0026#39;Delayed operation.\u0026#39;); }, 2000); // 2000 milliseconds (2 seconds) 2. setInterval - Periodic Execution // Periodic execution of a repeated operation const intervalId = setInterval(function() { console.log(\u0026#39;Repeated operation.\u0026#39;); }, 3000); // 3000 milliseconds (3 seconds) // Cancel periodic execution // clearInterval(intervalId); 3. Usage in Postman In Postman, you can use these timers in the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; sections. For example, delaying an operation in the \u0026ldquo;Tests\u0026rdquo; section:\n// Delayed operation in the \u0026#34;Tests\u0026#34; section setTimeout(function() { console.log(\u0026#39;Delayed operation in Tests.\u0026#39;); }, 2000); // 2000 milliseconds (2 seconds) Please note that the code executed in the \u0026ldquo;Pre-request Script\u0026rdquo; or \u0026ldquo;Tests\u0026rdquo; sections of Postman is running in the Node.js environment, so you can use most features supported by Node.js, including timers.\nIn the examples above, setTimeout executes an operation once after a specified delay, and setInterval executes an operation periodically at a specified interval. In Postman, you can use these timers according to your specific needs.\nEvents Handling: events In the Postman script environment, you can use Node.js events module to handle events. The events module provides the EventEmitter class, which can be used to define and trigger events. Here are some basic usages of using the events module in Postman with Node.js:\n1. Creating an Event Emitter const EventEmitter = require(\u0026#39;events\u0026#39;); const myEmitter = new EventEmitter(); 2. Defining an Event Handling Function // Define an event handling function function myEventHandler() { console.log(\u0026#39;Event handled.\u0026#39;); } 3. Registering an Event Handling Function // Register an event handling function myEmitter.on(\u0026#39;myEvent\u0026#39;, myEventHandler); 4. Triggering an Event // Trigger an event myEmitter.emit(\u0026#39;myEvent\u0026#39;); 5. Example In the Postman script environment, you can use events to implement callbacks or handling for asynchronous operations. Here\u0026rsquo;s a simple example demonstrating how to trigger an event after completing an asynchronous operation:\nconst EventEmitter = require(\u0026#39;events\u0026#39;); const myEmitter = new EventEmitter(); // Simulate an asynchronous operation function performAsyncOperation() { setTimeout(function() { console.log(\u0026#39;Async operation completed.\u0026#39;); // Trigger the event myEmitter.emit(\u0026#39;asyncOperationComplete\u0026#39;); }, 2000); } // Register an event handling function myEmitter.on(\u0026#39;asyncOperationComplete\u0026#39;, function() { console.log(\u0026#39;Handling async operation completion.\u0026#39;); // You can perform logic here after the asynchronous operation completes }); // Execute the asynchronous operation performAsyncOperation(); In the above example, the performAsyncOperation function simulates an asynchronous operation, and when the operation completes, the asyncOperationComplete event is triggered using myEmitter.emit. In the event handling function, you can write logic to handle what happens after the asynchronous operation completes.\nPlease note that the execution of asynchronous operations in Postman scripts may be subject to limitations, so careful consideration is required in practical use.\nReference Documents Postman Official Documentation Newman Official Documentation ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/postman-tutorial-advance-usage-common-test-scripts-and-commonly-used-third-party-packages/","summary":"A deep dive into advanced usage of Postman API automation testing, focusing on commonly used test scripts and third-party package examples. Explores how to write powerful test scripts that cover a variety of testing scenarios and introduces some common third-party packages that optimize the testing process.","title":"Postman API Automation Testing Tutorial Advance Usage Common Test Scripts and Third-Party Packages"},{"content":"Advanced Usage This section will cover some advanced usages of Postman and Newman, including testing data, testing scripts, testing reports, and report integration. It will also explain how to integrate Postman and Newman into the CI/CD process for automated testing.\nGenerating HTML Test Reports Using the newman-reporter-htmlextra as an example, the demo will illustrate how to generate HTML test reports.\nInstalling the newman-reporter-htmlextra Dependency npm install newman-reporter-htmlextra --save-dev Note: Currently, there are compatibility issues with some packages in the latest version (V6) of Newman regarding HTML test reports. Therefore, version 5.1.2 is used here.\nAdjusting package.json In the package.json file, update the test script to run test cases and generate HTML test reports:\n\u0026#34;test\u0026#34;: \u0026#34;newman run Testcase/demo.postman_collection.json -e Env/DemoEnv.postman_environment.json -r htmlextra --reporter-htmlextra-export ./Report/Postman-newman-demo-api-testing-report.html\u0026#34; Specify the path for the HTML test report output as Report/Postman-newman-demo-api-testing-report.html\nRun Test Cases to Generate HTML Report Run the test cases npm run test Check the Report folder, you will find that a Postman-newman-demo-api-testing-report.html file has been generated. Open the Postman-newman-demo-api-testing-report.html file in a browser to view the HTML test report. Generating Reports in Multiple Formats The previous configuration is for generating HTML-format test reports. If you want to output reports in multiple formats, such as the command line (CLI) report, add the following script to the package.json file:\n\u0026#34;test\u0026#34;: \u0026#34;newman run Testcase/demo.postman_collection.json -e Env/DemoEnv.postman_environment.json -r cli,htmlextra --reporter-htmlextra-export ./Report/Postman-newman-demo-api-testing-report.html\u0026#34; Run the test cases again, and you will find both HTML and CLI format test reports in the Report folder.\nContinuous Integration (CI) with CI/CD Integrating API automation test code into the CI/CD process enables automated testing, improving testing efficiency.\nIntegrating with GitHub Actions Taking GitHub Actions as an example, similar steps can be followed for other CI tools.\nRefer to the demo: Postman-Newman-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory named .github/workflows. This will be the place to store GitHub Actions workflow files.\nCreate the workflow file: In the .github/workflows directory, create a YAML-formatted workflow file, for example, postman.yml.\nEdit the postman.yml file: Copy and paste the following content into the file:\nname: RUN Postman API Test CI on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: RUN-Postman-API-Test: runs-on: ubuntu-latest strategy: matrix: node-version: [ 18.x] # See supported Node.js release schedule at https://nodejs.org/en/about/releases/ steps: - uses: actions/checkout@v3 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v3 with: node-version: ${{ matrix.node-version }} cache: \u0026#39;npm\u0026#39; - name: Installation of related packages run: npm ci - name: RUN SuperTest API Testing run: npm test - name: Archive Postman test report uses: actions/upload-artifact@v3 with: name: Postman-test-report path: Report - name: Upload Postman report to GitHub uses: actions/upload-artifact@v3 with: name: Postman-test-report path: Report Commit your code: Add the postman.yml file to the repository and commit the changes. View the test report: In GitHub, navigate to your repository. Click on the Actions tab at the top and then click on the RUN-Postman-API-Test workflow on the left. You should see the workflow running, and once it completes, you can view the results. Integrating Allure Test Report Allure is a lightweight, flexible, and multi-language-supported test reporting tool that can generate various types of test reports, including pie charts, bar charts, line charts, etc., making it easy to visualize test results.\nInstalling Allure Test Report Dependencies npm install newman-reporter-allure --save-dev Adjusting the Script in package.json for Generating Allure Test Reports \u0026#34;test\u0026#34;: \u0026#34;newman run Testcase/demo.postman_collection.json -e Env/DemoEnv.postman_environment.json -r cli,allure --reporter-allure-export ./allure-results\u0026#34; Adjusting Postman Test Cases Modify the Tests script in the \u0026ldquo;get-demo\u0026rdquo; request. Add the following script to generate Allure test reports: // @allure.label.suite=postman-new-api-testing-demo // @allure.label.story=\u0026#34;Verify-the-get-api-return-correct-data\u0026#34; // @allure.label.owner=\u0026#34;naodeng\u0026#34; // @allure.label.tag=\u0026#34;GETAPI\u0026#34; pm.test(\u0026#34;res.status should be 200\u0026#34;, function () { pm.response.to.have.status(200); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(1); pm.expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); Adjust the Tests script in the \u0026ldquo;post-demo\u0026rdquo; request. Add the following script to generate Allure test reports: // @allure.label.suite=postman-new-api-testing-demo // @allure.label.story=\u0026#34;Verify-the-post-api-return-correct-data\u0026#34; // @allure.label.owner=\u0026#34;naodeng\u0026#34; // @allure.label.tag=\u0026#34;POSTAPI\u0026#34; pm.test(\u0026#34;res.status should be 201\u0026#34;, function () { pm.response.to.have.status(201); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(101); pm.expect(data.title).to.equal(\u0026#39;foo\u0026#39;); }); Save the modified Postman test cases, export the test case file again, and replace the original test case file. Run Test Cases to Generate Allure Report Run the test cases npm run test The allure-results folder will be generated in the project folder, containing the execution results of the test cases.\nPreviewing the Allure Test Report allure serve Reference Postman docs newman docs newman-reporter-htmlextra newman-reporter-allure github action docs ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/postman-tutorial-advance-usage-integration-html-report-and-allure-report-integration-github-action/","summary":"This advanced guide focuses on the integration of Postman API automation testing with CI/CD and GitHub Actions, along with the incorporation of Allure test reports. Learn how to seamlessly integrate Postman tests into the CI/CD process, achieving automated testing through GitHub Actions. Additionally, understand how to integrate the Allure test report framework to generate detailed test result reports.","title":"Postman API Automation Testing Tutorial Advance Usage Integration CI CD and allure test report"},{"content":"Introduction Introduction to API Testing What is API? API, which stands for Application Programming Interface, is a computing interface that defines the interactions between multiple software intermediaries. It specifies the types of calls or requests that can be made, how they are made, the data format to be used, and the conventions to be followed. APIs can also provide extension mechanisms, allowing users to extend existing functionalities in various ways. An API can be custom-made for a specific component or designed based on industry standards to ensure interoperability. By hiding information, APIs enable modular programming, allowing users to work independently using interfaces.\nWhat is API Testing? API testing is a type of software testing that includes two types: specifically testing the functionality of Application Programming Interfaces (referred to as API) and, more broadly, testing the overall functionality, reliability, security, and performance in integration testing by invoking APIs.\nAPI Best Practice:\nAPI definition follows the RESTful API style, with semantic URI definitions, accurate HTTP status codes, and the ability to understand the relationships between resources through API definitions. Detailed and accurate API documentation (such as Swagger documentation). External APIs may include version numbers for quick iteration (e.g., https://thoughtworks.com/v1/users/). Testing in different quadrants of the testing pyramid has different purposes and strategies. API testing mainly resides in the second and fourth quadrants.\nAPI testing holds a relatively high position in the testing pyramid, focusing on testing functionality and business logic at the boundaries of systems and services. It is executed after the service is built and deployed in the testing environment for validation.\nTypes of API Testing Functional Testing\nCorrectness Testing Exception Handling Internal Logic \u0026hellip; Non-functional Testing\nPerformance Security \u0026hellip; Steps in API Testing Send Request Get Response Verify Response Result Introduction to Postman and Newman Postman is a popular API development tool that provides an easy-to-use graphical interface for creating, testing, and debugging APIs. Postman also features the ability to easily write and share test scripts. It supports various HTTP request methods, including GET, POST, PUT, DELETE, etc., and can use various authentication and authorization methods for API testing.\nNewman is the command-line tool for Postman, used to run test suites without using the Postman GUI. With Newman, users can easily export Postman collections as an executable file and run them in any environment. Additionally, Newman supports generating test reports in HTML or Junit format and integrating into CI/CD pipelines for automated testing.\nIn summary, Postman is a powerful API development and testing tool, while Newman is a convenient command-line tool for running test suites without using the Postman GUI. Their combination enhances the efficiency and accuracy of API testing and development.\nIn addition to basic functionalities, Postman has the following features:\nEnvironment and Variable Management: Postman supports switching between different environments, such as development, testing, and production, and variable management, making it easy to set variables for different test cases and requests. Automated Testing: Users can create and run automated tests using Postman, integrating them into continuous integration or deployment processes for more accurate and efficient testing. Collaboration and Sharing: Postman supports sharing collections and environments with teams, facilitating collaboration among team members. Monitoring: Postman provides API monitoring, allowing real-time monitoring of API availability and performance. Meanwhile, Newman has the following characteristics:\nCommand-Line Interface: Newman can run in the command line, making it convenient for automated testing and integration into CI/CD processes. Support for Multiple Output Formats: Newman supports multiple output formats, including HTML, JSON, and JUnit formats, making it easy to use in different scenarios. Concurrent Execution: Newman supports concurrent test execution, improving testing efficiency. Lightweight: Compared to the Postman GUI, Newman is a lightweight tool, requiring fewer resources during test execution. In conclusion, Postman and Newman are essential tools for modern API testing, offering powerful features for efficient, accurate, and automated API testing and development.\nIn addition to the mentioned features and characteristics, Postman and Newman have other important functionalities and advantages:\nIntegration: Postman and Newman can integrate with many other tools and services, such as GitHub, Jenkins, Slack, etc., making it easy to integrate into development and deployment processes for more efficient API development and testing. Documentation Generation: Postman can generate API documentation using requests and responses, ensuring accurate and timely documentation. Test Scripts: Postman can use JavaScript to write test scripts, providing flexibility and customization in testing. Users can easily write custom test scripts to ensure the expected behavior of the API. History: Postman can store the history of API requests, making it convenient for users to view and manage previous requests and responses. This is useful for debugging and issue troubleshooting. Multi-Platform Support: Postman and Newman can run on multiple platforms, including Windows, MacOS, and Linux. In summary, Postman and Newman are powerful tools for modern API testing and development, offering rich features and flexible test scripts to help developers and testers build and test APIs faster and more accurately.\nProject Dependencies The following environments need to be installed in advance\nNode.js, with the demo version being v21.1.0 Postman installed, you can download the installation package from the official website and complete the installation Project Structure The following is the file structure of an API automation testing project for Postman and Newman, which contains test configuration files, test case files, test tool files, and test report files. It can be used for reference.\nPostman-Newman-demo ├── README.md ├── package.json ├── package-lock.json ├── Data // Test data folder │ └── testdata.csv // Test data file ├── Testcase // Test case folder │ └── APITestDemo.postman_collection.json // Test case file ├── Env // Test environment folder │ └── DemoEnv.postman_environment.json // Test environment file ├── Report // Test report folder │ └── report.html ├── .gitignore └── node_modules // Project dependencies └── ... Building a Postman API Automation Test Project from 0 to 1 Below, we will introduce how to build a Postman and Newman API automation test project from scratch, including test configuration, test cases, test environment, testing tools, and test reports.\nYou can refer to the demo project: Postman-Newman-demo\nCreate a New Project Folder mkdir Postman-Newman-demo Project initialization // enter the project folder cd Postman-Newman-demo // nodejs project initialization npm init -y Install dependencies Currently, the latest version of newman has some package compatibility issues reported by the html test, so we\u0026rsquo;re using version 5.1.2 here.\n// Install newman library npm install newman@5.1.2--save-dev Writing API Test Cases in Postman Creating a Collection and Request in Postman Open Postman, click the New button in the top left corner, select Collection, enter the name of the collection, click the Create Collection button to create a collection named \u0026ldquo;demo.\u0026rdquo; In the collection, click the three dots in the top right corner, select Add Request, enter the name of the request, and click the Save button to create a request named \u0026ldquo;get-demo.\u0026rdquo; Add another request named \u0026ldquo;post-demo.\u0026rdquo; Editing Request and Writing Test Cases Refer to the interface documentation in the demoAPI.md file in the project folder to obtain information such as the URL, request method, request headers, and request body used by the \u0026ldquo;demo\u0026rdquo; requests.\nget-demo In the \u0026ldquo;get-demo\u0026rdquo; request, select the GET request method and enter the URL as https://jsonplaceholder.typicode.com/posts/1. In the Headers section, add a header with Key as \u0026ldquo;Content-Type\u0026rdquo; and Value as \u0026ldquo;application/json.\u0026rdquo; Under Tests, add the following script to verify the response result: pm.test(\u0026#34;res.status should be 200\u0026#34;, function () { pm.response.to.have.status(200); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(1); pm.expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); Click the Send button to send the request and verify the response result. Confirm that the response result is correct, click the Save button to save the request.\npost-demo In the Request of the post-demo, select the POST request method and enter the URL as https://jsonplaceholder.typicode.com/posts. In Headers, add a request header with Key as Content-Type and Value as application/json. In Body, select raw, select JSON format, and enter the following request body: { \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } Under Tests, add the following script to verify the response result: pm.test(\u0026#34;res.status should be 201\u0026#34;, function () { pm.response.to.have.status(201); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(101); pm.expect(data.title).to.equal(\u0026#39;foo\u0026#39;); }); Confirm that the response result is correct, click the Save button to save the request.\nConfiguring Test Environment in Postman The following steps involve using the host of the API requests as environment variables for demonstration purposes.\nAdding Environment Variables In the top right corner of Postman, click the gear icon, select Manage Environments, click the Add button, enter the environment name as \u0026ldquo;DemoEnv,\u0026rdquo; and click the Add button to create an environment named \u0026ldquo;DemoEnv.\u0026rdquo; Edit the environment variables, add a key named \u0026ldquo;host\u0026rdquo; with a value of https://jsonplaceholder.typicode.com. Click the Add button to save the environment variables. Updating Requests In the \u0026ldquo;get-demo\u0026rdquo; request, update the URL to {{host}}/posts/1. In the \u0026ldquo;post-demo\u0026rdquo; request, update the URL to {{host}}/posts. Verifying Environment Variables In the top right corner of Postman, click the gear icon, select DemoEnv to switch to the \u0026ldquo;DemoEnv\u0026rdquo; environment. Select the \u0026ldquo;get-demo\u0026rdquo; request, click the Send button to send the request, and verify the response result. After confirming the correctness of the response, click the Save button to save the request. Select the \u0026ldquo;post-demo\u0026rdquo; request, click the Send button to send the request, and verify the response result. After confirming the correctness of the response, click the Save button to save the request. Exporting Environment Variables and Test Case Files In the top right corner of Postman, click the gear icon, select Export, choose DemoEnv, and click the Export button to export the environment variables. Select the demo Collection containing the \u0026ldquo;get-demo\u0026rdquo; and \u0026ldquo;post-demo\u0026rdquo; requests, click the three dots in the top right corner, select Export, choose Collection v2.1, and click the Export button to export the test case file. Adjusting Project File Structure Creating Env and Testcase Folders In the project folder, create a folder named Env to store environment variable files. // Create Env folder mkdir Env In the project folder, create a folder named Testcase to store test case files. // Create Testcase folder mkdir Testcase Organizing Case and Environment Files\nPlace the exported environment variable files and test case files into the Env and Testcase folders within the project folder.\nAdjusting the package.json file In the package.json file, add the following script to run the test cases: \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;newman run Testcase/demo.postman_collection.json -e Env/DemoEnv.postman_environment.json\u0026#34; } Running Test Cases npm run test Reference Postman docs newman docs ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/postman-tutorial-getting-started-and-building-your-own-project-from-0-to-1/","summary":"This guide provides a comprehensive introduction to getting started with Postman API automation testing, covering both the basics and the step-by-step process of building a project from scratch. Learn how to effectively use Postman for API testing, understand the foundational structure of project setup, environment configuration, and writing test cases from the ground up.","title":"Postman API Automation Testing Tutorial: Getting Started and Building a Postman API Automation Test project from 0 to 1"},{"content":"Advanced Usage concurrent testing and distributed testing In the daily process of API automation testing, concurrent execution of test cases is required to improve testing efficiency.\nSometimes it is also necessary to introduce distributed testing in order to run test cases on multiple machines at the same time, which can also better improve testing efficiency.\npytest-xdist is a plugin for Pytest that provides some corresponding functionality, mainly for supporting concurrent and distributed testing.\npytest-xdist Feature Introduction Concurrently run tests:\nUse the -n option: pytest -n NUM allows running tests concurrently, where NUM is the number of concurrent workers. This can speed up test execution, especially on computers with multiple CPU cores. pytest -n 3 # Start 3 concurrent workers to execute the test Distributed testing:\nUse pytest --dist=loadscope: allows tests to be executed on multiple nodes and test runs can be completed faster with distributed testing. pytest --dist=loadscope Use pytest --dist=each: run a set of tests per node, for distributed testing. pytest --dist=each Parameterized tests and Concurrency:\nUse of pytest.mark.run: In conjunction with the pytest.mark.run tag, tests with different tags can optionally be run on different processes or nodes. @pytest.mark.run(processes=2) def test_example(): pass Distributed environment setup:\nUse pytest_configure_node: you can configure the tests before running them on the node. def pytest_configure_node(node): node.slaveinput[\u0026#39;my_option\u0026#39;] = \u0026#39;some value\u0026#39; Use pytest_configure_node: you can configure the tests before running them on the node. def pytest_configure_node(node): node.slaveinput[\u0026#39;my_option\u0026#39;] = \u0026#39;some value\u0026#39; Distributed test environment destruction:\nUse pytest_configure_node: you can clean up after running tests on a node. def pytest_configure_node(node): # Configure the node yield # Perform cleanup after running tests on nodes print(\u0026#34;Cleaning up after test run on node %s\u0026#34; % node.gateway.id) These are some of the features provided by pytest-xdist that can help you perform concurrent and distributed tests more efficiently to speed up test execution and increase efficiency. Be sure to consult the pytest-xdist documentation for more detailed information and usage examples before using it.\nInstalling pytest-xdist dependency pip install pytest-xdist Example of running a test case concurrently Execute test cases concurrently with 3 workers Run the following commands to see how long the test cases take to execute\nConcurrent Execution pytest -n 3 Default Parallel Execution pytest Parallel execution took 9.81s while Concurrent execution took 1.63s, you can see that concurrent execution of test cases can greatly improve the Parallel of testing.\nconcurrently executes the test cases with 3 workers, and each worker prints the progress of the test cases pytest -n 3 -v The progress of the test is printed in the test results, which provides a better understanding of the execution of the test cases.\nDistributed testing example Distributed test where each node runs a set of tests pytest --dist=each Distributed testing allows for faster test runs.\nDistributed testing, where each node runs a set of tests and each worker prints the progress of the test cases pytest --dist=each -v The progress of the test will be printed in the test results, so you can better understand the execution of the test cases.\nDistributed testing, each node runs a set of tests, and each worker prints the progress of the test cases, as well as the output of the test logs pytest --dist=each -v --capture=no The output of the test log is printed in the test results, which gives a better understanding of the execution of the test cases.\nFiltering test case execution In the daily API testing process, we need to selectively execute test cases according to the actual situation in order to improve the testing efficiency.\nGenerally, when we use allure test reports, we can use the Allure tag feature to filter the use cases corresponding to the tag to execute the test, but the Pytest framework does not directly support running tests based on Allure tags. However, the Pytest framework does not directly support running tests based on Allure tags, so you can use Pytest markers to accomplish this.\nPytest provides a marks tagging feature that can be used to tag different types of test cases and then filter them for execution.\nThe general process is that you can mark tests with custom markers (e.g. Regression/Smoke) and then use pytest\u0026rsquo;s -m option to run only those tests.\nDefining Pytest Markers Edit the pytest.ini file and add the following: customize the type of markers\nRegression: Marks the use case for regression testing. Smoke: mark it as a use case for smoke testing markers = Regression: marks tests as Regression Smoke: marks tests as Smoke Marking Test Cases The operation steps are:\nIntroduce pytest Mark the test case with @pytest.mark. To differentiate, create a new test case file named test_demo_filter.py.\nimport pytest import requests import json class TestPytestMultiEnvDemo: @pytest.mark.Regression # mark the test case as regression def test_get_demo_filter(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] get_api = env_config[\u0026#34;getAPI\u0026#34;] get_api_response_data = env_response_data[\u0026#34;getAPI\u0026#34;] # send request response = requests.get(host+get_api) # assert assert response.status_code == 200 assert response.json() == get_api_response_data @pytest.mark.Smoke # mark the test case as smoke def test_post_demo_filter(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] post_api = env_config[\u0026#34;postAPI\u0026#34;] post_api_request_data = env_request_data[\u0026#34;postAPI\u0026#34;] print(\u0026#34;make the request\u0026#34;) post_api_response_data = env_response_data[\u0026#34;postAPI\u0026#34;] # Your test code here response = requests.post(host + post_api, json=post_api_request_data) print(\u0026#34;verify the response status code\u0026#34;) assert response.status_code == 201 print(\u0026#34;verify the response data\u0026#34;) assert response.json() == post_api_response_data Filtering Test Case Execution Running Regression-tagged test cases pytest -m Regression This command tells pytest to run only the tests labeled Regression.\nRunning Smoke-tagged test cases pytest -m Smoke This command tells pytest to run only the tests labeled Smoke.\nreference pytest-xdist docs:https://pytest-xdist.readthedocs.io/en/stable/ pytest makers docs:https://docs.pytest.org/en/6.2.x/example/markers.html pytest docs:https://docs.pytest.org/en/6.2.x/ ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/pytest-tutorial-advance-usage-filter-testcase-and-concurrent-testing-distributed-testing/","summary":"Focus on test case screening, concurrency testing and distributed testing. Learn how to execute test cases in a targeted manner to improve testing efficiency. Explore Pytest concurrent testing features and learn how to execute multiple test cases at the same time to reduce testing time.","title":"Pytest API Automation Testing Tutorial Advance Usage Filtering test case execution and Concurrent testing"},{"content":"Advanced Usage Multi-environment support In the actual API automation testing process, we need to run test cases in different environments to ensure that the API works properly in each environment.\nBy using Pytest\u0026rsquo;s fixture feature, we can easily support multiple environments.\nRefer to the demo:https://github.com/Automation-Test-Starter/Pytest-API-Test-Demo\nNew test configuration files for different environments Configuration file will be stored in json format for example, other formats such as YAML, CSV, etc. are similar, can refer to the\n// Create a new test configuration folder mkdir config // Go to the test configuration folder cd config // Create a new test configuration file for the development environment touch dev_config.json // Create a new test configuration file for the production environment touch prod_config.json Writing different environment test profiles Writing Development Environment Test Profiles Configure the development environment test profiles according to the actual situation.\n{ \u0026#34;host\u0026#34;: \u0026#34;https://jsonplaceholder.typicode.com\u0026#34;, \u0026#34;getAPI\u0026#34;: \u0026#34;/posts/1\u0026#34;, \u0026#34;postAPI\u0026#34;:\u0026#34;/posts\u0026#34; } Configuring Production Environment Test Profiles Configure production environment test profiles according to the actual situation\n{ \u0026#34;host\u0026#34;: \u0026#34;https://jsonplaceholder.typicode.com\u0026#34;, \u0026#34;getAPI\u0026#34;: \u0026#34;/posts/1\u0026#34;, \u0026#34;postAPI\u0026#34;:\u0026#34;/posts\u0026#34; } New Different Environment Test Data File The different environments request data file and the response data file store the different environments request data and the different environments expected response data for the test cases, respectively.\n// Create a new test data folder mkdir data // Go to the test data folder cd data // Create a new dev request data file touch dev_request_data.json // Create a new dev response data file touch dev_response_data.json // Create a new request data file for the production environment touch prod_request_data.json // Create a new production response data file touch prod_response_data.json Writing test data files for different environments Write the dev environment request data file The dev environment request data file is configured with the request data for the getAPI API and the request data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } } Writing the dev Environment Response Data File The dev environment response data file is configured with the response data for the getAPI API and the response data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 } } Write the prod environment request data file The prod environment request data file is configured with the request data for the getAPI API and the request data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } } Writing the prod Environment Response Data File The prod environment response data file is configured with the response data for the getAPI API and the response data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 } } Configure fixture to support multiple environments The \u0026gt; fixture will be stored in the conftest.py file as an example, other formats such as YAML, CSV, etc. are similar.\nCreate a new conftest.py file in the project root directory. mkdrir conftest.py Writing the conftest.py file import pytest import json import json import os @pytest.fixture(scope=\u0026#34;session\u0026#34;) def env_config(request): # get config file from different env env = os.getenv(\u0026#39;ENV\u0026#39;, \u0026#39;dev\u0026#39;) with open(f\u0026#39;config/{env}_config.json\u0026#39;, \u0026#39;r\u0026#39;) as config_file: config = json.load(config_file) return config @pytest.fixture(scope=\u0026#34;session\u0026#34;) def env_request_data(request): # get request data file from different env env = os.getenv(\u0026#39;ENV\u0026#39;, \u0026#39;dev\u0026#39;) with open(f\u0026#39;data/{env}_request_data.json\u0026#39;, \u0026#39;r\u0026#39;) as request_data_file: request_data = json.load(request_data_file) return request_data @pytest.fixture (scope=\u0026#34;session\u0026#34;) def env_response_data(request): # get response data file from different env env = os.getenv(\u0026#39;ENV\u0026#39;, \u0026#39;dev\u0026#39;) with open(f\u0026#39;data/{env}_response_data.json\u0026#39;, \u0026#39;r\u0026#39;) as response_data_file: response_data = json.load(response_data_file) return response_data Update test case to support multi environment To make a distinction, here is a new test case file named test_demo_multi_environment.py\nimport requests import json class TestPytestMultiEnvDemo: def test_get_demo_multi_env(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] get_api = env_config[\u0026#34;getAPI\u0026#34;] get_api_response_data = env_response_data[\u0026#34;getAPI\u0026#34;] # send request response = requests.get(host+get_api) # assert assert response.status_code == 200 assert response.json() == get_api_response_data def test_post_demo_multi_env(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] post_api = env_config[\u0026#34;postAPI\u0026#34;] post_api_request_data = env_request_data[\u0026#34;postAPI\u0026#34;] post_api_response_data = env_response_data[\u0026#34;postAPI\u0026#34;] # send request response = requests.post(host + post_api, post_api_request_data) # assert assert response.status_code == 201 assert response.json() == post_api_response_data Run this test case to confirm that multi-environment support is in effect Run the dev environment test case ENV=dev pytest test_case/test_demo_multi_environment.py Run the prod environment test case ENV=prod pytest test_case/test_demo_multi_environment.py Integration with allure reporting allure is a lightweight, flexible, and easily extensible test reporting tool that provides a rich set of report types and features to help you better visualize your test results.\nallure reports can be integrated with Pytest to generate detailed test reports.\nRefer to the demo:https://github.com/Automation-Test-Starter/Pytest-API-Test-Demo\nInstall allure-pytest library pip install allure-pytest To avoid conflicts between the previously installed pytest-html-reporter and the allure-pytest package, it is recommended to uninstall the pytest-html-reporter package first.\npip uninstall pytest-html-reporter Configuration allure-pytest library Update the pytest.ini file to specify where allure reports are stored\n[pytest] # allure addopts = --alluredir ./allure-results Adjusting test cases to support allure reporting To differentiate, create a new test case file here, named test_demo_allure.py\nimport allure import requests @allure.feature(\u0026#34;Test example API\u0026#34;) class TestPytestAllureDemo: @allure.story(\u0026#34;Test example get endpoint\u0026#34;) @allure.title(\u0026#34;Verify the get API\u0026#34;) @allure.description(\u0026#34;verify the get API response status code and data\u0026#34;) @allure.severity(\u0026#34;blocker\u0026#34;) def test_get_example_endpoint_allure(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] get_api = env_config[\u0026#34;getAPI\u0026#34;] get_api_request_data = env_request_data[\u0026#34;getAPI\u0026#34;] get_api_response_data = env_response_data[\u0026#34;getAPI\u0026#34;] # send get request response = requests.get(host + get_api) # assert print(\u0026#34;response status code is\u0026#34; + str(response.status_code)) assert response.status_code == 200 print(\u0026#34;response data is\u0026#34; + str(response.json())) assert response.json() == get_api_response_data @allure.story(\u0026#34;Test example POST API\u0026#34;) @allure.title(\u0026#34;Verify the POST API\u0026#34;) @allure.description(\u0026#34;verify the POST API response status code and data\u0026#34;) @allure.severity(\u0026#34;Critical\u0026#34;) def test_post_example_endpoint_allure(self, env_config, env_request_data, env_response_data): host = env_config[\u0026#34;host\u0026#34;] post_api = env_config[\u0026#34;postAPI\u0026#34;] post_api_request_data = env_request_data[\u0026#34;postAPI\u0026#34;] post_api_response_data = env_response_data[\u0026#34;postAPI\u0026#34;] # send request response = requests.post(host + post_api, json=post_api_request_data) # assert print(\u0026#34;response status code is\u0026#34; + str(response.status_code)) assert response.status_code == 201 print(\u0026#34;response data is\u0026#34; + str(response.json())) assert response.json() == post_api_response_data Run test cases to generate allure reports ENV=dev pytest test_case/test_demo_allure.py View allure report Run the following command to view the allure report in the browser\nallure serve allure-results Adapting CI/CD processes to support allure reporting Github action is an example, other CI tools are similar.\nUpdate the contents of the .github/workflows/pytest.yml file to upload allure reports to GitHub.\nname: Pytest API Testing on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] permissions: contents: read jobs: Pytes-API-Testing: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up Python 3.10 uses: actions/setup-python@v3 with: python-version: \u0026#34;3.10\u0026#34; - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Test with pytest run: | ENV=dev pytest - name: Archive Pytest allure test report uses: actions/upload-artifact@v3 with: name: Pytest-allure-report path: allure-results - name: Upload Pytest allure report to GitHub uses: actions/upload-artifact@v3 with: name: Pytest-allure-report path: allure-results View github action allure report In GitHub, navigate to your repository. Click the Actions tab at the top, and then click the Pytest API Testing workflow on the left. You should see the workflow running, wait for the execution to complete, and then you can view the results.\nReference Pytest docs Allure docs ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/pytest-tutorial-advance-usage-multiple-environment-support-and-integration-allure-report/","summary":"A deep dive into advanced Pytest usage, focusing on how Pytest is support multiple environment and integration allure report.","title":"Pytest API Automation Testing Tutorial Advance Usage Multiple Environment Support and Integration Allure Report"},{"content":"Advanced Usage Common Assertions Using Pytest During the writing of API automation test cases, we need to use various assertions to verify the expected results of the tests.\nPytest provides more assertions and a flexible library of assertions to fulfill various testing needs.\nThe following are some of the commonly used Pytest API automation test assertions:\nEquality assertion: checks whether two values are equal.\nassert actual_value == expected_value Unequality Assertion: checks if two values are not equal.\nassert actual_value != expected_value Containment assertion: checks whether a value is contained in another value, usually used to check whether a string contains a substring.\nassert substring in full_string Membership Assertion: checks whether a value is in a collection, list, or other iterable object.\nassert item in iterable Truth Assertion: checks whether an expression or variable is true.\nassert expression OR\nassert variable False Value Assertion: checks whether an expression or variable is false.\nassert not expression OR\nassert not variable Greater Than, Less Than, Greater Than Equal To, Less Than Equal To Assertion: checks whether a value is greater than, less than, greater than equal to, or less than equal to another value.\nassert value \u0026gt; other_value assert value \u0026lt; other_value assert value \u0026gt;= other_value assert value \u0026lt;= other_value Type Assertion: checks that the type of a value is as expected.\nassert isinstance(value, expected_type) For example, to check if a value is a string:\nassert isinstance(my_string, str) Exception Assertion: checks to see if a specific type of exception has been raised in a block of code.\nwith pytest.raises(ExpectedException): # Block of code that is expected to raise an ExpectedException. Approximate Equality Assertion: checks whether two floating-point numbers are equal within some margin of error.\nassert math.isclose(actual_value, expected_value, rel_tol=1e-9) List Equality Assertion: checks if two lists are equal.\nassert actual_list == expected_list Dictionary Equality Assertion: checks if two dictionaries are equal.\nassert actual_dict == expected_dict Regular Expression Match Assertion: checks if a string matches the given regular expression.\nimport re assert re.match(pattern, string) Null Assertion: checks whether a value is None。\nassert value is None Non-null value assertion: checks if a value is not None。\nassert value is not None Boolean Assertion: checks whether a value of True or False。\nassert boolean_expression Empty Container Assertion: checks if a list, collection or dictionary is empty.\nassert not container # Check if the container is empty Contains Subset Assertion: checks whether a set contains another set as a subset.\nassert subset \u0026lt;= full_set String Beginning or End Assertion: checks whether a string begins or ends with the specified prefix or suffix.\nassert string.startswith(prefix) assert string.endswith(suffix) Quantity Assertion: checks the number of elements in a list, collection, or other iterable object.\nassert len(iterable) == expected_length Range Assertion: checks if a value is within the specified range.\nassert lower_bound \u0026lt;= value \u0026lt;= upper_bound Document Existence Assertion: checking whether a document exists or not。\nimport os assert os.path.exists(file_path) These are some common Pytest assertions, but depending on your specific testing needs, you may want to use other assertions or combine multiple assertions to more fully validate your test results. Detailed documentation on assertions can be found on the official Pytest website at:Pytest - Built-in fixtures, marks, and nodes\nData-driven In the process of API automation testing. The use of data-driven is a regular testing methodology where the input data and expected output data of the test cases are stored in data files, and the testing framework executes multiple tests based on these data files to validate various aspects of the API.\nThe test data can be easily modified without modifying the test case code.\nData-driven testing helps you cover multiple scenarios efficiently and ensures that the API works properly with a variety of input data.\nRefer to the demo:https://github.com/Automation-Test-Starter/Pytest-API-Test-Demo\nCreate the test configuration file Configuration file will be stored in json format for example, other formats such as YAML, CSV, etc. are similar, can be referred to.\n// create a new config folder mkdir config // enter the config folder cd config // create a new configuration file touch config.json Writing Test Configuration Files The configuration file stores the configuration information of the test environment, such as the URL of the test environment, database connection information, and so on.\nThe contents of the test configuration file in the demo are as follows:\nConfigure host information Configure the getAPI API information. Configure the postAPI API information. { \u0026#34;host\u0026#34;: \u0026#34;https://jsonplaceholder.typicode.com\u0026#34;, \u0026#34;getAPI\u0026#34;: \u0026#34;/posts/1\u0026#34;, \u0026#34;postAPI\u0026#34;:\u0026#34;/posts\u0026#34; } Create the test data file The request data file and the response data file store the request data and the expected response data of the test case, respectively.\n// create a new data folder mkdir data // enter the data folder cd data // create a new request data file touch request_data.json // create a new response data file touch response_data.json Writing test data files Writing the request data file The request data file is configured with the request data for the getAPI API and the request data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } } Writing the response data file The request data file is configured with the response data for the getAPI API and the response data for the postAPI API.\n{ \u0026#34;getAPI\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, \u0026#34;postAPI\u0026#34;:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 } } Updating test cases to support data driving To differentiate, here is a new test case file named test_demo_data_driving.py\nimport requests import json # get the test configuration information from the configuration file with open(\u0026#34;config/config.json\u0026#34;, \u0026#34;r\u0026#34;) as json_file: config = json.load(json_file) # get the request data from the test data file with open(\u0026#39;data/request_data.json\u0026#39;, \u0026#39;r\u0026#39;) as json_file: request_data = json.load(json_file) # get the response data from the test data file with open(\u0026#39;data/response_data.json\u0026#39;, \u0026#39;r\u0026#39;) as json_file: response_data = json.load(json_file) class TestPytestDemo: def test_get_demo(self): host = config.get(\u0026#34;host\u0026#34;) get_api = config.get(\u0026#34;getAPI\u0026#34;) get_api_response_data = response_data.get(\u0026#34;getAPI\u0026#34;) # send request response = requests.get(host+get_api) # assert assert response.status_code == 200 assert response.json() == get_api_response_data def test_post_demo(self): host = config.get(\u0026#34;host\u0026#34;) post_api = config.get(\u0026#34;postAPI\u0026#34;) post_api_request_data = request_data.get(\u0026#34;postAPI\u0026#34;) post_api_response_data = response_data.get(\u0026#34;postAPI\u0026#34;) # send request response = requests.post(host + post_api, post_api_request_data) # assert assert response.status_code == 201 assert response.json() == post_api_response_data Run the test case to confirm the data driver is working If you run the data driver support test case with demo project: test_demo_data_driving.py, it is recommended to block other test cases first, otherwise it may report errors.\npytest tests/test_demo_data_driving.py Reference Pytest docs ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/pytest-tutorial-advance-usage-common-assertions-and-data-driven/","summary":"A deep dive into advanced Pytest usage, focusing on how Pytest is commonly asserted and data-driven.","title":"Pytest API Automation Testing Tutorial Advance Usage Common Assertions and Data Driven"},{"content":"Advanced Usage CI/CD integration Integration github action Use github action as an example, and other CI tools similarly\nSee the demo at https://github.com/Automation-Test-Starter/Pytest-API-Test-Demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as pytest.yml, in the .github/workflows directory.\nEdit the pytest.yml file: Copy the following into the file\n# This workflow will install Python dependencies, run tests and lint with a single version of Python # For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python name: Pytest API Testing on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] permissions: contents: read jobs: Pytes-API-Testing: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up Python 3.10 uses: actions/setup-python@v3 with: python-version: \u0026#34;3.10\u0026#34; - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Test with pytest run: | pytest - name: Archive Pytest test report uses: actions/upload-artifact@v3 with: name: SuperTest-test-report path: report - name: Upload Pytest report to GitHub uses: actions/upload-artifact@v3 with: name: Pytest-test-report path: report Commit the code: Add the pytest.yml file to your repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Pytest API Testing workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. reference Pytest official document: https://docs.pytest.org/en/6.2.x/contents.html gitHub action official document: https://docs.github.com/en/actions ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/pytest-tutorial-advance-usage-integration-ci-cd-and-github-action/","summary":"dive into advanced usage of Pytest, focusing on how to integrate Pytest into a CI/CD process and how to automate tests using GitHub Actions.","title":"Pytest API Automation Testing Tutorial Advance Usage Integration CI CD and Github Action"},{"content":"Build a Pytest API Automation Test Project from 0 to 1 1. Create a project directory mkdir Pytest-API-Testing-Demo 2.Project initialization // Go to the project folder cd Pytest-API-Testing-Demo // Create the project python project virtual environment python -m venv .env // Enable the project python project virtual environment source .env/bin/activate 3.Install project dependencies // Install the requests package pip install requests // Install the pytest package pip install pytest // Save the project dependencies to the requirements.txt file. pip freeze \u0026gt; requirements.txt 4. Create new test files and test cases // Create a new tests folder mkdir tests // Create a new test case file cd tests touch test_demo.py 5. Writing Test Cases The test API can be referred to the demoAPI.md file in the project.\nimport requests class TestPytestDemo: def test_get_demo(self): base_url = \u0026#34;https://jsonplaceholder.typicode.com\u0026#34; # SEND REQUEST response = requests.get(f\u0026#34;{base_url}/posts/1\u0026#34;) # ASSERT assert response.status_code == 200 assert response.json()[\u0026#39;userId\u0026#39;] == 1 assert response.json()[\u0026#39;id\u0026#39;] == 1 def test_post_demo(self): base_url = \u0026#34;https://jsonplaceholder.typicode.com\u0026#34; requests_data = { \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } # SEND REQUEST response = requests.post(f\u0026#34;{base_url}/posts\u0026#34;, requests_data) # ASSERT assert response.status_code == 201 print(response.json()) assert response.json()[\u0026#39;userId\u0026#39;] == \u0026#39;1\u0026#39; assert response.json()[\u0026#39;id\u0026#39;] == 101 6.Run test cases pytest 7.View test report 8.Integration pytest-html-reporter test report https://github.com/prashanth-sams/pytest-html-reporter\n8.1 Install pytest-html-reporter dependency pip install pytest-html-reporter 8.2 Configuring Test Report Parameters Create a new pytest.ini file in the project root directory. Add the following [pytest] addopts = -vs -rf --html-report=./report --title=\u0026#39;PYTEST REPORT\u0026#39; --self-contained-html 8.3 Run test cases pytest 8.4 Viewing the test report The report is located in the report directory in the project root directory, use your browser to open the pytest_html_report.html file to view it.\nreference pytest: https://docs.pytest.org/en/latest/ ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/pytest-tutorial-building-your-own-project-from-0-to-1/","summary":"dive into how to build a Pytest API automation testing project from scratch.Pytest is a popular Java library for performing REST API testing, providing powerful tools that make it easy to write automated test scripts to validate the API\u0026rsquo;sbehavior.","title":"Pytest API Automation Testing Tutorial: Building a Pytest API Automation Test project from 0 to 1"},{"content":"Introduction Introducing Pytest Pytest is a popular Python testing framework for writing, organizing, and running various types of automated tests. It provides a rich set of features that make it easy to write and manage test cases, as well as generate detailed test reports. Here are some of the key features and benefits of Pytest:\nSimple and easy to use Pytest is designed to make writing test cases simple and easy to understand. You can write test assertions using Python\u0026rsquo;s standard assert statement without having to learn a new assertion syntax.\nAutomatic Discovery of Test Cases Pytest can automatically discover and run test cases in your project without explicitly configuring the test suite. Test case files can be named test_*.py or *_test.py, or use a specific test function naming convention.\nRich plugin ecosystem Pytest can be extended with plugins. There are many third-party plug-ins available to meet different testing needs, such as Allure reporting, parameterization, coverage analysis, and so on.\nParameterized Testing Pytest supports parameterized testing, which allows you to run the same test case multiple times, but with different parameters. This reduces code duplication and improves test coverage.\nException and fault localization Pytest provides detailed error and exception information that helps you locate and resolve problems more easily. It also provides detailed traceback information.\nParallel Test Execution Pytest supports parallel execution of test cases, which increases the speed of test execution, especially in large projects.\nMultiple Report Formats Pytest supports multiple test report formats, including terminal output, JUnit XML, HTML reports and Allure reports. These reports can help you visualize test results.\nCommand Line Options Pytest provides a rich set of command line options to customize the behavior of test runs, including filtering, retrying, coverage analysis, and more.\nIntegration Pytest can be easily integrated with other testing frameworks and tools (e.g. Selenium, Django, Flask, etc.) as well as continuous integration systems (e.g. Jenkins, Travis CI, etc.).\nActive Community Pytest has an active community with extensive documentation and tutorials for learning and reference. You can also get support and solve problems in the community.\nIn short, Pytest is a powerful and flexible testing framework for projects of all sizes and types. Its ease of use, automation capabilities, and rich set of plugins make it one of the go-to tools in Python testing.\nOfficial website: https://docs.pytest.org/en/latest/\nIntroduction to python virtual environments A Python virtual environment is a mechanism for creating and managing multiple isolated development environments within a single Python installation. Virtual environments help resolve dependency conflicts between different projects by ensuring that each project can use its own independent Python packages and libraries without interfering with each other. Here are the steps on how to create and use a Python virtual environment:\nInstall the Virtual Environment Tool Before you begin, make sure you have installed Python\u0026rsquo;s virtual environment tools. In Python 3.3 and later, the venv module is built-in and can be used to create virtual environments. If you\u0026rsquo;re using an older version of Python, you can install the virtualenv tool.\nFor Python 3.3+, the venv tool is built-in and does not require additional installation.\nFor Python 2.x, you can install the virtualenv tool with the following command:\npip install virtualenv Creating a virtual environment Open a terminal, move to the directory where you wish to create the virtual environment, and run the following command to create the virtual environment:\nUse venv (for Python 3.3+):\npython -m venv myenv Use virtualenv (for Python 2.x):\nvirtualenv myenv In the above command, myenv is the name of the virtual environment and you can customize the name.\nActivate virtual environment To start using the virtual environment, you need to activate it. The activation command is slightly different for different operating systems:\non macOS and Linux: source myenv/bin/activate On Windows (using Command Prompt): myenv\\Scripts\\activate On Windows (using PowerShell): .\\myenv\\Scripts\\Activate.ps1 Once the virtual environment is activated, you will see the name of the virtual environment in front of the terminal prompt, indicating that you are in the virtual environment.\nInstalling dependencies in a virtual environment In a virtual environment, you can use pip to install any Python packages and libraries required by your project, and these dependencies will be associated with that virtual environment. Example:\npip install requests Using a virtual environment When working in a virtual environment, you can run Python scripts and use packages installed in the virtual environment. This ensures that your project runs in a separate environment and does not conflict with the global Python installation.\nExiting the virtual environment To exit the virtual environment, simply run the following command in a terminal:\ndeactivate This returns you to the global Python environment.\nBy using a virtual environment, you can maintain clean dependencies between projects and ensure project stability and isolation. This is a good practice in Python development.\nProject dependencies The following environments need to be installed in advance\npython, demo version is v3.11.6 Just install python 3.x or higher.\nProject directory structure The following is an example of the directory structure of a Pytest API automation test project:\nSubsequent demo projects will introduce allure reports, so there will be an additional allure-report directory.\nPytest-allure-demo/ ├── tests/ # test case files │ ├── test_login.py # Example test case file │ ├── test_order.py # Example test case file │ └── ... ├── data/ # test data files (e.g. JSON, CSV, etc.) │ ├── dev_test_data.json # Test data file for development environment. │ ├── prod_test_data.json # Test data file for prod environment. │ ├── ... ├── config/ │ ├── dev_config.json # Development environment configuration file │ ├── prod_config.json # Production environment configuration file │ ├── ... ├── conftest.py # Pytest\u0026#39;s global configuration file ├── pytest.ini # Pytest configuration file ├── requirements.txt # Project dependencies file └── allure-report/ # Allure reports reference pytest: https://docs.pytest.org/en/latest/ ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/pytest-tutorial-getting-started-and-own-environment-preparation/","summary":"a tutorial on Pytest, focusing on getting started and preparing the environment to be built.","title":"Pytest API Automation Testing Tutorial: Getting Started and Setting Up Your Environment"},{"content":"Multiple Environment Support When using Jest or Mocha for API testing, you may need to support testing different environments, such as development, test and production environments. This can be achieved by configuring different test scripts and environment variables.\nThe following is a brief description of how to configure multi-environment support in Jest and Mocha, with a demo demonstrating support for two environments.\nMocha version can be found in the demo project: https://github.com/Automation-Test-Starter/SuperTest-Mocha-demo.\nThe Jest version can be found in the demo project: https://github.com/Automation-Test-Starter/SuperTest-Jest-demo.\nThe mocha version is similar to the Jest version, so here is an example of the mocha version.\nCreate Multi-Environment Test Configuration File // create test configuration folder, if already exists, skip this step mkdir Config // create test configuration file for test environment cd Config touch testConfig-test.js // create test configuration file for dev environment touch testConfig-dev.js Edit Multi-Environment Test Configuration File edit test configuration file for test environment: testConfig-test.js based on actual situation, edit test configuration file for test environment\n// Test config file for test environment module.exports = { host: \u0026#39;https://jsonplaceholder.typicode.com\u0026#39;, // Test endpoint getAPI: \u0026#39;/posts/1\u0026#39;, // Test GET API URL postAPI: \u0026#39;/posts\u0026#39;, // Test POST API URL }; edit test configuration file for dev environment: testConfig-dev.js based on actual situation, edit test configuration file for dev environment\n// Test config file for dev environment module.exports = { host: \u0026#39;https://jsonplaceholder.typicode.com\u0026#39;, // Test endpoint getAPI: \u0026#39;/posts/1\u0026#39;, // Test GET API URL postAPI: \u0026#39;/posts\u0026#39;, // Test POST API URL }; Create Multi-Environment Test Data File // create test data folder, if already exists, skip this step mkdir testData // enter test data folder cd testData // create request data file for test environment touch requestData-test.js // create response data file for test environment touch responseData-test.js // create request data file for dev environment touch requestData-dev.js // create response data file for dev environment touch responseData-dev.js Edit Multi-Environment Test Data File edit request data file for test environment: requestData-test.js based on actual situation, edit request data file for test environment\n// Test request data file for test environment module.exports = { getAPI: \u0026#39;\u0026#39;, // request data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }, // request data for POST API }; edit response data file for test environment: responseData-test.js based on actual situation, edit response data file for test environment\n// Test response data file for test environment module.exports = { getAPI: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, // response data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 }, // response data for POST API }; edit request data file for dev environment: requestData-dev.js based on actual situation, edit request data file for dev environment\n// Test request data file for dev environment module.exports = { getAPI: \u0026#39;\u0026#39;, // request data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }, // request data for POST API }; edit response data file for dev environment: responseData-dev.js based on actual situation, edit response data file for dev environment\n// Test response data file for dev environment module.exports = { getAPI: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, // response data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 }, // response data for POST API }; Update test cases to support multiple environments To differentiate, here is a new test case file named multiEnvTest.spec.js\n// Test: multiEnvTest.spec.js const request = require(\u0026#39;supertest\u0026#39;); // import supertest require(\u0026#39;chai\u0026#39;); // import chai const expect = require(\u0026#39;chai\u0026#39;).expect; // import expect const config = process.env.NODE_ENV === \u0026#39;test\u0026#39; ? require(\u0026#39;../Config/testConfig-test\u0026#39;) : require(\u0026#39;../Config/testConfig-dev\u0026#39;); // import test config const requestData = process.env.NODE_ENV === \u0026#39;test\u0026#39; ? require(\u0026#39;../TestData/requestData-test\u0026#39;) : require(\u0026#39;../TestData/requestData-dev\u0026#39;); // import request data const responseData= process.env.NODE_ENV === \u0026#39;test\u0026#39; ? require(\u0026#39;../TestData/responseData-test\u0026#39;) : require(\u0026#39;../TestData/responseData-dev\u0026#39;); // import response data // Test Suite describe(\u0026#39;multiEnv-Verify that the Get and POST API returns correctly\u0026#39;, function(){ // Test case 1 it(\u0026#39;multiEnv-Verify that the GET API returns correctly\u0026#39;, function(done){ request(config.host) // Test endpoint .get(config.getAPI) // API endpoint .expect(200) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(responseData.getAPI.id) expect(res.body.userId).to.equal(responseData.getAPI.userId) expect(res.body.title).to.equal(responseData.getAPI.title) expect(res.body.body).to.equal(responseData.getAPI.body) }) // expected response body .end(done) // end the test case }); // Test case 2 it(\u0026#39;multiEnv-Verify that the POST API returns correctly\u0026#39;, function(done){ request(config.host) // Test endpoint .post(config.postAPI) // API endpoint .send(requestData.postAPI) // request body .expect(201) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(responseData.postAPI.id ) expect(res.body.userId).to.equal(responseData.postAPI.userId ) expect(res.body.title).to.equal(responseData.postAPI.title ) expect(res.body.body).to.equal(responseData.postAPI.body ) }) // expected response body .end(done) // end the test case }); }); Update test scripts to support multiple environments \u0026lsquo;\u0026lsquo;\u0026lsquo;json // package.json \u0026ldquo;scripts\u0026rdquo;: { \u0026ldquo;test\u0026rdquo;: \u0026ldquo;NODE_ENV=test mocha\u0026rdquo; // run test script for test environment \u0026ldquo;dev\u0026rdquo;: \u0026ldquo;NODE_ENV=dev mocha\u0026rdquo; // run test script for dev environment }, \u0026rsquo;\u0026rsquo;\u0026rsquo;\nRun the test case to check if the multi environment support is working. If you use demo project to run multi-environment support test case: multiEnvTest.spec.js, it is recommended to block dataDrivingTest.spec.js and test.spec.js test cases first, otherwise it will report an error.\nRun the test environment test script npm run test Run the dev environment test script npm run dev ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/supertest-tutorial-advance-usage-multiple-environment-support/","summary":"focuses on advanced usage of SuperTest with an emphasis on multi-environment support. You will learn how to configure and manage multiple test environments for different stages of development and deployment.","title":"SuperTest API Automation Testing Tutorial Advance Usage - Multiple Environment Support"},{"content":"Data Driven Data-driven for API testing is a testing methodology in which the input data and expected output data for test cases are stored in data files, and the testing framework executes multiple tests against these data files to validate various aspects of the API. Data-driven testing can help you effectively cover multiple scenarios and ensure that the API works properly with a variety of input data.\nThe Mocha version can be found in the demo project: https://github.com/Automation-Test-Starter/SuperTest-Mocha-demo.\nThe Jest version can be found in the demo project: https://github.com/Automation-Test-Starter/SuperTest-Jest-demo.\nThe mocha version is similar to the Jest version, so here is an example of the mocha version.\nCreate test configuration files // create test configuration folder mkdir Config // create test configuration file cd Config touch config.js Edit test configuration files // Test config file module.exports = { host: \u0026#39;https://jsonplaceholder.typicode.com\u0026#39;, // Test endpoint getAPI: \u0026#39;/posts/1\u0026#39;, // Test GET API URL postAPI: \u0026#39;/posts\u0026#39;, // Test POST API URL }; Create test data files // create test data folder mkdir testData // enter test data folder cd testData // create request data file touch requestData.js // create response data file touch responseData.js Edit test data files Edit request data files // Test request data file module.exports = { getAPI: \u0026#39;\u0026#39;, // request data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }, // request data for POST API }; Edit response data files // Test response data file module.exports = { getAPI: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; }, // response data for GET API postAPI:{ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 }, // response data for POST API }; Update test cases to support data-driven To differentiate, create a new test case file named dataDrivingTest.spec.js.\n// Test: dataDrivingTest.spec.js const request = require(\u0026#39;supertest\u0026#39;); // import supertest require(\u0026#39;chai\u0026#39;); // import chai const expect = require(\u0026#39;chai\u0026#39;).expect; // import expect const config = require(\u0026#39;../Config/testConfig\u0026#39;); // import test config const requestData = require(\u0026#39;../TestData/requestData\u0026#39;); // import request data const responseData = require(\u0026#39;../TestData/responseData\u0026#39;); // import response data // Test Suite describe(\u0026#39;Data Driving-Verify that the Get and POST API returns correctly\u0026#39;, function(){ // Test case 1 it(\u0026#39;Data Driving-Verify that the GET API returns correctly\u0026#39;, function(done){ request(config.host) // Test endpoint .get(config.getAPI) // API endpoint .expect(200) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(responseData.getAPI.id) expect(res.body.userId).to.equal(responseData.getAPI.userId) expect(res.body.title).to.equal(responseData.getAPI.title) expect(res.body.body).to.equal(responseData.getAPI.body) }) // expected response body .end(done) // end the test case }); // Test case 2 it(\u0026#39;Data Driving-Verify that the POST API returns correctly\u0026#39;, function(done){ request(config.host) // Test endpoint .post(config.postAPI) // API endpoint .send(requestData.postAPI) // request body .expect(201) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(responseData.postAPI.id ) expect(res.body.userId).to.equal(responseData.postAPI.userId ) expect(res.body.title).to.equal(responseData.postAPI.title ) expect(res.body.body).to.equal(responseData.postAPI.body ) }) // expected response body .end(done) // end the test case }); }); Run the test case to check whether the data driver is effective. If you run the data driver support test case: dataDrivingTest.spec.js with the demo project, it is recommended to skip the test.spec.js test case first, otherwise it will report an error.\n","permalink":"https://naodeng.com.cn/posts/api-automation-testing/supertest-tutorial-advance-usage-data-driven/","summary":"advanced usage of Supertest, focusing on data-driven testing. You will learn how to extend and optimize your Supertest test suite with data parameterization to improve test coverage.","title":"SuperTest API Automation Testing Tutorial Advance Usage - Data Driven"},{"content":"Common Assertions The following is an overview of common assertions used by SuperTest, CHAI and Jest.\nSuperTest\u0026rsquo;s built-in assertions Supertest is a more advanced library built on SuperAgent, so Supertest can easily use SuperAgent\u0026rsquo;s HTTP assertions.\nExamples are as follows:\n.expect(status[, fn]) // Assert response status code. .expect(status, body[, fn]) // Assert response status code and body. .expect(body[, fn]) // Assert response body text with a string, regular expression, or parsed body object. .expect(field, value[, fn]) // Assert header field value with a string or regular expression. .expect(function(res) {}) // Pass a custom assertion function. It\u0026#39;ll be given the response object to check. If the check fails, throw an error. Common Assertions for CHAI Equality Assertions expect(actual).to.equal(expected) // Verify that the actual value is equal to the expected value. expect(actual).to.deep.equal(expected) // Verify that the actual value is deeply equal to the expected value, for object and array comparisons. expect(actual).to.eql(expected) // Same as deep.equal for deep-equal comparisons. Inclusion Assertions expect(array).to.include(value) // Verify that the array contains the specified value. expect(string).to.include(substring) // Verify that the string contains the specified substring. expect(object).to.include(key) // Verify that the object contains the specified key. Type Assertions expect(actual).to.be.a(type) // Verify that the type of the actual value is equal to the specified type. expect(actual).to.be.an(type) // Same as to.be.a for type assertions. expect(actual).to.be.an.instanceof(constructor) // Verify that the actual value is an instance of the specified constructor. Truthiness Assertions expect(value).to.be.true // Verify that the value is true. expect(value).to.be.false // Verify that the value is false. expect(value).to.exist // Verify that the value exists, is not null and is not undefined. Length Assertions expect(array).to.have.length(length) // Verify that the length of the array is equal to the specified length. expect(string).to.have.lengthOf(length) // Verify that the length of the string is equal to the specified length. Empty Assertions expect(array).to.be.empty // Verify if the array is empty. expect(string).to.be.empty // Verify that the string is empty. Range Assertions expect(value).to.be.within(min, max) // Verify that the value is within the specified range. expect(value).to.be.above(min) // Verify that the value is greater than the specified value. expect(value).to.be.below(max) // Verify that the value is less than the specified value. Exception Assertions expect(fn).to.throw(error) // Verify that the function throws an exception of the specified type. expect(fn).to.throw(message) // Verify that the function throws an exception containing the specified message. Existence Assertions expect(object).to.have.property(key) // Verify that the object contains the specified property. expect(array).to.have.members(subset) // Verify that the array contains the specified members. For more chai assertions, see https://www.chaijs.com/api/assert/\nCommon Assertions for Jest Equality Assertions expect(actual).toBe(expected) // Verify that the actual value is strictly equal to the expected value. expect(actual).toEqual(expected) // Verify that the actual value is deeply equal to the expected value, for object and array comparisons. Inequality Assertions expect(actual).not.toBe(expected) // Verify that the actual value is not equal to the expected value. Inclusion Assertions expect(array).toContain(value) // Verify that the array contains the specified value. Type Assertions expect(actual).toBeTypeOf(expected) // Verify that the type of the actual value is equal to the specified type. Truthiness Assertions expect(value).toBeTruthy() // Verify that the value is true. expect(value).toBeFalsy() // Verify that the value is false. Asynchronous Assertions await expect(promise).resolves.toBe(expected) // Verify that the asynchronous operation completed successfully and return a result matching the expected value. Exception Assertions expect(fn).toThrow(error) // Verify that the function throws an exception of the specified type. expect(fn).toThrow(message) // Verify that the function throws an exception containing the specified message. Scope Assertions expect(value).toBeGreaterThanOrEqual(min) // Verify that the value is greater than or equal to the specified minimum. expect(value).toBeLessThanOrEqual(max) // Verify that the value is less than or equal to the specified maximum. Object Property Assertions expect(object).toHaveProperty(key, value) // Verify that the object contains the specified property and that the value of the property is equal to the specified value. For more Jest assertions, seehttps://jestjs.io/docs/expect\n","permalink":"https://naodeng.com.cn/posts/api-automation-testing/supertest-tutorial-advance-usage-common-assertions/","summary":"This blog focuses on advanced usage of Supertest, with a particular focus on commonly used assertions.","title":"SuperTest API Automation Testing Tutorial: Advanced Usage - Common Assertions"},{"content":"CI/CD integration Integration github action Use github action as an example, and other CI tools similarly\nThe mocha version integration github action See the demo at https://github.com/Automation-Test-Starter/SuperTest-Mocha-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as mocha.yml, in the .github/workflows directory.\nEdit the mocha.yml file: Copy the following into the file\nname: RUN SuperTest API Test CI on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: RUN-SuperTest-API-Test: runs-on: ubuntu-latest strategy: matrix: node-version: [ 18.x] # See supported Node.js release schedule at https://nodejs.org/en/about/releases/ steps: - uses: actions/checkout@v3 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v3 with: node-version: ${{ matrix.node-version }} cache: \u0026#39;npm\u0026#39; - name: Installation of related packages run: npm ci - name: RUN SuperTest API Testing run: npm test - name: Archive SuperTest mochawesome test report uses: actions/upload-artifact@v3 with: name: SuperTest-mochawesome-test-report path: Report - name: Upload SuperTest mochawesome report to GitHub uses: actions/upload-artifact@v3 with: name: SuperTest-mochawesome-test-report path: Report Commit the code: Add the mocha.yml file to your repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the RUN SuperTest API Test CI workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. The jest version integration github action See the demo at https://github.com/Automation-Test-Starter/SuperTest-Jest-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as jest.yml, in the .github/workflows directory.\nEdit the jest.yml file: Copy the following into the file\nname: RUN SuperTest API Test CI on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: RUN-SuperTest-API-Test: runs-on: ubuntu-latest strategy: matrix: node-version: [ 18.x] # See supported Node.js release schedule at https://nodejs.org/en/about/releases/ steps: - uses: actions/checkout@v3 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v3 with: node-version: ${{ matrix.node-version }} cache: \u0026#39;npm\u0026#39; - name: Installation of related packages run: npm ci - name: RUN SuperTest API Testing run: npm test - name: Archive SuperTest test report uses: actions/upload-artifact@v3 with: name: SuperTest-test-report path: Report - name: Upload SuperTest report to GitHub uses: actions/upload-artifact@v3 with: name: SuperTest-test-report path: Report Commit the code: Add the jest.yml file to the repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the RUN-SuperTest-API-Test workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/supertest-tutorial-advance-usage-integration-ci-cd-and-github-action/","summary":"dive into advanced usage of Supertest, focusing on how to integrate Supertest into a CI/CD process and how to automate tests using GitHub Actions.","title":"SuperTest API Automation Testing Tutorial: Advanced Usage - Integration CI CD and Github action"},{"content":"Build a SuperTest API automation test project from 0 to 1 The following is a demo of building a SuperTest API automation test project from 0 to 1, using either Jest or Mocha as the test framework.\nMocha version You can refer to the demo project at https://github.com/Automation-Test-Starter/SuperTest-Mocha-demo.\nCreate a new project folder mkdir SuperTest-Mocha-demo Project Initialization // enter the project folder cd SuperTest-Mocha-demo // nodejs project initialization npm init -y Install dependencies // install supertest library npm install supertest --save-dev // install mocha test framework npm install mocha --save-dev // install chai assertion library npm install chai --save-dev Create new test folder and test cases // create test folder mkdir Specs // create test case file cd Specs touch test.spec.js Writing Test Cases The test API can be found in the demoAPI.md file in the project.\n// Test: test.spec.js const request = require(\u0026#39;supertest\u0026#39;); // import supertest const chai = require(\u0026#39;chai\u0026#39;); // import chai const expect = require(\u0026#39;chai\u0026#39;).expect; // import expect // Test Suite describe(\u0026#39;Verify that the Get and POST API returns correctly\u0026#39;, function(){ // Test case 1 it(\u0026#39;Verify that the GET API returns correctly\u0026#39;, function(done){ request(\u0026#39;https://jsonplaceholder.typicode.com\u0026#39;) // Test endpoint .get(\u0026#39;/posts/1\u0026#39;) // API endpoint .expect(200) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(1 ) expect(res.body.userId).to.equal(1) expect(res.body.title) .to.equal(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;) expect(res.body.body) .to.equal(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;) }) // expected response body .end(done) // end the test case }); // Test case 2 it(\u0026#39;Verify that the POST API returns correctly\u0026#39;, function(done){ request(\u0026#39;https://jsonplaceholder.typicode.com\u0026#39;) // Test endpoint .post(\u0026#39;/posts\u0026#39;) // API endpoint .send({ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }) // request body .expect(201) // expected response status code .expect(function (res) { expect(res.body.id).to.equal(101 ) expect(res.body.userId).to.equal(1) expect(res.body.title).to.equal(\u0026#34;foo\u0026#34;) expect(res.body.body).to.equal(\u0026#34;bar\u0026#34;) }) // expected response body .end(done) // end the test case }); }); Configuring mocha config files Create a new mocha configuration file // create configuration file in the project root directory touch .mocharc.js Updating configuration files // mocha config module.exports = { timeout: 5000, // set the default timeout for test cases (milliseconds) spec: [\u0026#39;Specs/**/*.js\u0026#39;], // specify the location of the test file }; Updating test scripts for mocha Add test scripts to the package.json file\n\u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;mocha\u0026#34; }, Running test cases // run test cases npm run test Test Report Terminal Test Report Integrated mochawesome test report Install mochawesome library npm install --save-dev mochawesome Updating mocha configuration files You can refer to the demo project athttps://github.com/Automation-Test-Starter/SuperTest-Mocha-demo\n// mocha config module.exports = { timeout: 5000, // Set the default timeout for test cases (milliseconds) reporter: \u0026#39;mochawesome\u0026#39;, // Use mochawesome as the test report generator \u0026#39;reporter-option\u0026#39;: [ \u0026#39;reportDir=Report\u0026#39;, // Report directory \u0026#39;reportFilename=[status]_[datetime]-[name]-report\u0026#39;, //Report file name \u0026#39;html=true\u0026#39;, // enable html report \u0026#39;json=false\u0026#39;, // disable json report \u0026#39;overwrite=false\u0026#39;, // disable report file overwrite \u0026#39;timestamp=longDate\u0026#39;, // add timestamp to report file name ], // mochawesome report generator options spec: [\u0026#39;Specs/**/*.js\u0026#39;], // Specify the location of the test file }; Running test cases // Run test cases npm run test View test report Test report folder: Report, click to open the latest html report file with your browser\nJest version You can refer to the demo project athttps://github.com/Automation-Test-Starter/SuperTest-Jest-demo\nCreate a new jest project folder mkdir SuperTest-Jest-demo Jest demo project initialization // enter the project folder cd SuperTest-Mocha-demo // nodejs project initialization npm init -y Jest demo install dependencies // install supertest library npm install supertest --save-dev // install jest test framework npm install jest --save-dev Create new Jest demo project test folder and test cases // create test folder mkdir Specs // enter test folder and create test case file cd Specs touch test.spec.js Writing Jest demo Test Cases The test API can be found in the demoAPI.md file in the project.\nconst request = require(\u0026#39;supertest\u0026#39;); // Test Suite describe(\u0026#39;Verify that the Get and POST API returns correctly\u0026#39;, () =\u0026gt; { // Test case 1 it(\u0026#39;Verify that the GET API returns correctly\u0026#39;, async () =\u0026gt; { const res = await request(\u0026#39;https://jsonplaceholder.typicode.com\u0026#39;) // Test endpoint .get(\u0026#39;/posts/1\u0026#39;) // API endpoint .send() // request body .expect(200); // use supertest\u0026#39;s expect to verify that the status code is 200 // user jest\u0026#39;s expect to verify the response body expect(res.status).toBe(200); // Verify that the status code is 200 expect(res.body.id).toEqual(1); // Verify that the id is 1 expect(res.body.userId).toEqual(1); // Verify that the userId is 1 expect(res.body.title) .toEqual(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;); expect(res.body.body) .toEqual(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;); }); // Test case 2 it(\u0026#39;Verify that the POST API returns correctly\u0026#39;, async() =\u0026gt;{ const res = await request(\u0026#39;https://jsonplaceholder.typicode.com\u0026#39;) // Test endpoint .post(\u0026#39;/posts\u0026#39;) // API endpoint .send({ \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 }) // request body .expect(201); // use supertest\u0026#39;s expect to verify that the status code is 201 // user jest\u0026#39;s expect to verify the response body expect(res.statusCode).toBe(201); expect(res.body.id).toEqual(101); expect(res.body.userId).toEqual(1); expect(res.body.title).toEqual(\u0026#34;foo\u0026#34;); expect(res.body.body).toEqual(\u0026#34;bar\u0026#34;); }); }); Configuring Jest config files Creating a new configuration file // Create a new configuration file in the project root directory touch jest.config.js Updating configuration files // Desc: Jest configuration file module.exports = { // Specify the location of the test file testMatch: [\u0026#39;**/Specs/*.spec.js\u0026#39;], }; Adapting Jest Test Scripts Add the test script to the package.json file\n\u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;jest\u0026#34; }, Runing test case // run test case npm run test Jest test report Jest terminal Test Report Integrating jest-html-reporters test reports Install jest-html-reporters library npm install --save-dev jest-html-reporters Updating jest configuration files You can refer to the demo project atttps://github.com/Automation-Test-Starter/SuperTest-Jest-demo\n// Desc: Jest configuration file module.exports = { // specify the location of the test file testMatch: [\u0026#39;**/Specs/*.spec.js\u0026#39;], // test report generator reporters: [ \u0026#39;default\u0026#39;, [ \u0026#39;jest-html-reporters\u0026#39;, { publicPath: \u0026#39;./Report\u0026#39;, // report directory filename: \u0026#39;report.html\u0026#39;, // report file name pageTitle: \u0026#39;SuperTest and Jest API Test Report\u0026#39;, // report title overwrite: true, // enable report file overwrite expand: true, // enable report file expansion }, ], ], }; Running test cases // run test case npm run test View test report Test report folder: Report, click on the browser to open the latest html report file\n","permalink":"https://naodeng.com.cn/posts/api-automation-testing/supertest-tutorial-building-your-own-project-from-0-to-1/","summary":"dive into how to build a Supertest API automation testing project from scratch.Supertest is a popular Java library for performing REST API testing, providing powerful tools that make it easy to write automated test scripts to validate the API\u0026rsquo;sbehavior.","title":"SuperTest API Automation Testing Tutorial: Building a Supertest API Automation Test project from 0 to 1"},{"content":"Introduction This project is a quick start tutorial for API automation testing using SuperTest, and will use Jest or Mocha as the testing framework for demo demonstration.\nWe will introduce SuperTest, Jest and Mocha in turn, so that you can understand the basic usage of these tools in advance.\nIntroduction of SuperTest \u0026ldquo;Supertest\u0026rdquo; is a popular JavaScript library for testing Node.js applications. It is primarily used for end-to-end testing, also known as integration testing, to make sure that your application works properly across different components.Supertest is typically used in conjunction with testing frameworks such as Mocha, Jasmine or Jest to write and run test cases.\nHere are some of the key features and uses of Supertest:\nInitiating HTTP requests: Supertest allows you to easily simulate HTTP requests such as GET, POST, PUT, DELETE, etc. to test your application\u0026rsquo;s routing and endpoints. Chained Syntax: Supertest provides a chained syntax that allows you to build and execute multiple requests in a single test case, which helps simulate different user actions in your application. Assertions and Expectations: You can use Supertest in conjunction with assertion libraries such as Chai to examine the content of the response, status codes, headers, etc. to ensure the expected behavior of your application. Authentication Testing: Supertest can be used to test endpoints that require authentication to ensure that user login and authorization functions properly. Asynchronous support: Supertest can handle asynchronous operations, such as waiting for a response to return before executing further test code. Easy Integration: Supertest can be easily used with different Node.js frameworks (e.g. Express, Koa, Hapi, etc.), so you can test all types of applications. Using Supertest can help you verify that your application is working as expected, as well as quickly catch potential problems when changes are made to your application. Typically, you need to install Supertest and the testing framework in your project, and then write test cases to simulate different requests and check responses. This helps improve code quality and maintainability and ensures that your application remains stable as it evolves.\nOfficial documentation: https://github.com/ladjs/supertest\nNote: Supertest can be used not only for API testing, but also for unit and integration testing.\ncode examples:\n// import supertest const request = require(\u0026#39;supertest\u0026#39;); request({URL}) // request(url) or request(app) .get() or .put() or.post() // http methods .set() // http options .send() // http body .expect() // http assertions .end() // end the request Introduction of Jest Jest is a popular JavaScript testing framework for writing and running unit, integration and end-to-end tests for JavaScript applications. Its goal is to provide simple, fast and easy-to-use testing tools for a wide variety of JavaScript applications, both front-end and back-end.\nHere are some of the key features and uses of Jest:\nBuilt-in Assertion Library: Jest includes a powerful assertion library that makes it easy to write assertions to verify that code behaves as expected. Automated mocks: Jest automatically creates mocks that help you simulate functions, modules, and external dependencies, making testing easier and more manageable. Fast and Parallel: Jest saves time by intelligently selecting which tests to run and executing them in parallel, allowing you to run a large number of test cases quickly. Comprehensive Test Suite: Jest supports unit, integration and end-to-end testing and can test a wide range of application types such as JavaScript, TypeScript, React, Vue, Node.js and more. Snapshot testing: Jest has a snapshot testing feature that can be used to capture UI changes by checking if the rendering of a UI component matches a previous snapshot. Automatic Watch Mode: Jest has a watch mode that automatically re-runs tests as code changes are made, supporting developers in continuous testing. Rich Ecosystem: Jest has a rich set of plug-ins and extensions that can be used to extend its functionality, such as coverage reporting, test reporting, and integration with other tools. Community Support: Jest is a popular testing framework with a large community that provides extensive documentation, tutorials and support resources. Jest is often used in conjunction with other tools such as Babel (for transcoding JavaScript), Enzyme (for React component testing), Supertest (for API testing), etc. to achieve comprehensive test coverage and ensure code quality. Whether you\u0026rsquo;re writing front-end or back-end code, Jest is a powerful testing tool that can help you catch potential problems and improve code quality and maintainability.\nOfficial Documentation: https://jestjs.io/docs/zh-Hans/getting-started\nCode examples:\n// import jest const jest = require(\u0026#39;jest\u0026#39;); describe(): // test scenarios it(): // detailed test case, it() is in the describe() before(): // this action is before all test cases after(): // this action is after all test cases Introduction of Mocha Mocha is a popular JavaScript testing framework for writing and running a variety of tests for JavaScript applications, including unit tests, integration tests, and end-to-end tests.Mocha provides flexibility and extensibility, allowing developers to easily customize the test suite to meet the needs of their projects.\nHere are some of the key features and uses of Mocha:\nMultiple Test Styles: Mocha supports multiple test styles including BDD (Behavior Driven Development) and TDD (Test Driven Development). This allows developers to write test cases according to their preferences. Rich Assertion Library: Mocha does not include an assertion library by itself, but it can be used in conjunction with a variety of assertion libraries (e.g., Chai, Should.js, Expect.js, etc.), allowing you to write tests using your favorite assertion style. Asynchronous Testing: Mocha has built-in support for asynchronous testing, allowing you to test asynchronous code, Promises, callback functions, etc. to ensure that your code is correct in asynchronous scenarios. Parallel Testing: Mocha allows you to run test cases in your test suite in parallel, improving the efficiency of test execution. Rich Plug-ins and Extensions: Mocha has a rich ecosystem of plug-ins that can be used to extend its functionality, such as test coverage reporting, test report generation, and so on. Easy to Integrate: Mocha can be used with various assertion libraries, test runners (such as Karma and Jest), browsers (using the browser test runner), etc. to suit different project and testing needs. Command Line API: Mocha provides an easy-to-use command line API for running test suites, generating reports, and other test-related operations. Continuous Integration Support: Mocha can be easily integrated into Continuous Integration (CI) tools such as Jenkins, Travis CI, CircleCI, etc. to ensure that code is tested after every commit. Mocha\u0026rsquo;s flexibility and extensibility make it a popular testing framework for a variety of JavaScript projects, including front-end and back-end applications. Developers can choose the testing tools, assertion libraries, and other extensions to meet the requirements of their projects based on their needs and preferences. Whether you are writing browser-side code or server-side code, Mocha is a powerful testing tool to help you ensure code quality and reliability.\nOfficial documentation: https://mochajs.org/\nCode examples:\n// import mocha const mocha = require(\u0026#39;mocha\u0026#39;); describe(): // test scenarios it(): // detailed test case, it() is in the describe() before(): // this action is before all test cases after(): // this action is after all test cases Introduction of CHAI Chai is a JavaScript assertion library for assertion and expectation validation when writing and running test cases. It is a popular testing tool that is often used in conjunction with testing frameworks (e.g. Mocha, Jest, etc.) to help developers write and execute various types of tests, including unit tests and integration tests.\nHere are some of the key features and uses of Chai:\nReadable Assertion Syntax: Chai provides an easy to read and write assertion syntax that makes test cases easier to understand. It supports natural language assertion styles such as expect(foo).to.be.a(\u0026lsquo;string\u0026rsquo;) or expect(bar).to.equal(42). Multiple Assertion Styles: Chai provides a number of different assertion styles to suit different developer preferences. The main styles include BDD (Behavior-Driven Development) style, TDD (Test-Driven Development) style and assert style. Plugin extensions: Chai can be extended with plugins to support more assertion types and features. This allows Chai to fulfill a variety of testing needs, including asynchronous testing, HTTP request testing, and more. Easy Integration: Chai can be easily integrated with various testing frameworks such as Mocha, Jest, Jasmine etc. This makes it a powerful tool for writing test cases. Chained Assertions Support: Chai allows you to chain calls to multiple assertions to make complex testing and validation easier. Official documentation: https://www.chaijs.com/\nCode examples:\n// import chai const chai = require(\u0026#39;chai\u0026#39;); const expect = chai.expect; // demo assertions .expect(\u0026lt;actual result\u0026gt;).to.{assert}(\u0026lt;expected result\u0026gt;) // Asserts that the target is strictly equal to value. .expect(‘hello\u0026#39;).to.equal(\u0026#39;hello\u0026#39;); // Asserts that the target is strictly equal to value. .expect({ foo: \u0026#39;bar\u0026#39; }).to.not.equal({ foo: \u0026#39;bar\u0026#39; }); // Asserts that the target is not strictly equal to value. .expect(\u0026#39;foobar\u0026#39;).to.contain(\u0026#39;foo\u0026#39;); // Asserts that the target contains the given substring. .expect(foo).to.exist; // Asserts that the target is neither null nor undefined. .expect(5).to.be.at.most(5); // Asserts that the target is less than or equal to value. Project dependencies The following environments need to be installed in advance\nnodejs, demo version v21.1.0 Project Structure The following is the file structure of a SuperTest API Automation Test project, which contains test configuration files, test case files, test tool files, and test report files. It can be used for reference.\nSuperTest-Jest-demo ├── README.md ├── package.json ├── package-lock.json ├── Config // Test configuration │ └── config.js ├── Specs // Test case │ └── test.spec.js ├── Utils // Test tool │ └── utils.js ├── Report // Test report │ └── report.html ├── .gitignore └── node_modules // Project dependencies ├── ... └── ... Next In the next post, we will introduce how to build a SuperTest API automation test project from 0 to 1 using Supertest, so stay tuned.\n","permalink":"https://naodeng.com.cn/posts/api-automation-testing/supertest-tutorial-getting-started-and-own-environment-preparation/","summary":"a tutorial on Supertest, focusing on getting started and preparing the environment to be built.","title":"SuperTest API Automation Testing Tutorial: Getting Started and Setting Up Your Environment"},{"content":"CI/CD integration integration github action Use github action as an example, and other CI tools similarly\nThe Gradle version integration github action See the demo at https://github.com/Automation-Test-Starter/RestAssured-gradle-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as gradle.yml, in the .github/workflows directory.\nEdit the gradle.yml file: Copy the following into the file\nname: Gradle and REST Assured Tests on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v3 - name: Setup Java uses: actions/setup-java@v3 with: java-version: \u0026#39;11\u0026#39; distribution: \u0026#39;adopt\u0026#39; - name: Build and Run REST Assured Tests with Gradle uses: gradle/gradle-build-action@bd5760595778326ba7f1441bcf7e88b49de61a25 # v2.6.0 with: arguments: build - name: Archive REST-Assured results uses: actions/upload-artifact@v2 with: name: REST-Assured-results path: build/reports/tests/test - name: Upload REST-Assured results to GitHub uses: actions/upload-artifact@v2 with: name: REST-Assured-results path: build/reports/tests/test Commit the code: Add the gradle.yml file to your repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Gradle and REST Assured Tests workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. The Maven version integration github action See the demo at https://github.com/Automation-Test-Starter/RestAssured-maven-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate a workflow file: Create a YAML-formatted workflow file, such as maven.yml, in the .github/workflows directory.\nEdit the maven.yml file: Copy the following into the file\nname: Maven and REST Assured Tests on: push: branches: [ \u0026#34;main\u0026#34; ] pull_request: branches: [ \u0026#34;main\u0026#34; ] jobs: Run-Rest-Assured-Tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up JDK 17 uses: actions/setup-java@v3 with: java-version: \u0026#39;17\u0026#39; distribution: \u0026#39;temurin\u0026#39; cache: maven - name: Build and Run REST Assured Tests with Maven run: mvn test - name: Archive REST-Assured results uses: actions/upload-artifact@v3 with: name: REST-Assured-results path: target/surefire-reports - name: Upload REST-Assured results to GitHub uses: actions/upload-artifact@v3 with: name: REST-Assured-results path: target/surefire-reports Commit the code: Add the maven.yml file to the repository and commit. View test reports: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Maven and REST Assured Tests workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. Integrating allure test reports allure Introduction Allure is an open source testing framework for generating beautiful, interactive test reports. It can be used with a variety of testing frameworks (e.g. JUnit, TestNG, Cucumber, etc.) and a variety of programming languages (e.g. Java, Python, C#, etc.).\nAllure test reports have the following features:\nAesthetically pleasing and interactive: Allure test reports present test results in an aesthetically pleasing and interactive way, including graphs, charts and animations. This makes test reports easier to read and understand. Multi-language support: Allure supports multiple programming languages, so you can write tests in different languages and generate uniform test reports. Test case level details: Allure allows you to add detailed information to each test case, including descriptions, categories, labels, attachments, historical data, and more. This information helps provide a more complete picture of the test results. Historical Trend Analysis: Allure supports test historical trend analysis, which allows you to view the historical performance of test cases, identify issues and improve test quality. Categories and Tags: You can add categories and tags to test cases to better organize and categorize test cases. This makes reporting more readable. Attachments and Screenshots: Allure allows you to attach files, screenshots, and other attachments to better document information during testing. Integration: Allure seamlessly integrates with a variety of testing frameworks and build tools (e.g. Maven, Gradle), making it easy to generate reports. Open Source Community Support: Allure is an open source project with an active community that provides extensive documentation and support. This makes it the tool of choice for many automated testing teams. The main goal of Allure test reports is to provide a clear, easy-to-read way to present test results to help development teams better understand the status and quality of their tests, quickly identify problems, and take the necessary action. Whether you are a developer, tester, or project manager, Allure test reports provide you with useful information to improve software quality and reliability.\nOfficial Website: https://docs.qameta.io/allure/\nIntegration steps The Maven version integration of allure Add allure dependency in POM.xml Copy the contents of the pom.xml file in this project\n\u0026lt;!-- https://mvnrepository.com/artifact/io.qameta.allure/allure-testng --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.qameta.allure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;allure-testng\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.24.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/io.qameta.allure/allure-rest-assured --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.qameta.allure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;allure-rest-assured\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.24.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Add allure plugin to POM.xml \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.qameta.allure\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;allure-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.12.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;resultsDirectory\u0026gt;../allure-results\u0026lt;/resultsDirectory\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; Create test code for testing the REST API under src/test/java. The following is an example of a demo, see the project for details: https://github.com/Automation-Test-Starter/RestAssured-maven-demo.\npackage com.example; import io.qameta.allure.*; import io.qameta.allure.restassured.AllureRestAssured; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; @Epic(\u0026#34;REST API Regression Testing using TestNG\u0026#34;) @Feature(\u0026#34;Verify that the Get and POST API returns correctly\u0026#34;) public class TestDemo { @Test(description = \u0026#34;To get the details of post with id 1\u0026#34;, priority = 1) @Story(\u0026#34;GET Request with Valid post id\u0026#34;) @Severity(SeverityLevel.NORMAL) @Description(\u0026#34;Test Description : Verify that the GET API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .filter(new AllureRestAssured()) // Set up the AllureRestAssured filter to display request and response information in the test report .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .statusCode(200) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(1)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } @Test(description = \u0026#34;To create a new post\u0026#34;, priority = 2) @Story(\u0026#34;POST Request\u0026#34;) @Severity(SeverityLevel.NORMAL) @Description(\u0026#34;Test Description : Verify that the post API returns correctly\u0026#34;) public void verifyPostAPI() { // Given given() .filter(new AllureRestAssured()) // Set up the AllureRestAssured filter to display request and response information in the test report .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .body(\u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;foo\\\u0026#34;, \\\u0026#34;body\\\u0026#34;: \\\u0026#34;bar\\\u0026#34;, \\\u0026#34;userId\\\u0026#34;: 1\\n}\u0026#34;) .post(\u0026#34;/posts\u0026#34;) // Then .then() .statusCode(201) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(101)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;foo\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;bar\u0026#34;)); } } Run tests and generate Allure reports mvn clean test The generated Allure report is in the allure-results file in the project root directory.\nPreview of the Allure Report mvn allure:serve Running the command automatically opens a browser to preview the Allure report.\nThe Gradle version of allure integration Add the allure plugin to your build.gradle. Copy the contents of the build.gradle file in this project\nid(\u0026#34;io.qameta.allure\u0026#34;) version \u0026#34;2.11.2\u0026#34; Add allure dependency to build.gradle Copy the contents of the build.gradle file in this project\nimplementation \u0026#39;io.qameta.allure:allure-testng:2.24.0\u0026#39; // Add allure report dependency implementation \u0026#39;io.qameta.allure:allure-rest-assured:2.24.0\u0026#39; // Add allure report dependency Create test code for testing the REST API under src/test/java. The following is an example of a demo, see the project for details: https://github.com/Automation-Test-Starter/RestAssured-gradle-demo.\npackage com.example; import io.qameta.allure.*; import io.qameta.allure.restassured.AllureRestAssured; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; @Epic(\u0026#34;REST API Regression Testing using TestNG\u0026#34;) @Feature(\u0026#34;Verify that the Get and POST API returns correctly\u0026#34;) public class TestDemo { @Test(description = \u0026#34;To get the details of post with id 1\u0026#34;, priority = 1) @Story(\u0026#34;GET Request with Valid post id\u0026#34;) @Severity(SeverityLevel.NORMAL) @Description(\u0026#34;Test Description : Verify that the GET API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .filter(new AllureRestAssured()) // Set up the AllureRestAssured filter to display request and response information in the test report .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .statusCode(200) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(1)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } @Test(description = \u0026#34;To create a new post\u0026#34;, priority = 2) @Story(\u0026#34;POST Request\u0026#34;) @Severity(SeverityLevel.NORMAL) @Description(\u0026#34;Test Description : Verify that the post API returns correctly\u0026#34;) public void verifyPostAPI() { // Given given() .filter(new AllureRestAssured()) // Set up the AllureRestAssured filter to display request and response information in the test report .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .body(\u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;foo\\\u0026#34;, \\\u0026#34;body\\\u0026#34;: \\\u0026#34;bar\\\u0026#34;, \\\u0026#34;userId\\\u0026#34;: 1\\n}\u0026#34;) .post(\u0026#34;/posts\u0026#34;) // Then .then() .statusCode(201) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(101)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;foo\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;bar\u0026#34;)); } } Run the test and generate the Allure report gradle clean test `` \u0026gt; The generated Allure report is in the build/allure-results file in the project root directory. - Preview the Allure report ```bash gradle allureServe Running the command automatically opens a browser to preview the Allure report.\nReference Rest assured official documentation: https://rest-assured.io/\nRest assured official github:https://github.com/rest-assured/rest-assured\nRest assured official docs in Chinese: https://github.com/RookieTester/rest-assured-doc\n","permalink":"https://naodeng.com.cn/posts/api-automation-testing/rest-assured-tutorial-advance-usage-integration-ci-cd-and-allure-report/","summary":"dive into advanced applications of REST Assured, focusing on how to integrate CI/CD (Continuous Integration/Continuous Delivery) tools and integrate Allure test reports.","title":"REST Assured API Automation Testing Tutorial: Advanced Usage - Integration CI CD and Integration Allure Report"},{"content":"Advanced Usage Verifying Response Data You can verify Response status code, Response status line, Response cookies, Response headers, Response content type and Response body.\nresponse body assertion json assertion Assume that the GET request (to http://localhost:8080/lotto) returns JSON as:\n{ \u0026#34;lotto\u0026#34;:{ \u0026#34;lottoId\u0026#34;:5, \u0026#34;winning-numbers\u0026#34;:[2,45,34,23,7,5,3], \u0026#34;winners\u0026#34;:[{ \u0026#34;winnerId\u0026#34;:23, \u0026#34;numbers\u0026#34;:[2,45,34,23,3,5] },{ \u0026#34;winnerId\u0026#34;:54, \u0026#34;numbers\u0026#34;:[52,3,12,11,18,22] }] } } REST assured makes it easy to make get requests and process response messages.\nAsserts whether the value of lottoId is equal to 5. For example: get(\u0026#34;/lotto\u0026#34;).then().body(\u0026#34;lotto.lottoId\u0026#34;, equalTo(5)); Assertion The values for winnerId include 23 and 54. For example: get(\u0026#34;/lotto\u0026#34;).then().body(\u0026#34;lotto.winners.winnerId\u0026#34;, hasItems(23, 54)); Note: equalTo and hasItems are Hamcrest matchers which you should statically import from org.hamcrest.Matchers.\nXML assertion XML can be verified in a similar way. Imagine that a POST request to http://localhost:8080/greetXML returns:\n\u0026lt;greeting\u0026gt; \u0026lt;firstName\u0026gt;{params(\u0026#34;firstName\u0026#34;)}\u0026lt;/firstName\u0026gt; \u0026lt;lastName\u0026gt;{params(\u0026#34;lastName\u0026#34;)}\u0026lt;/lastName\u0026gt; \u0026lt;/greeting\u0026gt; Asserts whether the firstName is returned correctly. For example: given(). parameters(\u0026#34;firstName\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;lastName\u0026#34;, \u0026#34;Doe\u0026#34;). when(). post(\u0026#34;/greetXML\u0026#34;). then(). body(\u0026#34;greeting.firstName\u0026#34;, equalTo(\u0026#34;John\u0026#34;)). Assert that firstname and lastname are returned correctly. For example: given(). parameters(\u0026#34;firstName\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;lastName\u0026#34;, \u0026#34;Doe\u0026#34;). when(). post(\u0026#34;/greetXML\u0026#34;). then(). body(\u0026#34;greeting.firstName\u0026#34;, equalTo(\u0026#34;John\u0026#34;)). body(\u0026#34;greeting.lastName\u0026#34;, equalTo(\u0026#34;Doe\u0026#34;)); with().parameters(\u0026#34;firstName\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;lastName\u0026#34;, \u0026#34;Doe\u0026#34;) .when().post(\u0026#34;/greetXML\u0026#34;) .then().body(\u0026#34;greeting.firstName\u0026#34;, equalTo(\u0026#34;John\u0026#34;), \u0026#34;greeting.lastName\u0026#34;, equalTo(\u0026#34;Doe\u0026#34;)); Cookie assertion Asserts whether the value of the cookie is equal to cookieValue. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().cookie(\u0026#34;cookieName\u0026#34;, \u0026#34;cookieValue\u0026#34;) Asserts whether the value of multiple cookies is equal to the cookieValue at the same time. For example: get(\u0026#34;/x\u0026#34;).then() .assertThat().cookies(\u0026#34;cookieName1\u0026#34;, \u0026#34;cookieValue1\u0026#34;, \u0026#34;cookieName2\u0026#34;, \u0026#34;cookieValue2\u0026#34;) Asserts whether the value of the cookie contains a cookieValue. For example: get(\u0026#34;/x\u0026#34;).then() .assertThat().cookies(\u0026#34;cookieName1\u0026#34;, \u0026#34;cookieValue1\u0026#34;, \u0026#34;cookieName2\u0026#34;, containsString(\u0026#34;Value2\u0026#34;)) Status Code Assertion Assertion Whether the status code is equal to 200. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().statusCode(200) Assertion Whether the status line is something. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().statusLine(\u0026#34;something\u0026#34;) Assertion Whether the status line contains some. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().statusLine(containsString(\u0026#34;some\u0026#34;)) Header Assertion Asserts whether the value of Header is equal to HeaderValue. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().header(\u0026#34;headerName\u0026#34;, \u0026#34;headerValue\u0026#34;) Asserts whether the value of multiple Headers is equal to HeaderValue at the same time. For example: get(\u0026#34;/x\u0026#34;).then() .assertThat().headers(\u0026#34;headerName1\u0026#34;, \u0026#34;headerValue1\u0026#34;, \u0026#34;headerName2\u0026#34;, \u0026#34;headerValue2\u0026#34;) Asserts whether the value of the Header contains a HeaderValue. For example: get(\u0026#34;/x\u0026#34;).then().assertThat() .headers(\u0026#34;headerName1\u0026#34;, \u0026#34;headerValue1\u0026#34;, \u0026#34;headerName2\u0026#34;, containsString(\u0026#34;Value2\u0026#34;)) Assert that the \u0026ldquo;Content-Length\u0026rdquo; of the Header is less than 1000. For example: The header can be first converted to int using the mapping function, and then asserted using the \u0026ldquo;integer\u0026rdquo; matcher before validation with Hamcrest:\nget(\u0026#34;/something\u0026#34;).then().assertThat().header(\u0026#34;Content-Length\u0026#34;, Integer::parseInt, lessThan(1000)); Content-Type Assertion Asserts whether the value of Content-Type is equal to application/json. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().contentType(ContentType.JSON) Full body/content matching Assertion Assertion Whether the response body is exactly equal to something. For example: get(\u0026#34;/x\u0026#34;).then().assertThat().body(equalTo(\u0026#34;something\u0026#34;)) Measuring Response Time As of version 2.8.0 REST Assured has support measuring response time. For example:\nlong timeInMs = get(\u0026#34;/lotto\u0026#34;).time() or using a specific time unit:\nlong timeInSeconds = get(\u0026#34;/lotto\u0026#34;).timeIn(SECONDS); where \u0026lsquo;SECONDS\u0026rsquo; is just a standard \u0026lsquo;TimeUnit\u0026rsquo;. You can also validate it using the validation DSL:\nwhen(). get(\u0026#34;/lotto\u0026#34;). then(). time(lessThan(2000L)); // Milliseconds or\nwhen(). get(\u0026#34;/lotto\u0026#34;). then(). time(lessThan(2L), SECONDS); Note that you can only referentially correlate these measurements to server request processing times (as response times will include HTTP roundtrips, REST Assured processing times, etc., and cannot be very accurate).\nFile Upload Often we use the multipart form data technique when transferring large amounts of data to the server, such as files. rest-assured provides a multiPart method to recognize whether this is a file, a binary sequence, an input stream, or uploaded text.\nUpload only one file in the form. For example: given(). multiPart(new File(\u0026#34;/path/to/file\u0026#34;)). when(). post(\u0026#34;/upload\u0026#34;); Uploading a file in the presence of a control name. For example: given(). multiPart(\u0026#34;controlName\u0026#34;, new File(\u0026#34;/path/to/file\u0026#34;)). when(). post(\u0026#34;/upload\u0026#34;); Multiple \u0026ldquo;multi-parts\u0026rdquo; entities in the same request. For example: byte[] someData = .. given(). multiPart(\u0026#34;controlName1\u0026#34;, new File(\u0026#34;/path/to/file\u0026#34;)). multiPart(\u0026#34;controlName2\u0026#34;, \u0026#34;my_file_name.txt\u0026#34;, someData). multiPart(\u0026#34;controlName3\u0026#34;, someJavaObject, \u0026#34;application/json\u0026#34;). when(). post(\u0026#34;/upload\u0026#34;); MultiPartSpecBuilder use cases. For example: For more usage referencesMultiPartSpecBuilder：\nGreeting greeting = new Greeting(); greeting.setFirstName(\u0026#34;John\u0026#34;); greeting.setLastName(\u0026#34;Doe\u0026#34;); given(). multiPart(new MultiPartSpecBuilder(greeting, ObjectMapperType.JACKSON_2) .fileName(\u0026#34;greeting.json\u0026#34;) .controlName(\u0026#34;text\u0026#34;) .mimeType(\u0026#34;application/vnd.custom+json\u0026#34;).build()). when(). post(\u0026#34;/multipart/json\u0026#34;). then(). statusCode(200); MultiPartConfig use cases. For example: MultiPartConfigYou can specify the default control name and file name.\ngiven().config(config().multiPartConfig(multiPartConfig().defaultControlName(\u0026#34;something-else\u0026#34;))) By default, the control name is configured as \u0026ldquo;something-else\u0026rdquo; instead of \u0026ldquo;file\u0026rdquo;. For more usage references blog introduction\nLogging When we are writing API test scripts, we may need to print some logs during the test process so that we can view the request and response information of the API and some other information during the test process.RestAssured provides some methods to print logs.\nRestAssured provides a global logging configuration method that allows you to configure logging before the test starts and then print the logs during the test. This method is applicable to all test cases, but it can only print request and response information, not other information.\nRestAssured also provides a localized log configuration method that prints logs during the test. This method prints request and response information as well as other information.\nGlobal logging configuration Steps to add global logging configuration Importing logging-related dependency classes import io.restassured.config.LogConfig; import io.restassured.filter.log.LogDetail; import io.restassured.filter.log.RequestLoggingFilter; import io.restassured.filter.log.ResponseLoggingFilter; Adding logging configuration to the setup() method Use LogConfig configuration to enable logging of requests and responses, as well as to enable nice output formatting. Enabled logging filters for requests and responses, which will log details of requests and responses.\n// Setting the Global Request and Response Logging Configuration RestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL) .enablePrettyPrinting(true)); Enabled global logging filters in the setup() method // Enable global request and response logging filters RestAssured.filters(new RequestLoggingFilter(), new ResponseLoggingFilter()); Global Logging Code Example package com.example; import io.restassured.RestAssured; // Importing logging-related dependency classes import io.restassured.config.LogConfig; import io.restassured.filter.log.LogDetail; import io.restassured.filter.log.RequestLoggingFilter; import io.restassured.filter.log.ResponseLoggingFilter; import org.testng.annotations.BeforeClass; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; public class TestDemo { @BeforeClass public void setup() { // Setting the Global Request and Response Logging Configuration RestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL) .enablePrettyPrinting(true)); // Enable global request and response logging filters RestAssured.filters(new RequestLoggingFilter(), new ResponseLoggingFilter()); } @Test(description = \u0026#34;Verify that the Get Post API returns correctly\u0026#34;) public void verifyGetAPI() { // Test cases have been omitted, refer to the demo } @Test(description = \u0026#34;Verify that the publish post API returns correctly\u0026#34;) public void verifyPostAPI() { // Test cases have been omitted, refer to the demo } } Viewing Global Log Output Open the Terminal window for this project and run the test script by executing the following command Viewing Log Output Localized logging configuration In RestAssured, you can make localized logging configurations to enable or disable logging for specific test methods or requests without affecting the global configuration.\nSteps to add Localized logging configuration Add logging configuration is enabled in the test method for which you want to print logs @Test(description = \u0026#34;Verify that the Get Post API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .log().everything(true) // Output request-related logs .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .log().everything(true) // Output response-related logs .statusCode(200) } Viewing Localized Log Output Open the Terminal window for this project and run the test script by executing the following command Viewing Log Output LogConfig Configuration Description In Rest-Assured, you can use the LogConfig class to configure logging of requests and responses. The LogConfig allows you to define the level of logging detail, the output format, the location of the output, and so on. The following are some common LogConfig configuration examples:\nEnable logging of requests and responses:\nRestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL));; This will enable logging of requests and responses only if validation fails.\nConfigure the output level:\nRestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.HEADERS));; This will log only the request and response headers.\nConfigure the location of the output:\nRestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL) .enablePrettyPrinting(true) .defaultStream(FileOutputStream(\u0026#34;log.txt\u0026#34;))); This outputs the log records to a file named \u0026ldquo;log.txt\u0026rdquo;.\nConfigure the nice output format:\nRestAssured.config = RestAssured.config() .logConfig(LogConfig.logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(LogDetail.ALL) .enablePrettyPrinting(true)); This will enable nice output formatting and make the logs easier to read.\nYou can combine these configuration options according to your specific needs and set it to RestAssured.config to configure global request and response logging. This will help log and review requests and responses in RestAssured for debugging and analyzing issues.\nRequest Logging Starting with version 1.5, REST Assured supports logging request specifications before they are sent to the server using RequestLoggingFilter. Note that HTTP Builder and HTTP Client may add headers other than what is printed in the log. The filter will only log the details specified in the request specification. That is, you cannot consider the details logged by the RequestLoggingFilter to be the details actually sent to the server. In addition, subsequent filters may change the request after logging has occurred. If you need to log what is actually sent over the network, see the HTTP Client Logging documentation or use an external tool such as fiddler.\nExamples：\ngiven().log().all() // Log all request specification details including parameters, headers and body given().log().params() // Log only the parameters of the request given().log().body() // Log only the request body given().log().headers() // Log only the request headers given().log().cookies() // Log only the request cookies given().log().method() // Log only the request method given().log().path() // Log only the request path Response Logging Wanting to print only the body of the response, regardless of the status code, you can do the following. , for example: get(\u0026#34;/x\u0026#34;).then().log().body() The response body will be printed whether or not an error occurs. If only interested in printing the response body when an error occurs, for example: get(\u0026#34;/x\u0026#34;).then().log().ifError() Record all details in the response, including status lines, headers, and cookies, for example: get(\u0026#34;/x\u0026#34;).then().log().all() Record only the status line, header, or cookie in the response, for example: get(\u0026#34;/x\u0026#34;).then().log().statusLine() // Only log the status line get(\u0026#34;/x\u0026#34;).then().log().headers() // Only log the response headers get(\u0026#34;/x\u0026#34;).then().log().cookies() // Only log the response cookies Configured to log a response only when the status code matches a value. for example: get(\u0026#34;/x\u0026#34;).then().log().ifStatusCodeIsEqualTo(302) // Only log if the status code is equal to 302 get(\u0026#34;/x\u0026#34;).then().log().ifStatusCodeMatches(matcher) // Only log if the status code matches the supplied Hamcrest matcher Log if validation fails Since REST Assured 2.3.1 you can log the request or response only if the validation fails. To log the request do. for example: given().log().ifValidationFails() To log the response. for example: then().log().ifValidationFails() It can be enabled for both requests and responses using LogConfig, for example: given().config(RestAssured.config().logConfig(logConfig() .enableLoggingOfRequestAndResponseIfValidationFails(HEADERS))) If authentication fails, the log only records the request header.\nAnother shortcut to enable request and response logging for all requests if authentication fails, for example: RestAssured.enableLoggingOfRequestAndResponseIfValidationFails(); Starting with version 4.5.0, you can also use specify the message that will be displayed if the onFailMessage test fails, for example: when(). get(). then(). onFailMessage(\u0026#34;Some specific message\u0026#34;). statusCode(200); Header Blacklist Configuration Starting with REST Assured 4.2.0, it is possible to blacklist headers so that they do not show up in request or response logs. Instead, the header value will be replaced with [ BLACKLISTED ] . You can enable this feature on a per-header basis using LogConfig, for example:\ngiven().config(config().logConfig(logConfig().blacklistHeader(\u0026#34;Accept\u0026#34;))) Filters In RestAssured, you can use filters to modify requests and responses. Filters allow you to modify requests and responses at different stages of the request and response process. For example, you can modify the request before the request or the response after the response. You can use filters to add request headers, request parameters, request bodies, response headers, response bodies, and so on.\nFilters can be used to implement custom authentication schemes, session management, logging, and so on. To create a filter, you need to implement the io.restassured.filter.Filter API. To use a filter, you can do the following:\ngiven().filter(new MyFilter()) There are a couple of filters provided by REST-Assured that are ready to use:\nio.restassured.filter.log.RequestLoggingFilter: A filter that\u0026rsquo;ll print the request specification details. io.restassured.filter.log.ResponseLoggingFilter: A filter that\u0026rsquo;ll print the response details if the response matches a given status code. io.restassured.filter.log.ErrorLoggingFilter: A filter that\u0026rsquo;ll print the response body if an error occurred (status code is between 400 and 500). Ordered Filters As of REST Assured 3.0.2 you can implement the io.restassured.filter.OrderedFilter API if you need to control the filter ordering. Here you implement the getOrder method to return an integer representing the precedence of the filter. A lower value gives higher precedence. The highest precedence you can define is Integer.MIN_VALUE and the lowest precedence is Integer.MAX_VALUE. Filters not implementing io.restassured.filter.OrderedFilter will have a default precedence of 1000.\nexamples\nResponse Builder If you need to change the Response from a filter you can use the ResponseBuilder to create a new Response based on the original response. For example if you want to change the body of the original response to something else you can do:\nResponse newResponse = new ResponseBuilder() .clone(originalResponse).setBody(\u0026#34;Something\u0026#34;).build(); ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/rest-assured-tutorial-advance-usage-verifying-response-and-logging/","summary":"provide an in-depth look at advanced uses of REST Assured, with a focus on validating API responses, logging, and the application of filters.","title":"REST Assured API Automation Testing Tutorial: Advanced Usage - Validating Responses and Logging, Filters, File Uploads"},{"content":"Building a REST Assured API test project from 0 to 1 REST Assured supports both Gradle and Maven build tools, you can choose one of them according to your preference. Below is a description of the initialization process for Gradle and Maven build tools.\nThis project is built using Gradle 8.44 and Maven 3.9.5, if you are using other versions, it may be different.\nGradle version See the demo project at https://github.com/Automation-Test-Starter/RestAssured-gradle-demo.\nInitialize an empty Gradle project mkdir RestAssured-gradle-demo cd RestAssured-gradle-demo gradle init Configuration build.gradle The demo project introduces the testNG testing framework. For reference only.\nCreate a build.gradle file in the project root directory to configure the project. For reference, the following is a sample configuration // plugins configuration plugins { id \u0026#39;java\u0026#39; // use java plugin } // repositories configuration repositories { mavenCentral() // user maven central repository } // dependencies configuration dependencies { testImplementation \u0026#39;io.rest-assured:rest-assured:5.3.1\u0026#39; // add rest-assured dependency testImplementation \u0026#39;org.testng:testng:7.8.0\u0026#39; // add testng testing framework dependency implementation \u0026#39;org.uncommons:reportng:1.1.4\u0026#39; // add testng reportng dependency implementation \u0026#39;org.slf4j:slf4j-api:2.0.9\u0026#39; // add slf4j dependency for test logging implementation \u0026#39;org.slf4j:slf4j-simple:2.0.9\u0026#39; // add slf4j dependency for test logging implementation group: \u0026#39;com.google.inject\u0026#39;, name: \u0026#39;guice\u0026#39;, version: \u0026#39;7.0.0\u0026#39; } // test configuration test { reports.html.required = false // set gradle html report to false reports.junitXml.required = false // set gradle junitXml report to false // use testng testing framework useTestNG() { useDefaultListeners = true suites \u0026#39;src/test/resources/testng.xml\u0026#39; // set testng.xml file path } testLogging.showStandardStreams = true // output test log to console testLogging.events \u0026#34;passed\u0026#34;, \u0026#34;skipped\u0026#34;, \u0026#34;failed\u0026#34; // deny output test log to console } You can copy the contents of the build.gradle file in this project. For more configuration refer to Official Documentation\ntestng.xml configuration Create a resources directory under the src/test directory to store test configuration files.\nCreate a testng.xml file in the resources directory to configure the TestNG test framework.\nFor reference, the following is a sample configuration\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE suite SYSTEM \u0026#34;http://testng.org/testng-1.0.dtd\u0026#34;\u0026gt; \u0026lt;suite name=\u0026#34;restAssured-gradleTestSuite\u0026#34;\u0026gt; \u0026lt;test thread-count=\u0026#34;1\u0026#34; name=\u0026#34;Demo\u0026#34;\u0026gt; \u0026lt;classes\u0026gt; \u0026lt;class name=\u0026#34;com.example.TestDemo\u0026#34;/\u0026gt; \u0026lt;!-- test case class--\u0026gt; \u0026lt;/classes\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;/suite\u0026gt; \u0026lt;!-- Suite --\u0026gt; gradle build project and initialize Open the Terminal window of the project with an editor and execute the following command to confirm that the project build was successful gradle build Initialization complete: After completing the wizard, Gradle will generate a basic Gradle project structure in the project directory initialization project directory The directory structure can be found in \u0026raquo; Project structure\nCreate a new test class in the project\u0026rsquo;s test source directory. By default, Gradle usually places the test source code in the src/test/java directory. You can create a package of test classes in that directory and create a new test class in the package\nTo create a test class for TestDemo, you can create files with the following structure\nsrc └── test └── java └── com └── example └── TestDemo.java Introduction of demo test API Get API HOST: https://jsonplaceholder.typicode.com API path: /posts/1 Request method: GET Request Parameters: None Request header: \u0026ldquo;Content-Type\u0026rdquo;: \u0026ldquo;application/json; charset=utf-8\u0026rdquo; Request Body: None Response status code: 200 Response header: \u0026ldquo;Content-Type\u0026rdquo;: \u0026ldquo;application/json; charset=utf-8\u0026rdquo; Response body: { \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34; } Post API HOST: https://jsonplaceholder.typicode.com API path:/posts Request method: POST Request Parameters: None Request header:\u0026ldquo;Content-Type\u0026rdquo;: \u0026ldquo;application/json; charset=utf-8\u0026rdquo; Request Body:raw json format Request Body: { \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1 } Response status code: 201 Response header:\u0026ldquo;Content-Type\u0026rdquo;: \u0026ldquo;application/json; charset=utf-8\u0026rdquo; Response body: { \u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;, \u0026#34;userId\u0026#34;: 1, \u0026#34;id\u0026#34;: 101 } Writing Test cases Open the TestDemo.java file and start writing the test script.\nThe example script is as follows. For reference\npackage com.example; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; public class TestDemo { @Test(description = \u0026#34;Verify that the Get Post API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .statusCode(200) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(1)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } @Test(description = \u0026#34;Verify that the publish post API returns correctly\u0026#34;) public void verifyPostAPI() { // Given given() .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .body(\u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;foo\\\u0026#34;, \\\u0026#34;body\\\u0026#34;: \\\u0026#34;bar\\\u0026#34;, \\\u0026#34;userId\\\u0026#34;: 1\\n}\u0026#34;) .post(\u0026#34;/posts\u0026#34;) // Then .then() .statusCode(201) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(101)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;foo\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;bar\u0026#34;)); } } Debugging test cases Open the Terminal window for this project and run the test script by executing the following command gradle test Viewing Test Reports Command Line Report testng html Report Open the project build/reports/tests/test directory. Click on the index.html file to view the test report. Maven version See the demo project at https://github.com/Automation-Test-Starter/RestAssured-maven-demo\nInitialize an empty Maven project mvn archetype:generate -DgroupId=com.example -DartifactId=RestAssured-maven-demo -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false Initialization complete: After completing the wizard, Maven will create a new project directory and a basic Maven project structure\nConfiguration pom.xml Add the following to the pom.xml file in your project\nYou can copy the contents of the pom.xml file in this project. For more information on configuration, please refer to the official documentation.\n\u0026lt;!-- dependencies config --\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/io.rest-assured/rest-assured --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.rest-assured\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rest-assured\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.3.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.testng/testng --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.testng\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;testng\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;7.8.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;!-- plugin config --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;suiteXmlFiles\u0026gt; \u0026lt;suiteXmlFile\u0026gt;src/test/resources/testng.xml\u0026lt;/suiteXmlFile\u0026gt; \u0026lt;/suiteXmlFiles\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; Configuration testng.xml Create a resources directory under the src/test directory to store test configuration files.\nCreate a testng.xml file in the resources directory to configure the TestNG test framework.\nFor reference, the following is a sample configuration\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE suite SYSTEM \u0026#34;http://testng.org/testng-1.0.dtd\u0026#34;\u0026gt; \u0026lt;suite name=\u0026#34;restAssured-gradleTestSuite\u0026#34;\u0026gt; \u0026lt;test thread-count=\u0026#34;1\u0026#34; name=\u0026#34;Demo\u0026#34;\u0026gt; \u0026lt;classes\u0026gt; \u0026lt;class name=\u0026#34;com.example.TestDemo\u0026#34;/\u0026gt; \u0026lt;!-- test case class--\u0026gt; \u0026lt;/classes\u0026gt; \u0026lt;/test\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;/suite\u0026gt; \u0026lt;!-- Suite --\u0026gt; initialization maven project directory The directory structure can be found in \u0026raquo; Project structure\nCreate a new test class in the project\u0026rsquo;s test source directory. By default, Gradle usually places the test source code in the src/test/java directory. You can create a package of test classes in that directory and create a new test class in the package\nTo create a test class for TestDemo, you can create files with the following structure\nsrc └── test └── java └── com └── example └── TestDemo.java The api used by Demo referable to \u0026raquo; Introduction of demo test API\nWriting Test cases Open the TestDemo.java file and start writing the test script.\nThe example script is as follows. For reference\npackage com.example; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; public class TestDemo { @Test(description = \u0026#34;Verify that the Get Post API returns correctly\u0026#34;) public void verifyGetAPI() { // Given given() .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .get(\u0026#34;/posts/1\u0026#34;) // Then .then() .statusCode(200) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(1)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } @Test(description = \u0026#34;Verify that the publish post API returns correctly\u0026#34;) public void verifyPostAPI() { // Given given() .baseUri(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // When .when() .body(\u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;foo\\\u0026#34;, \\\u0026#34;body\\\u0026#34;: \\\u0026#34;bar\\\u0026#34;, \\\u0026#34;userId\\\u0026#34;: 1\\n}\u0026#34;) .post(\u0026#34;/posts\u0026#34;) // Then .then() .statusCode(201) // To verify correct value .body(\u0026#34;userId\u0026#34;, equalTo(1)) .body(\u0026#34;id\u0026#34;, equalTo(101)) .body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;foo\u0026#34;)) .body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;bar\u0026#34;)); } } Debugging test cases Open the Terminal window for this project and run the test script by executing the following command mvn test Viewing Test Reports terminal report testng html report Open the project target/surefire-reports directory. Click on the index.html file to view the test report. More info Visit my personal blog: https://naodeng.tech/ My QA automation quickstart project page: https://github.com/Automation-Test-Starter ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/rest-assured-tutorial-building-your-own-project-from-0-to-1/","summary":"dive into how to build a REST Assured API automation testing project from scratch.","title":"REST Assured API Automation Testing Tutorial: Building a REST Assured API Automation Test project from 0 to 1"},{"content":"Introduction of RestAssured REST Assured is a Java testing framework for testing RESTful APIs that enables developers/testers to easily write and execute API tests. It is designed to make API testing simple and intuitive, while providing rich functionality and flexibility. The following are some of the key features and uses of REST Assured:\nInitiating HTTP requests: REST Assured allows you to easily build and initiate HTTP GET, POST, PUT, DELETE and other types of requests. You can specify the request\u0026rsquo;s URL, headers, parameters, body, and other information.\nChained Syntax: REST Assured uses chained syntax to make test code more readable and easy to write. You can describe your test cases in a natural way without writing tons of code.\nAssertions and Checksums: REST Assured provides a rich set of checksums that can be used to validate API response status codes, response bodies, response headers, and so on. You can add multiple assertions according to your testing needs.\nSupport for multiple data formats: REST Assured supports a variety of data formats, including JSON, XML, HTML, Text and so on. You can use appropriate methods to handle different formats of response data.\nIntegration with BDD (Behavior-Driven Development): REST Assured can be used in conjunction with BDD frameworks (such as Cucumber), allowing you to better describe and manage test cases.\nSimulate HTTP Server: REST Assured also includes a simulation of an HTTP server, allowing you to simulate the behavior of an API for end-to-end testing.\nExtensibility: REST Assured can be customized with plug-ins and extensions to meet specific testing needs.\nOverall, REST Assured is a powerful and easy-to-use API testing framework that helps you easily perform RESTful API testing and provides many tools to verify the correctness and performance of an API. Whether you are a beginner or an experienced developer/tester, REST Assured is a valuable tool for quickly getting started with API automation testing.\nProject structure Gradle-built versions - src - main - java - (The main source code of the application) - test - test - api - (REST Assured test code) - UsersAPITest.java - ProductsAPITest.java - TestConfig.java - TestConfig.java - resources - (configuration files, test data, etc.) - (other project files and resources) - build.gradle (Gradle project configuration file) In this example directory structure:\nsrc/test/java/api directory is used to hold REST Assured test classes, each of which typically involves tests for one or more related API endpoints. For example, UsersAPITest.java and ProductsAPITest.java could contain tests for user management and product management. The src/test/java/util directory can be used to store tool classes that are shared among tests, such as TestConfig.java for configuring REST Assured. The src/test/resources directory can contain test data files, configuration files, and other resources that can be used in tests. build.gradle is the gradle project\u0026rsquo;s configuration file, which is used to define the project\u0026rsquo;s dependencies, build configuration, and other project settings. Maven-built versions - src - main - java - (The main source code of the application) - test - java - api - (REST Assured test code) - UsersAPITest.java - ProductsAPITest.java - util - TestConfig.java - resources - (configuration files, test data, etc.) - (other project files and resources) - pom.xml (Maven project configuration file) In this example directory structure:\nsrc/test/java/api directory is used to hold REST Assured test classes, each of which typically involves tests for one or more related API endpoints. For example, UsersAPITest.java and ProductsAPITest.java could contain tests for user management and product management. The src/test/java/util directory can be used to store tool classes that are shared among tests, such as TestConfig.java for configuring REST Assured. The src/test/resources directory can contain test data files, configuration files, and other resources that can be used in the tests. pom.xml is a Maven project configuration file that is used to define project dependencies, build configurations, and other project settings. Project dependency JDK 1.8+, I\u0026rsquo;m using JDK 19 Gradle 6.0+ or Maven 3.0+, I\u0026rsquo;m using Gradle 8.44 and Maven 3.9.5 RestAssured 4.3.3+, I\u0026rsquo;m using the latest version 5.3.2 Syntax Example Here\u0026rsquo;s a simple example of RestAssured syntax that demonstrates how to perform a GET request and validate the response: First, make sure you have added a RestAssured dependency to your Gradle or Maven project.\nFirst, make sure you have added a RestAssured dependency to your Gradle or Maven project.\nGradle dependency:\ndependency { implementation \u0026#39;io.rest-assured:rest-assured:5.3.1\u0026#39; } Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.rest-assured\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rest-assured\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.3.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Next, create a test class and write the following code:\nimport io.restassured.RestAssured; import io.restassured.response.Response; import org.testng.annotations.Test; import static io.restassured.RestAssured.given; import static org.hamcrest.Matchers.equalTo; public class RestAssuredDemo { @Test public void testGetRequest() { // Set the base URI, using JSONPlaceholder as an example RestAssured.baseURI = \u0026#34;https://jsonplaceholder.typicode.com\u0026#34;; // Send a GET request and save the response Response response = given() .when() .get(\u0026#34;/posts/1\u0026#34;) .then() .extract() .response(); // Print the JSON content of the response System.out.println(\u0026#34;Response JSON: \u0026#34; + response.asString()); // Verify that the status code is 200. // Validate that the status code is 200 response.then().statusCode(200); // validate that the response has a status code of 200. // Validate a specific field value in the response response.then().body(\u0026#34;userId\u0026#34;, equalTo(1)); response.then().body(\u0026#34;id\u0026#34;, equalTo(1)); response.then().body(\u0026#34;title\u0026#34;, equalTo(\u0026#34;sunt aut facere repellat provident occaecati excepturi optio reprehenderit\u0026#34;)); response.then().body(\u0026#34;body\u0026#34;, equalTo(\u0026#34;quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto\u0026#34;)); } } The above code executes a GET request to JSONPlaceholder\u0026rsquo;s /posts/1 endpoint and validates the response with a status code and values for specific fields. You can modify the base URI and validation conditions to suit your needs.\nIn this example, we\u0026rsquo;re using the TestNG testing framework, but you can also use other testing frameworks such as JUnit. make sure your test classes contain the appropriate import statements and configure them appropriately as needed.\nThis is a simple example of RestAssured syntax for performing a GET request and validating the response. You can build more complex test cases based on the needs of your project and the complexity of your API.\n","permalink":"https://naodeng.com.cn/posts/api-automation-testing/rest-assured-tutorial-and-environment-preparation/","summary":"a tutorial on REST Assured, focusing on getting started and preparing the environment to be built.","title":"REST Assured API Automation Testing Tutorial: Getting Started and Setting Up Your Environment"},{"content":"CI/CD Integration Accessing github action Take github action as an example, and other CI tools as well.\nGradle + Scala version See the demo at https://github.com/Automation-Test-Starter/gatling-gradle-scala-demo.\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate the workflow file: Create a YAML-formatted workflow file, such as gatling.yml, in the .github/workflows directory.\nEdit the gatling.yml file: Copy the following into the file.\nname: Gatling Performance Test on: push: branches: - main jobs: performance-test: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Java uses: actions/setup-java@v2 with: java-version: 11 distribution: \u0026#39;adopt\u0026#39; - name: Run Gatling tests run: | ./gradlew gatlingRun env: GATLING_SIMULATIONS_FOLDER: src/gatling/scala - name: Archive Gatling results uses: actions/upload-artifact@v2 with: name: gatling-results path: build/reports/gatling - name: Upload Gatling results to GitHub uses: actions/upload-artifact@v2 with: name: gatling-results path: build/reports/gatling Commit the code: Add the gatling.yml file to your repository and commit. View the test report: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Performance Test workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. Maven + Scala version See the demo at https://github.com/Automation-Test-Starter/gatling-maven-scala-demo\nCreate the .github/workflows directory: In your GitHub repository, create a directory called .github/workflows. This will be where the GitHub Actions workflow files will be stored.\nCreate the workflow file: Create a YAML-formatted workflow file, such as gatling.yml, in the .github/workflows directory.\nEdit the gatling.yml file: Copy the following into the file.\nname: Gatling Performance Test on: push: branches: - main jobs: performance-test: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up Java uses: actions/setup-java@v2 with: java-version: 11 distribution: \u0026#39;adopt\u0026#39; - name: Run Gatling tests run: | mvn gatling:test env: GATLING_SIMULATIONS_FOLDER: src/test/scala - name: Archive Gatling results uses: actions/upload-artifact@v2 with: name: gatling-results path: target/gatling - name: Upload Gatling results to GitHub uses: actions/upload-artifact@v2 with: name: gatling-results path: target/gatling Commit the code: Add the gatling.yml file to your repository and commit. View the test report: In GitHub, navigate to your repository. Click the Actions tab at the top and then click the Performance Test workflow on the left. You should see the workflow running, wait for the execution to complete and you can view the results. reference galting official website: https://gatling.io/ galting official documentation: https://gatling.io/docs/gatling/ galting official github: https://github.com/gatling/ ","permalink":"https://naodeng.com.cn/posts/performance-testing/gatling-tool-tutorial-ci-cd-integration/","summary":"This article introduces the advanced usage of the performance testing tool gatling: CI/CD integration, using github action as an example to introduce how to integrate gatling into the CI/CD process.","title":"Gatling Performance Testing Tutorial advanced usage: CI/CD Integration"},{"content":"Test report analysis Overview Overall view Open the detailed html report after the performance test execution is finished; Your report can be analyzed by metrics, active users and requests/responses over time, as well as distributions\nThe name of Simulation is displayed in the center of the page in the header The list on the left side shows a menu of different types of reports, which can be switched by clicking on them. The middle of the page shows an overview of the performance test report, including: total number of requests, total number of successful requests, total number of unsuccessful requests, shortest response time, longest response time, average response time, throughput, standard deviation, percentage distribution, etc. It also shows the version of gatling and the time and duration of this report. The version of gatling and the time and duration of this report run are also displayed. The Global menu points to aggregate statistics. The Details menu points to statistics for each request type. Response time ranges This chart shows the distribution of response times within the standard range The list on the left shows all requests and the distribution of request response times, with the red color representing failed requests. On the right, Number of requests represents the number of concurrent users, as well as the number of requests for each request and their success and failure status.\nThese ranges can be configured in the gatling.conf file\nSummary This chart shows some standard statistics such as minimum, maximum, average, standard deviation and percentile for global and per request. stats shows the specific success and failure of all requests OK for success, KO for failure, and 99th pct for 99th percentile response time for total requests for this API.\nThese percentiles can be configured in the gatling.conf file.\nActive users over time This chart shows that the number of active users refers to the number of users who are making requests during the test time period. At the beginning of the test, the number of active users is 0. When users start sending requests, the number of active users starts to increase. When a user completes a request, the number of active users begins to decrease. The maximum number of active users is the number of users sending requests at the same time during the test period.\nResponse time distribution This chart shows the distribution of response times, including response times for successful requests and response times for failed requests.\nResponse time percentiles over time This chart shows various response time percentiles over time, but only for successful requests. Since failed requests may end early or be caused by timeouts, they can have a huge impact on the percentile calculation.\nRequests per second over time This chart shows the number of requests per second, including the number of successful requests and the number of failed requests.\nResponse per second over time This chart shows the number of responses per second, including the number of successful responses and the number of failed responses.\nSingle request analysis report You can click the details menu on the report page to switch to the details tab and view a detailed report for a single request.\nThe Details page primarily shows per-request statistics, and similarly to the global report includes a graph of response time distribution, response time percentile, requests per second, and responses per second. The difference is that there is a graph at the bottom that depicts the response time of a single request relative to all requests globally. The horizontal coordinate of this graph is the number of all requests per second globally, and the vertical coordinate is the response time of a single request.\nPerformance Scenario Setting Injection What is Injection In Gatling performance testing, \u0026ldquo;Injection\u0026rdquo; refers to a method of introducing virtual users (or load) into the system. It defines how simulated users are introduced into a test scenario, including the number, rate, and manner of users.Injection is a key concept used in Gatling to control load and concurrency, allowing you to simulate different user behaviors and load models.\nUser injection profiles are defined using the injectOpen and injectClosed methods (inject in Scala). This method takes as arguments a sequence of injection steps that are processed sequentially. Each step defines a set of users and how these users are injected into the scene.\nMore from the web site: https://gatling.io/docs/gatling/reference/current/core/injection/\nCommon Injection Scenario Open Model Scenario setUp( scn.inject( nothingFor(4), // 1 atOnceUsers(10), // 2 rampUsers(10).during(5), // 3 constantUsersPerSec(20).during(15), // 4 constantUsersPerSec(20).during(15).randomized, // 5 rampUsersPerSec(10).to(20).during(10.minutes), // 6 rampUsersPerSec(10).to(20).during(10.minutes).randomized, // 7 stressPeakUsers(1000).during(20) // 8 ).protocols(httpProtocol) ) nothingFor(duration): set a period of time to stop, this time to do nothing atOnceUsers(nbUsers): immediately inject a certain number of virtual users rampUsers(nbUsers) during(duration): set a certain number of virtual users to be injected gradually during a specified period of time. constantUsersPerSec(rate) during(duration): Define a constant number of concurrent users per second for a specified period of time. constantUsersPerSec(rate) during(duration) randomized: defines a randomized concurrency increase/decrease around a specified number of concurrencies per second, for a specified period of time rampUsersPerSec(rate1) to (rate2) during(duration): defines a concurrency interval that runs for the specified time, with the concurrency growth period being a regular value. rampUsersPerSec(rate1) to (rate2) during(duration) randomized: define a concurrency interval, run for a specified time, the concurrency growth period is a random value stressPeakUsers(nbUsers).during(duration) : injects a given number of users according to a smooth approximation of a step function that stretches to a given duration. users. Closed Model Scenario setUp( scn.inject( constantConcurrentUsers(10).during(10), // 1 rampConcurrentUsers(10).to(20).during(10) // 2 ) ) constantConcurrentUsers(fromNbUsers).during(duration) : inject to make the number of concurrent users in the system constant rampConcurrentUsers(fromNbUsers).to(toNbUsers).during(duration) : inject so that the number of concurrent users in the system increases linearly from one number to the next Meta DSL Scenario \u0026ldquo;Meta DSL is a special Domain Specific Language (DSL) for describing the metadata and global configuration of performance test scenarios.Meta DSL allows you to define a number of global settings and parameters in a performance test that affect the entire test process, rather than being specific to a particular scenario.\nThe elements of the Meta DSL can be used to write tests in a simpler way. If you want to link levels and ramps to reach the limits of your application (a test sometimes referred to as a capacity load test), you can do this manually using the regular DSL and looping with map and flatMap.\nincrementUsersPerSec setUp( // Generate an open workload injection profile // 10, 15, 20, 25 and 30 users arrive every second // Each level lasts 10 seconds // Each level lasts 10 seconds scn.inject( incrementUsersPerSec(5.0) .times(5) .eachLevelLasting(10) .separatedByRampsLasting(10) .startingFrom(10) // Double ) incrementConcurrentUsers setUp( // Generate a closed workload injection profile // Concurrent users at levels 10, 15, 20, 25, and 30 // Each level lasts 10 seconds // Each level lasts 10 seconds scn.inject( incrementConcurrentUsers(5) .times(5) .eachLevelLasting(10) .separatedByRampsLasting(10) .startingFrom(10) // Int ) ) incrementUsersPerSec is used for open workloads, incrementConcurrentUsers is used for closed workloads (users/sec vs concurrent users).\nseparatedByRampsLasting and startingFrom are both optional. If you do not specify a ramp, the test jumps from one level to another as soon as it finishes. If you do not specify the number of starting users, the test will start with 0 concurrent users or 0 users per second and move to the next step immediately.\nConcurrent Scenario setUp( scenario1.inject(injectionProfile1), scenario2.inject(injectionProfile2) ) You can configure multiple scenes to start simultaneously and execute concurrently in the same setUp block.\nOther Scenarios Check out the website: https://gatling.io/docs/gatling/reference/current/core/injection/\n","permalink":"https://naodeng.com.cn/posts/performance-testing/gatling-tool-tutorial-advanced-usage/","summary":"This article introduces the advanced usage of the performance testing tool gatling: analysis of performance test reports, introduction of different types of test report reports, and configuration of performance test scenarios under different business types.","title":"Gatling Performance Testing Tutorial advanced usage: Test report analysis and Performance Scenario Setting"},{"content":"Build your own Gatling project from 0 to 1 Gradle + Scala versions Create an empty Gradle project mkdir gatling-gradle-demo cd gatling-gradle-demo gradle init Configure the project build.gradle Add the following to the build.gradle file in the project\nYou can copy the content of the build.gradle file in this project, for more configurations, please refer to the official documentation.\n// Plugin Configuration plugins { id \u0026#39;scala\u0026#39; // scala plugin declaration (based on the development tools plugin) id \u0026#39;io.gatling.gradle\u0026#39; version \u0026#39;3.9.5.6\u0026#39; // declaration of the version of the gradle-based gatling framework plugin } // Repository source configuration repositories { // Use the maven central repository source mavenCentral() } // gatling configuration gatling { // logback root level, defaults to the Gatling console log level if logback.xml does not exist in the configuration folder logLevel = \u0026#39;WARN\u0026#39; // Enforce logging of HTTP requests at a level of detail // set to \u0026#39;ALL\u0026#39; for all HTTP traffic in TRACE, \u0026#39;FAILURES\u0026#39; for failed HTTP traffic in DEBUG logHttp = \u0026#39;FAILURES\u0026#39; // Simulations filter simulations = { include \u0026#34;**/simulation/*.scala\u0026#34; } } // Dependencies dependencies { // Charts library for generating report charts gatling \u0026#39;io.gatling.highcharts:gatling-charts-highcharts:3.8.3\u0026#39; } gradle build project and initialize Open the Terminal window of the project with an editor and execute the following command to confirm that the project build was successful gradle build Initialization complete: After completing the wizard, Gradle will generate a basic Gradle project structure in the project directory Initialization Directory Create a simulation directory in the src/gatling/scala directory to hold test scripts\nGatling tests are usually located in the src/gatling directory. You need to manually create the src directory in the project root, and then create the gatling directory under the src directory. In the gatling directory, you can create your test simulation folder simulation, as well as other folders such as data, bodies, resources, and so on.\nWriting Scripts Create a demo.scala file in the simulation directory to write your test scripts.\nFor reference, the following is a sample script\nThe script contains two scenarios, one for get requests and one for post requests. The get API validates that the API returns a status code of 200 and the post API validates that the API returns a status code of 201. The get API uses rampUsers, the post API uses constantConcurrentUsers. rampUsers: incrementally increase the number of concurrent users over a specified period of time, constantConcurrentUsers: keep the number of concurrent users constant over a specified period of time. The number of concurrent users is 10 for both APIs, and the duration is 10 seconds for both APIs. The request interval is 2 seconds for both APIs.\npackage simulation import scala.concurrent.duration._ import io.gatling.core.Predef._ import io.gatling.http.Predef._ class demo extends Simulation { val httpProtocol = http .baseUrl(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) // 5 val scn = scenario(\u0026#34;GetSimulation\u0026#34;) .exec(http(\u0026#34;get_demo\u0026#34;) .get(\u0026#34;/posts/1\u0026#34;) .check(status.is(200))) .pause(2) val scn1 = scenario(\u0026#34;PostSimulation\u0026#34;) .exec(http(\u0026#34;post_demo\u0026#34;) .post(\u0026#34;/posts\u0026#34;) .body(StringBody(\u0026#34;\u0026#34;\u0026#34;{\u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;,\u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;,\u0026#34;userId\u0026#34;: 1}\u0026#34;\u0026#34;\u0026#34;)).asJson .check(status.is(201))) .pause(2) setUp( scn.inject(rampUsers(10) during(10 seconds)), scn1.inject(constantConcurrentUsers(10) during(10 seconds)) ).protocols(httpProtocol) } Debugging Scripts Execute the following command to run the test script and view the report\ngradle gatlingRun Maven + Scala version Create an empty Maven project mvn archetype:generate -DgroupId=demo.gatlin.maven -DartifactId=gatling-maven-demo Initialization complete: After completing the wizard, Maven will create a new project directory and generate a basic Maven project structure in the\nConfigure the project pom.xml Add the following contents to the pom.xml file in the project\nYou can copy the contents of the pom.xml file in this project, for more configuration, please refer to the official documentation.\n\u0026lt;!-- dependencies Configuration --\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.gatling.highcharts\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;gatling-charts-highcharts\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.9.5\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;!-- Plugin Configuration --\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.gatling\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;gatling-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.6.0\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.alchim31.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scala-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.8.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;scalaVersion\u0026gt;2.13.12\u0026lt;/scalaVersion\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;jvmArgs\u0026gt; \u0026lt;jvmArg\u0026gt;-Xss100M\u0026lt;/jvmArg\u0026gt; \u0026lt;/jvmArgs\u0026gt; \u0026lt;args\u0026gt; \u0026lt;arg\u0026gt;-deprecation\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;-feature\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;-unchecked\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;-language:implicitConversions\u0026lt;/arg\u0026gt; \u0026lt;arg\u0026gt;-language:postfixOps\u0026lt;/arg\u0026gt; \u0026lt;/args\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; Initialization Directory Create a simulation directory in the src/test/scala directory to hold the test scripts\nscala tests are usually located in the src/test directory. You need to create a scala directory under the project test directory. In the scala directory, you can create your test simulation folder simulation, as well as other folders such as data, bodies, resources, and so on.\nWriting Scripts Create a demo.scala file in the simulation directory to write your test scripts.\nFor reference, the following is a sample script\nThe script contains two scenarios, one for get requests and one for post requests. The get API validates that the API returns a status code of 200 and the post API validates that the API returns a status code of 201. The get API uses rampUsers, the post API uses constantConcurrentUsers. rampUsers: incrementally increase the number of concurrent users over a specified period of time, constantConcurrentUsers: keep the number of concurrent users constant over a specified period of time. The number of concurrent users is 10 for both APIs, and the duration is 10 seconds for both APIs. The request interval is 2 seconds for both APIs.\npackage simulation import scala.concurrent.duration._ import io.gatling.core.Predef._ import io.gatling.http.Predef._ class demo extends Simulation { val httpProtocol = http .baseUrl(\u0026#34;https://jsonplaceholder.typicode.com\u0026#34;) // 5 val scn = scenario(\u0026#34;GetSimulation\u0026#34;) .exec(http(\u0026#34;get_demo\u0026#34;) .get(\u0026#34;/posts/1\u0026#34;) .check(status.is(200))) .pause(2) val scn1 = scenario(\u0026#34;PostSimulation\u0026#34;) .exec(http(\u0026#34;post_demo\u0026#34;) .post(\u0026#34;/posts\u0026#34;) .body(StringBody(\u0026#34;\u0026#34;\u0026#34;{\u0026#34;title\u0026#34;: \u0026#34;foo\u0026#34;,\u0026#34;body\u0026#34;: \u0026#34;bar\u0026#34;,\u0026#34;userId\u0026#34;: 1}\u0026#34;\u0026#34;\u0026#34;)).asJson .check(status.is(201))) .pause(2) setUp( scn.inject(rampUsers(10) during(10 seconds)), scn1.inject(constantConcurrentUsers(10) during(10 seconds)) ).protocols(httpProtocol) } Debugging Scripts Execute the following command to run the test script and view the report\nmvn gatling:test ","permalink":"https://naodeng.com.cn/posts/performance-testing/gatling-tool-tutorial2/","summary":"The article introduces the performance testing tool gatling advanced introduction: from 0 to 1 build your own Gatling project, introduces the basic use of Gatling, and how to build your own Gatling project, write performance test scripts, view the test report and so on.","title":"gatling Performance Testing Tutorial: building your own gatling project from 0 to 1"},{"content":"Gatling Introduction Gatling is an open source tool for performance and load testing, especially for testing web applications. It is a high-performance tool based on the Scala programming language for simulating and measuring the performance of applications under different loads.\nHere are some of the key features and benefits of Gatling:\nBased on Scala programming language: Gatling\u0026rsquo;s test scripts are written in Scala, which makes it a powerful programming tool that allows users to write complex test scenarios and logic. High Performance: Gatling is designed as a high performance load testing tool. It uses non-blocking I/O and an asynchronous programming model that is capable of simulating large numbers of concurrent users to better mimic real-world load situations. Easy to learn and use: Although Gatling\u0026rsquo;s test scripts are written in Scala, its DSL (Domain Specific Language) is very simple and easy to learn. Even if you are not familiar with Scala, you can quickly learn how to create test scripts. Rich Features: Gatling provides a rich set of features, including request and response processing, data extraction, conditional assertions, performance report generation, and more. These features enable you to create complex test scenarios and perform comprehensive evaluation of application performance. Multi-Protocol Support: In addition to HTTP and HTTPS, Gatling supports other protocols such as WebSocket, JMS, and SMTP, making it suitable for testing a wide range of different types of applications. Real-time results analysis: Gatling provides real-time performance data and graphical reports during test runs to help you quickly identify performance issues. Open source and active community: Gatling is an open source project with an active community that constantly updates and improves the tool. CI/CD Integration Support: Gatling can be integrated with CI/CD tools such as Jenkins to perform performance testing in continuous integration and continuous delivery processes. Overall, Gatling is a powerful performance testing tool for testing a wide range of application types, helping development teams identify and resolve performance issues to ensure consistent performance and scalability of applications in production environments.\nEnvironment setup Since I\u0026rsquo;m a macbook user, I\u0026rsquo;ll use the macbook demo as an example in the introduction, but windows users can refer to it on their own.\nVSCode + Gradle + Scala Version Preparation Development tool: VSCode Install Gradle version \u0026gt;= 6.0, I am using Gradle 8.44. Install JDK version \u0026gt;= 8, I use JDK 19 install plugins VSCode search for Scala (Metals) plugin for installation VSCode search for Gradle for Java plugin for installation official demo initialization \u0026amp; debugging We will use the official demo project for initialization and debugging first, and then we will introduce how to create your own project later.\nClone the official demo project git clone git@github.com:gatling/gatling-gradle-plugin-demo-scala.git Open the cloned official demo project with VSCode.\nOpen the project\u0026rsquo;s Terminal window with VSCode and execute the following command\ngradle build Run the demo in the project gradle gatlingRun Viewing the results of a command line run Click on the html report link in the command line report and open it with your browser to view the detailed report information VSCode + Maven + Scala version Preparation Development tool: VSCode Install Maven, I use Maven 3.9.5 JDK version \u0026gt;= 8, I use JDK 19 install plugins VSCode search for Scala (Metals) plugins to install VSCode search for Maven for Java plugins to install Official demo initialization \u0026amp; debugging We will use the official demo project for initialization and debugging first, and then we will introduce how to create your own project.\nClone the official demo project git clone git@github.com:gatling/gatling-maven-plugin-demo-scala.git Use VSCode to open the cloned official demo project.\nOpen the Terminal window of this project with VSCode and execute the following command to run the demo in the project\nmvn gatling:test Viewing the results of a command line run Click on the html report link in the command line report and open it with your browser to view the detailed report information IDEA + Gradle + Scala version This is similar to the VSCode version, so I won\u0026rsquo;t repeat it here.\nThe differences are as follows:\nIDEA searches for Scala plugins to install New way to run: right click and select Engine.scala file in the project directory, select Run \u0026lsquo;Engine\u0026rsquo; to run the demo (you need to press enter to confirm the run). IDEA + Maven + Scala version This is similar to the VSCode version, so I won\u0026rsquo;t repeat it here.\nThe differences are as follows:\nIDEA searches for Scala plugins to install New way to run: right-click the Engine.scala file in the project directory and select Run \u0026lsquo;Engine\u0026rsquo; to run the demo (you need to press enter to confirm during the run). ","permalink":"https://naodeng.com.cn/posts/performance-testing/gatling-tool-tutorial1/","summary":"This article describes how to get started with the performance testing tool gatling, how to set up the environment, and how to get the official demo up and running.","title":"Gatling Performance Testing Tutorial: Getting Started"},{"content":"Since Postman announced in May 2023 that it will phase out the Scratch Pad model with offline capabilities, teams that need to isolate API workspace data from third-party servers have been looking for alternatives. Teams that need to isolate API workspace data from third-party servers have had to look for alternatives. bruno is one of those alternatives: an open source desktop application designed for API testing, development, and debugging.\nBruno is one of those alternatives: an open source desktop application designed for API testing, development, and debugging. Why bruno, how to get started, and how to migrate postman scripts are all covered in this article!\nwhy bruno Official description: https://github.com/usebruno/bruno/discussions/269\nComparison with postman: https://www.usebruno.com/compare/bruno-vs-postman\nOpen source, MIT License\nClient platform support (Mac/linux/Windows)\nOffline client, no cloud synchronization plan\nSupports Postman/insomina script import (only API request scripts can be imported, not test scripts)\nRelatively active community and clear product development roadmap.\nInstall bruno Download link: https://www.usebruno.com/downloads\nMac computer recommended brew command download\n​ brew install Bruno\nGetting Started Default main API API collection Create API collection On the home page, click on the \u0026lsquo;Create Collection\u0026rsquo; link to open the Create API Request Collection pop-up window.\nOn the popup window, enter\nName: input the name of the API request collection\nLocation: input the path of the folder where you want to save the API request collection files (we suggest you choose the path where this project is located).\nFolder Name: you can enter the name of the API request set (a folder with the corresponding name will be created under the path you just selected).\nClick Create button to finish creating the API request set and display it on the API (the list of newly created API request set will be displayed on the left side).\nOpen API collection Click on the \u0026lsquo;Open Collection\u0026rsquo; link on the home page to open the folder of the selected API request collection in bruno format. Click open to complete the selection and display it in the API (the collection list on the left side will display the selected API request collection information). Import API collection Click the \u0026lsquo;Import Collection\u0026rsquo; link on the home page to open the popup window for importing API collections (Bruno/Postman/Insomnia are supported). On the popup window, select the link of the corresponding format, and then select the path of the existing file of the corresponding format. Click open to complete the selection and display it on the API (the collection list on the left side will display the information of the selected API collection). RUN API collection Select the API request set you want to run from the collection list on the left side of the main API. Select Run on the menu, the Runner tab will be opened on the right side of the API, it will show some information about the requests in the selected API request collection. Click on the Run Collection button to run it locally (you will see the allowed results on the screen after running). Export API collection Select the API request set you want to run from the collection list on the left side of the main API, and right-click to open the menu. Select Export on the menu, and then select the path of the file you want to export to complete the export (the exported file is also in json format). API request Create API request Pre-requisite: An API request collection has already been created (see Creating an API Request Collection above). Select the API request set you want to create a new API request from the collection list on the left side of the main API. Select New Request on the menu, the right API will open the Request tab, it will show some information of requests in the selected API request set. On the new Request window, first select the request type: HTTP/GraphQL. In the new Request window, first select the request type: HTTP/GraphQL. Name: Enter the name of the API request. URL: enter the URL of the API request Method: Select the Method of the API request. Click Create button to finish creating the API request and display it on the API (the left request set list will display the information of the newly created API request). Edit API request Pre-requisite: you have already created an API request collection and an API request (refer to Creating an API request collection and New API request above).\nSelect the API request collection you want to edit in the collection list on the left side of the main API, and then select the API request you want to edit.\nThen you can edit different fields of the request according to the type of API request. Body: Enter the Body of the API request.\nHeaders: Enter the headers of the API request.\nParams: Enter the Params of the API request.\nAuth: enter the Auth of the API request\nVars: enter the Vars of the API request\nScript: enter the Script of the API request\nAssert: Enter the Assert of the API request.\nTests: Enter the Tests of the API request.\nClick the Save button to finish editing the API request and display it on the API (the list of request sets on the left side will display the information of the edited API request).\nRUN API request Pre-requisite: you have already created an API request collection and an API request (refer to Creating an API request collection and New API request above). In the collection list on the left side of the main API, select the API request set that you want to edit the API request, and then select the API request that you want to edit. Click the right button after the API url edit box to finish running the API request and display it on the API (the Request tab on the right side will display the information of the running API request). API request generate code Pre-requisite: you have already created an API request collection and an API request (refer to Creating an API request collection and New API request above). In the collection list on the left side of the main API, select the API request set that you want to edit the API request, and then select the API request that you want to edit. Right click on the menu and select Generate Code, then select the language you want to generate code for. The Generate Code window will show the request code of different languages. Write API request test scripts API request Assert Introducing Assert Open any API request and switch to the Assert tab.\nThe Assert tab displays the Assert information of the API request.\nAssert is used to determine whether the result of the API request meets the expectation.\nExpr: input the expression of expected result, it can be the value of a field of the API request, two types can be input: Status Code and Response Body. Status Code: determine whether the returned status code of the API request meets the expectation (default is 200). Response Body: determine whether the returned result of the API request meets the expectation (default is true).\nOperator: the validation method for inputting the expected result. Supports multiple judgment methods: Equal and Not Equal, etc. Equal: determine whether the returned result of the API request is equal to the expected result. Not Equal: determine if the returned result of the API request is not equal to the expected result.\nValue: input the value of the expected result, supports two ways of inputting the expected result: Static and Dynamic. Static: input the static value of the expected result. Dynamic: input the dynamic value of the expected result, which can be the value of a field in the return result of the API request.\nAssert demo Assert status code is 200 Taking https://jsonplaceholder.typicode.com/posts/1 as an example (the API request returns https://jsonplaceholder.typicode.com/posts/1) I want to verify that the API request returns a status is 200. Open the API request and switch to the Assert tab. Enter the following information Expr: res.status Operator: Equal Value: 200 Assert repsponse body as expected Using https://jsonplaceholder.typicode.com/posts/1 as an example (the API request returned https://jsonplaceholder.typicode.com/posts/1) I want to verify that the API request\u0026rsquo;s repsponse body is as expected Open the API request and switch to the Assert tab. Assert1 Enter the following information in order Expr: res.body.id Operator: Equal Value: 1 Assert2 Input the following information in order Expr: res.body.title Operator: contains Value: provider Debug Assert Pre-requisite: you have already created an API request set and an API request (refer to Creating an API request set and New API request above), and you have also written the corresponding Assert according to the demo. Select the API request set you want to edit in the collection list on the left side of the main API, and then select the API request you want to edit. Click the right button after the API url edit box to finish running the API request and display it on the API (the Request tab on the right side will display the information of the running API request). Switch to the Tests tab to display the Tests information of the API request, which also includes the Assert information of the request. API request Tests Introduction Tests Open any API request and switch to the Tests tab. Tests tab will show the Tests information of the API request. Tests are used to write test scripts for API requests, currently javascript language is supported. You can write multiple test scripts inside Tests, each test script can be run separately. Tests demo Verify status code is 200 Taking https://jsonplaceholder.typicode.com/posts/1 as an example (the API request returns https://jsonplaceholder.typicode.com/posts/1), I want to verify that the API request returns a status is 200. Open the API request and switch to the Tests tab. Enter the following script test(\u0026#34;res.status should be 200\u0026#34;, function() { const data = res.getBody(); expect(res.getStatus()).to.equal(200); }); Verify repsponse body as expected Taking https://jsonplaceholder.typicode.com/posts/1 as an example (the API request returned https://jsonplaceholder.typicode.com/posts/1) I want to verify that the repsponse body is as expected Open the API request and switch to the Tests tab. Enter the following script test(\u0026#34;res.body should be correct\u0026#34;, function() { const data = res.getBody(); expect(data.id).to.equal(1); expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); Debugging Tests Prerequisites: You have already created an API request set and an API request (refer to Creating an API Request Set and New API Request above), and you have also written the corresponding Tests according to the demo. Select the API request set you want to edit in the collection list on the left side of the main API, and then select the API request you want to edit. Click the right button after the API url edit box to finish running the API request and display it on the API (the Request tab on the right side will display the information of the running API request). Switch to the Tests tab, it will show the Tests information of the API request, which will also include the requested Tests information. environment variables Creating Environment Variables Prerequisites: An API request set and an API request have already been created (see Creating an API request set and New API request above). Select the API request for which you want to create an environment variable Click the \u0026lsquo;No Environment\u0026rsquo; link in the upper right corner of the page (default is No Environment) and select the configure button in the menu to open the environment variable management popup window (supports creating new environment variables and importing existing environment variables). Click Create Environment button on the popup window, enter the name of the environment variable and click create button to create the environment variable. Then click Add Variable button on the popup window, enter the key and value of the environment variable, and click Save button to add the environment variable. environment variable demo Requirement: Create a demo environment variable that contains an environment variable with key host and value https://jsonplaceholder.typicode.com.\nSelect the API request for which you want to create the environment variable Click the \u0026lsquo;No Environment\u0026rsquo; link in the upper right corner of the page (default is No Environment), and select the configure button in the menu to open the environment variable management popup. Click the Create Environment button on the popup window, enter the name of the environment variable demo, and click the create button to create the environment variable demo. Select the demo environment variable, and then click Add Variable button on the page, enter the key of the environment variable as host and the value as https://jsonplaceholder.typicode.com, and click Save button to add the environment variable. As shown in the following figure ! env-intro Using Environment Variables Prerequisites: You have already created an API request set and an API request (see Creating an API request set and creating a new API request above), and you have also created a demo environment variable. Select the API request for which you want to use environment variables Click the \u0026lsquo;No Environment\u0026rsquo; link in the top right corner of the page (default is No Environment), and select the demo button in the menu to use the demo environment variable. Then change the URL of the API request to {{host}}/posts/1 to use the environment variable. Test script automation Pre-conditions API request set has been created (example named :api-collects) API request has been created (example name: api request1) an environment variable has been created (example name: demo) has also written an assert or tests script for the API request api automation project demo Installed node.js Install npm create a new project folder (example name: bruno-test) Execute npm init in the project folder to initialize the project as an npm project Install @usebruno/cli dependency (script: npm install @usebruno/cli) Open the folder directory where the API request sets are stored, and copy all the files in the api-collects directory to the bruno-test project directory The project directory looks like this bruno-test //项目主文件夹 api request1.bru //api 请求 enviroments //环境变量 demo.bru bruno.json node_modules //node 包依赖 package-lock.json package.json //npm 项目配置文件 Run the following command in the project directory to run the API request bruno run --env demo The result is as follows Getting into CI Getting into github action Take github action as an example, other CI tools are similar.\nPrepare: Add the following script to the project package.json file \u0026#34;test\u0026#34;: \u0026#34;bru run --env demo\u0026#34; Create .github/workflows folder in the project root folder Create main.yml file under .github/workflows folder The contents of the main.yml file are as follows name: bruno cli CI on: push: branches: [ main ] pull_request: branches: [ main ] jobs: run_bruno_api_test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - run: npm install - name: run tests run: npm run test submit code to github, will automatically trigger github action View the result of the github action, as shown in the example: The code for this project can be pulled for reference:https://github.com/dengnao-tw/Bruno-API-Test-Starter\nTest report\u0026mdash;TODO bruno More usage\u0026mdash;TODO Postman script migration API Request Collection Migration Click on the \u0026lsquo;Import Collection\u0026rsquo; link on the home page to open the Import API collection popup window. Click on the Select Postman Collection link and select the path to an existing Postman request collection file. Then you can import Postman request collection. However, only API requests can be imported, not test scripts, as shown in the figure (but it doesn\u0026rsquo;t affect the request invocation). Environment Variable Migration Select the Postman request you just imported on the home page. Click the \u0026lsquo;No Environment\u0026rsquo; link in the upper right corner of the page (default is No Environment), and select the configure button in the menu to open the environment variable management popup window. Click on the \u0026lsquo;Import Environment\u0026rsquo; link to open the Import Environment popup. Click on the \u0026lsquo;Postman Environment\u0026rsquo; link to open the Import Environment popup window Click on the \u0026lsquo;Postman Environment\u0026rsquo; link and select the path to an existing Postman environment file You can import Postman environment variables. Test Script Migration Reference The syntax of the test scripts for the two tools is partially different and needs to be modified manually\nPostman test script syntax reference: https://learning.postman.com/docs/writing-scripts/test-scripts/ Postman test script example pm.test(\u0026#34;res.status should be 200\u0026#34;, function () { pm.response.to.have.status(200); }); pm.test(\u0026#34;res.body should be correct\u0026#34;, function() { var data = pm.response.json(); pm.expect(data.id).to.equal(1); pm.expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); Bruno test script syntax reference: https://docs.usebruno.com/testing/introduction.html Bruno test script example test(\u0026#34;res.status should be 200\u0026#34;, function() { const data = res.getBody(); expect(res.getStatus()).to.equal(200); }); test(\u0026#34;res.body should be correct\u0026#34;, function() { const data = res.getBody(); expect(data.id).to.equal(1); expect(data.title).to.contains(\u0026#39;provident\u0026#39;); }); ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/introduction_of_bruno/","summary":"Article introduces postman replacement tool bruno beginner\u0026rsquo;s introduction, how to migrate postman scripts to bruno","title":"Introducing Bruno for Replacement Postman"},{"content":"什么是 API? API:应用程序接口（全称：application programming interface），缩写为 API，是一种计算接口，它定义多个软件中介之间的交互，以及可以进行的调用（call）或请求（request）的种类，如何进行调用或发出请求，应使用的数据格式，应遵循的惯例等。它还可以提供扩展机制，以便用户可以通过各种方式对现有功能进行不同程度的扩展。一个 API 可以是完全定制的，针对某个组件的，也可以是基于行业标准设计的以确保互操作性。通过信息隐藏，API 实现了模块化编程，从而允许用户实现独立地使用接口。\n什么是 API 测试？ 接口测试是软件测试的一种，它包括两种测试类型：狭义上指的是直接针对应用程序接口（下面使用缩写 API 指代，其中文简称为接口）的功能进行的测试；广义上指集成测试中，通过调用 API 测试整体的功能完成度、可靠性、安全性与性能等指标。\nAPI Best Practice:\nAPI 定义遵循 RESTFUL API 风格，语意化的 URI 定义，准确的 HTTP 状态码，通过 API 的定义就可以知道资源间的关系 配有详细且准确的 API 文档（如 Swagger 文档） 对外的 API 可以包含版本号以快速迭代（如 https://thoughtworks.com/v1/users/） API 测试与测试四象限 测试四象限中不同象限的测试，其测试目的跟测试策略也不同，API 测试主要位于第二、第四象限\nAPI 测试与测试金字塔 API 测试在测试金子塔中处于一个相对靠上的位置，主要站在系统、服务边界来测试功能和业务逻辑，执行时机是在服务完成构建、部署到测试环境之后再执行、验证。\nAPI 测试类型 功能测试\n正确性测试 异常处理 内部逻辑 …… 非功能测试\n性能 安全 …… API 测试步骤 发送请求 得到响应 验证响应结果 API 功能测试设计 设计理论\n正面 负面 异常处理 内部逻辑 …… 测试方法\n等价类划分 边界值 错误推断 …… API 非功能测试设计 安全测试\n随机测试 SQL 注入 XSS …… 性能测试\n性能瓶颈 稳定性测试 …… API 测试工具 API 请求工具\nCURL Soap UI Postman Swagger UI …… Http proxy 工具\nFiddler Charles …… API 性能测试工具\nab(apache bench) Jmeter …… ","permalink":"https://naodeng.com.cn/posts/api-automation-testing/introduction_of_api_test/","summary":"文章介绍接口测试的简介，类型和工具","title":"Introduce of API Testing"}]